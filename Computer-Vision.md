---
layout: default
title: Deep Learning 101, Taiwanâ€™s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101
---

<p align="center">
  <strong>Deep Learning 101, Taiwanâ€™s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101</strong>  
</p>
<p align="center">
  AIæ˜¯ä¸€æ¢å­¤ç¨ä¸”å……æ»¿æƒ¶æåŠæœªçŸ¥çš„æ—…ç¨‹ï¼ŒèŠ±ä¿çµ¢éº—çš„æ”¶è²»èª²ç¨‹æˆ–æ´»å‹•çµ•éé€šå¾€æˆåŠŸçš„æ·å¾‘ã€‚<br>
  è¡·å¿ƒæ„Ÿè¬ç•¶æ™‚ä¾†è‡ªä¸åŒå–®ä½çš„AIåŒå¥½åƒèˆ‡è€…å¯¦ååˆ†äº«çš„å¯¶è²´ç¶“é©—ï¼›å¦‚æ¬²ç§»é™¤è³‡è¨Šé‚„è«‹å‘ŠçŸ¥ã€‚<br>
  ç”± <a href="https://www.twman.org/" target="_blank">TonTon Huang Ph.D.</a> ç™¼èµ·ï¼ŒåŠå…¶ç•¶æ™‚ä»»è·å…¬å¸(å°ç£é›ªè±¹ç§‘æŠ€)ç„¡å„Ÿè´ŠåŠ©å ´åœ°åŠèŒ¶æ°´é»å¿ƒã€‚<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="180"></a>
    <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 100px !important;width: 180px !important;" ></a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101" target="_blank">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/" target="_blank">Facebook</a> |
  <a href="https://deep-learning-101.github.io/"> å› GitHub Pages</a> |
  <a href="http://DeepLearning101.TWMAN.ORG" target="_blank">ç¶²ç«™</a> |
  <a href="https://huggingface.co/DeepLearning101" target="_blank">Hugging Face Space</a>
</p>

---

<div align="center">

<table>
  <tr>
    <td align="center"><a href="https://deep-learning-101.github.io/Large-Language-Model">å¤§èªè¨€æ¨¡å‹</a></td>
    <td align="center"><a href="https://deep-learning-101.github.io/Speech-Processing">èªéŸ³è™•ç†</a></td>
    <td align="center"><a href="https://deep-learning-101.github.io/Natural-Language-Processing">è‡ªç„¶èªè¨€è™•ç†</a></td>
    <td align="center"><a href="https://deep-learning-101.github.io//Computer-Vision">é›»è…¦è¦–è¦º</a></td>
  </tr>
  <tr>
    <td><a href="https://github.com/Deep-Learning-101/Natural-Language-Processing-Paper?tab=readme-ov-file#llm">Large Language Model</a></td>
    <td><a href="https://github.com/Deep-Learning-101/Speech-Processing-Paper">Speech Processing</a></td>
    <td><a href="https://github.com/Deep-Learning-101/Natural-Language-Processing-Paper">Natural Language Processing, NLP</a></td>
    <td><a href="https://github.com/Deep-Learning-101/Computer-Vision-Paper">Computer Vision</a></td>
  </tr>
</table>

</div>

---

<details>
<summary>æ‰‹æŠŠæ‰‹å¸¶ä½ ä¸€èµ·è¸© AI å‘</summary>

<h3><a href="https://blog.twman.org/p/deeplearning101.html" target="_blank">æ‰‹æŠŠæ‰‹å¸¶ä½ ä¸€èµ·è¸© AI å‘</a>ï¼š<a href="https://www.twman.org/AI" target="_blank">https://www.twman.org/AI</a></h3>

<ul>
  <li>
    <b><a href="https://blog.twman.org/2025/03/AIAgent.html" target="_blank">é¿é–‹ AI Agent é–‹ç™¼é™·é˜±ï¼šå¸¸è¦‹å•é¡Œã€æŒ‘æˆ°èˆ‡è§£æ±ºæ–¹æ¡ˆ</a></b>ï¼š<a href="https://deep-learning-101.github.io/agent" target="_blank">æ¢è¨å¤šç¨® AI ä»£ç†äººå·¥å…·çš„æ‡‰ç”¨ç¶“é©—èˆ‡æŒ‘æˆ°ï¼Œåˆ†äº«å¯¦ç”¨ç¶“é©—èˆ‡å·¥å…·æ¨è–¦ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/08/LLM.html" target="_blank">ç™½è©±æ–‡æ‰‹æŠŠæ‰‹å¸¶ä½ ç§‘æ™® GenAI</a></b>ï¼š<a href="https://deep-learning-101.github.io/GenAI" target="_blank">æ·ºé¡¯ä»‹ç´¹ç”Ÿæˆå¼äººå·¥æ™ºæ…§æ ¸å¿ƒæ¦‚å¿µï¼Œå¼·èª¿ç¡¬é«”è³‡æºå’Œæ•¸æ“šçš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/09/LLM.html" target="_blank">å¤§å‹èªè¨€æ¨¡å‹ç›´æ¥å°±æ‰“å®Œæ”¶å·¥ï¼Ÿ</a></b>ï¼š<a href="https://deep-learning-101.github.io/1010LLM" target="_blank">å›é¡§ LLM é ˜åŸŸæ¢ç´¢æ­·ç¨‹ï¼Œè¨è«–ç¡¬é«”å‡ç´šå° AI é–‹ç™¼çš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/07/RAG.html" target="_blank">æª¢ç´¢å¢å¼·ç”Ÿæˆ(RAG)ä¸æ˜¯è¬éˆä¸¹ä¹‹å„ªåŒ–æŒ‘æˆ°æŠ€å·§</a></b>ï¼š<a href="https://deep-learning-101.github.io/RAG" target="_blank">æ¢è¨ RAG æŠ€è¡“æ‡‰ç”¨èˆ‡æŒ‘æˆ°ï¼Œæä¾›å¯¦ç”¨ç¶“é©—åˆ†äº«å’Œå·¥å…·å»ºè­°ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/02/LLM.html" target="_blank">å¤§å‹èªè¨€æ¨¡å‹ (LLM) å…¥é–€å®Œæ•´æŒ‡å—ï¼šåŸç†ã€æ‡‰ç”¨èˆ‡æœªä¾†</a></b>ï¼š<a href="https://deep-learning-101.github.io/0204LLM" target="_blank">æ¢è¨å¤šç¨® LLM å·¥å…·çš„æ‡‰ç”¨èˆ‡æŒ‘æˆ°ï¼Œå¼·èª¿ç¡¬é«”è³‡æºçš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2023/04/GPT.html" target="_blank">ä»€éº¼æ˜¯å¤§èªè¨€æ¨¡å‹ï¼Œå®ƒæ˜¯ä»€éº¼ï¼Ÿæƒ³è¦å—ï¼Ÿ(Large Language Modelï¼ŒLLM)</a></b>ï¼š<a href="https://deep-learning-101.github.io/GPU" target="_blank">æ¢è¨ LLM çš„ç™¼å±•èˆ‡æ‡‰ç”¨ï¼Œå¼·èª¿ç¡¬é«”è³‡æºåœ¨é–‹ç™¼ä¸­çš„é—œéµä½œç”¨ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/11/diffusion.html" target="_blank">Diffusion Model å®Œå…¨è§£æï¼šå¾åŸç†ã€æ‡‰ç”¨åˆ°å¯¦ä½œ (AI åœ–åƒç”Ÿæˆ)</a></b>ï¼›<a href="https://deep-learning-101.github.io/diffusion" target="_blank">æ·±å…¥æ¢è¨å½±åƒç”Ÿæˆèˆ‡åˆ†å‰²æŠ€è¡“çš„æ‡‰ç”¨ï¼Œå¼·èª¿ç¡¬é«”è³‡æºçš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2024/02/asr-tts.html" target="_blank">ASR/TTS é–‹ç™¼é¿å‘æŒ‡å—ï¼šèªéŸ³è¾¨è­˜èˆ‡åˆæˆçš„å¸¸è¦‹æŒ‘æˆ°èˆ‡å°ç­–</a></b>ï¼š<a href="https://deep-learning-101.github.io/asr-tts" target="_blank">æ¢è¨ ASR å’Œ TTS æŠ€è¡“æ‡‰ç”¨ä¸­çš„å•é¡Œï¼Œå¼·èª¿æ•¸æ“šè³ªé‡çš„é‡è¦æ€§ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2021/04/NLP.html" target="_blank">é‚£äº› NLP è¸©çš„å‘</a></b>ï¼š<a href="https://deep-learning-101.github.io/nlp" target="_blank">åˆ†äº« NLP é ˜åŸŸçš„å¯¦è¸ç¶“é©—ï¼Œå¼·èª¿æ•¸æ“šè³ªé‡å°æ¨¡å‹æ•ˆæœçš„å½±éŸ¿ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2021/04/ASR.html" target="_blank">é‚£äº›èªéŸ³è™•ç†è¸©çš„å‘</a></b>ï¼š<a href="https://deep-learning-101.github.io/speech" target="_blank">åˆ†äº«èªéŸ³è™•ç†é ˜åŸŸçš„å¯¦å‹™ç¶“é©—ï¼Œå¼·èª¿è³‡æ–™å“è³ªå°æ¨¡å‹æ•ˆæœçš„å½±éŸ¿ã€‚</a>
  </li>
  <li>
    <b><a href="https://blog.twman.org/2020/05/DeepLearning.html" target="_blank">æ‰‹æŠŠæ‰‹å­¸æ·±åº¦å­¸ç¿’å®‰è£ç’°å¢ƒ</a></b>ï¼š<a href="https://deep-learning-101.github.io/101" target="_blank">è©³ç´°ä»‹ç´¹åœ¨ Ubuntu ä¸Šå®‰è£æ·±åº¦å­¸ç¿’ç’°å¢ƒçš„æ­¥é©Ÿï¼Œåˆ†äº«å¯¦éš›æ“ä½œç¶“é©—ã€‚</a>
  </li>
</ul>

</details>

---

# CV
Computer Vision (é›»è…¦è¦–è¦º)

## Anomaly Detection (ç•°å¸¸æª¢æ¸¬)
- 2025-05-15ï¼š[AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection](https://www.alphaxiv.org/overview/2407.15795)ï¼›[Github](https://github.com/aiiu-lab/AdaptCLIP)ï¼›[é¨°è¨Šé–‹æºAdaptCLIP æ¨¡å‹åˆ·æ–°å¤šé ˜åŸŸSOTA](https://mp.weixin.qq.com/s/w5x6T18aSZt9jxqMIdf-Yg)
- 2025-05-05ï¼š[Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models](https://www.alphaxiv.org/zh/overview/2505.02626)ï¼›[DeepWiki](https://deepwiki.com/Sassanmtr/VELM)ï¼›[æ•¸æ“šé›†](https://www.mvtec.com/company/research/datasets/mvtec-ad)
- 2025-04-27ï¼š[AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection](https://www.alphaxiv.org/overview/2310.18961)ï¼›[DeepWiki](https://deepwiki.com/zqhang/AnomalyCLIP)
- 2025-04-12ï¼š[Anomaly-Aware CLIP, AA-CLIP: Enhancing Zero-shot Anomaly Detection via Anomaly-Aware CLIP](https://www.alphaxiv.org/zh/overview/2503.06661)ï¼›[DeepWiki](https://deepwiki.com/Mwxinnn/AA-CLIP)

## Object Detection (ç›®æ¨™åµæ¸¬)
- [AAAI2025, Multi-clue Consistency Learning to Bridge Gaps Between General and Oriented Object in Semi-supervised Detection](https://www.alphaxiv.org/abs/2407.05909)ï¼›[Github](https://github.com/facias914/sood-mcl)ï¼›[AAAI2025 ä¸€å€‹é™æ„ŸåŠç›£ç£ç›®æ¨™åµæ¸¬ï¼ˆåŠç›£ç£æ—‹è½‰ç›®æ¨™åµæ¸¬ï¼‰æ–¹æ³•](https://zhuanlan.zhihu.com/p/26788012528)
- 2025-03-14ï¼š[Falcon: A Remote Sensing Vision-Language Foundation Model](https://www.alphaxiv.org/abs/2503.11070)ï¼›[DeepWiki](https://deepwiki.com/TianHuiLab/Falcon)
 

## Segmentation (åœ–åƒåˆ†å‰²)
- [RESAnything: Attribute Prompting for Arbitrary Referring Segmentation](https://www.alphaxiv.org/abs/2505.02867)ï¼›[Project](https://suikei-wang.github.io/RESAnything/)
- [CVPR 2025, Segment Any Motion in Videos, Segment Any Motion in Videos](https://www.alphaxiv.org/zh/overview/2503.22268)ï¼›[Github](https://github.com/nnanhuang/SegAnyMo)
- [CVPR 2025 Highlight, Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation](https://www.alphaxiv.org/zh/overview/2412.03968)ï¼›[Github](https://github.com/MiSsU-HH/Exact)ï¼›[Exactï¼šåŸºæ–¼é™æ„Ÿå½±åƒæ™‚é–“åºåˆ—å¼±ç›£ç£å­¸ç¿’çš„ä½œç‰©æå–æ–¹æ³•](https://zhuanlan.zhihu.com/p/38754229963)
- [MatAnyone](https://github.com/pq-yang/MatAnyone)ï¼š[è¦–è¨Šæ‘³åœ–MatAnyoneä¾†äº†ï¼Œä¸€æ¬¡æŒ‡å®šå…¨ç¨‹è¿½è¸ªï¼Œé«®çµ²ç´šé‚„åŸ](https://www.jiqizhixin.com/articles/2025-04-17-27)
- [Meta Segment Anything Model 2 (SAM 2)](https://ai.meta.com/sam2/)
   - [60è¡Œç¨‹å¼ç¢¼è¨“ç·´/å¾®èª¿Segment Anything 2](https://mp.weixin.qq.com/s/YfgYCzvi0cXxOFIfQvE_9w)
   - [CLIPSegï¼šImage Segmentation Using Text and Image Prompts](https://github.com/timojl/clipseg)ï¼š[Huggingface Space](https://huggingface.co/spaces/taesiri/CLIPSeg)
      - [å“¥å»·æ ¹å¤§å­¸æå‡ºCLIPSegï¼Œèƒ½åŒæ™‚ä½œä¸‰å€‹åˆ†å‰²ä»»å‹™çš„æ¨¡å‹](https://mp.weixin.qq.com/s/evKssKulZiUssLN71t6_Lw)
      - [SAMèˆ‡CLIPå¼·å¼·è¯æ‰‹ï¼Œå¯¦ç¾22000é¡çš„åˆ†å‰²èˆ‡è­˜åˆ¥](https://mp.weixin.qq.com/s/evKssKulZiUssLN71t6_Lw)
- [SAMURAI](https://yangchris11.github.io/samurai/)
   - [ç„¡éœ€è¨“ç·´æˆ–å¾®èª¿å³å¯å¾—åˆ°ç©©å®šã€æº–ç¢ºçš„è¿½è¹¤æ•ˆæœï¼ KF + SAM2 è§£æ±ºå¿«é€Ÿç§»å‹•æˆ–è‡ªé®æ“‹çš„ç‰©ä»¶è¿½è¹¤å•é¡Œ](https://mp.weixin.qq.com/s/iU3Bk_uO01GWUxAtIBsrWQ)
   - [ç¶“å…¸å¡çˆ¾æ›¼æ¿¾æ³¢å™¨æ”¹é€²å½±ç‰‡ç‰ˆã€Œåˆ†å‰²ä¸€åˆ‡ã€ï¼Œç¶²å‹ï¼šå¥½å„ªé›…çš„æ–¹æ³•](https://www.qbitai.com/2024/11/223020.html)
- [Grounded SAM 2: Ground and Track Anything in Videos](https://github.com/IDEA-Research/Grounded-SAM-2)
   - [Grounded-Segment-Anything](https://huggingface.co/spaces/yizhangliu/Grounded-Segment-Anything)
- [SAM2Long](https://github.com/Mark12Ding/SAM2Long)ï¼š[å¤§å¹…æå‡SAM 2æ€§èƒ½ï¼æ¸¯ä¸­æ–‡æå‡ºSAM2Longï¼Œè¤‡é›œé•·è¦–é »çš„åˆ†å‰²æ¨¡å‹](https://mp.weixin.qq.com/s/henvaxGoNgx24NLQV1Qj2w)
- [SAM2-Adapter](https://github.com/tianrun-chen/SAM-Adapter-PyTorch)ï¼š[SAM 2ç„¡æ³•åˆ†å‰²ä¸€åˆ‡ï¼Ÿ SAM2-Adapterï¼šé¦–æ¬¡è®“SAM 2åœ¨ä¸‹æ¸¸ä»»å‹™é©æ‡‰èª¿æ ¡ï¼](https://mp.weixin.qq.com/s/3z-LshKAgbSzNCzyoLOuag)
- [SAM2Point](https://github.com/ZiyuGuo99/SAM2Point)ï¼š[å¯æç¤º3D åˆ†å‰²ç ”ç©¶é‡Œç¨‹ç¢‘ï¼ SAM2Pointï¼šSAM2åŠ æŒå¯æ³›åŒ–ä»»3Då ´æ™¯ã€ä»»æ„æç¤ºï¼](https://mp.weixin.qq.com/s/TnTK5UE7O_hcrNzloxBmAw)

## Diffusion model (æ“´æ•£æ¨¡å‹)
- 2025-05-19ï¼š[Index-AniSora](https://deepwiki.com/bilibili/Index-anisora)ï¼›[Aligning Anime Video Generation with Human Feedback](https://www.alphaxiv.org/overview/2504.10044)ï¼›[Bç«™é–‹æºSOTAå‹•ç•«å½±ç‰‡ç”Ÿæˆæ¨¡å‹Index-AniSoraï¼](https://zhuanlan.zhihu.com/p/1908150671540224717)
- 2025-04-24ï¼š[å­—ç¯€Phantom](https://github.com/Phantom-video/Phantom)ï¼š[1280x720å½±ç‰‡ç”Ÿæˆé©å‘½ï¼ä½å…ƒçµ„Phantomæ¨¡å‹å¯¦æ¸¬ï¼š10Gé¡¯å­˜æ•ˆæœä¸è¼¸æŸéˆä»˜è²»ç‰ˆ](https://zhuanlan.zhihu.com/p/1898688574477545694)
- 2025-04-22ï¼š[MAGI-1](https://github.com/SandAI-org/Magi-1)ï¼š[Sand AI å‰µæ¥­åœ˜éšŠæ¨å‡ºäº†å…¨çƒé¦–å€‹è‡ªå›æ­¸å½±ç‰‡ç”Ÿæˆå¤§æ¨¡å‹MAGI-1ï¼Œè©²æ¨¡å‹æœ‰å“ªäº›æ•ˆèƒ½äº®é»ï¼Ÿ](https://www.zhihu.com/question/1898030232184795448)
- 2025-04-22ï¼š[SkyReels V2](https://github.com/SkyworkAI/SkyReels-V2)ï¼š[å…¨çƒé¦–å€‹ç„¡é™æ™‚é•·å½±ç‰‡ç”Ÿæˆï¼æ–°æ“´æ•£æ¨¡å¼å¼•çˆ†å…†å¸‚å ´ï¼Œé›»å½±ç´šç†è§£ï¼Œå…¨é¢é–‹æº](https://www.qbitai.com/2025/04/275531.html)
- 2025-04-14ï¼š[FramePack](https://github.com/kijai/ComfyUI-FramePackWrapper)ï¼š[ä¸æ˜¯å¯éˆç”¨ä¸èµ·ï¼Œè€Œæ˜¯FramePackæ›´æœ‰æ€§åƒ¹æ¯”ï¼é–‹æºå°ˆæ¡ˆï¼š6Gé¡¯å­˜è·‘13Bæ¨¡å‹ï¼Œæ”¯æ´1åˆ†é˜å½±ç‰‡ç”¢ç”Ÿ](https://zhuanlan.zhihu.com/p/1896487969470251546)
- 2025-04-14ï¼š[fantasy-talking](https://fantasy-amap.github.io/fantasy-talking/)ï¼š[è§£è®€æœ€æ–°åŸºæ–¼Wan2.1çš„éŸ³è¨Šé©…å‹•æ•¸ä½äººFantasyTalking](https://zhuanlan.zhihu.com/p/1892895916354148118)
- 2025-03-10ï¼š[HunyuanVideo-I2V](https://github.com/Tencent/HunyuanVideo-I2V)ï¼š[é¨°è¨Šé–‹æºHunyuanVideo-I2Våœ–ç”Ÿè¦–è¨Šæ¨¡å‹+LoRAè¨“ç·´è…³æœ¬ï¼Œç¤¾ç¾¤éƒ¨ç½²ã€æ¨ç†å¯¦æˆ°æ•™å­¸ä¾†å§](https://zhuanlan.zhihu.com/p/29110060025)
- 2025-02-25ï¼š[Wan-Video](https://github.com/Wan-Video/Wan2.1)ï¼š[è¶…è¶ŠSoraï¼é˜¿é‡Œè¬ç›¸å¤§æ¨¡å‹æ­£å¼é–‹æºï¼å…¨æ¨¡æ…‹ã€å…¨å°ºå¯¸å¤§æ¨¡å‹é–‹æº](https://finance.sina.com.cn/jjxw/2025-02-26/doc-inemukxr9127437.shtml)
- 2025-02-14ï¼š[FlashVideo](https://github.com/FoundationVision/FlashVideo)ï¼š[ä¾†è‡ªä½å…ƒçµ„çš„è¦–è¨Šå¢å¼·å…¨æ–°é–‹æºæ¼”ç®—æ³•ï¼Œ102ç§’ç”¢ç”Ÿ1080Pè¦–é »](https://zhuanlan.zhihu.com/p/23702953115)
- 2025-01-28ï¼š[Sana](https://github.com/NVlabs/Sana)ï¼š[ICLR 2025 Oral] Efficient High-Resolution Image Synthesis with Linear Diffusion Transformerï¼›[æ¯”FLUXå¿«100å€ï¼è‹±å‰é”è¯æ‰‹MITã€æ¸…è¯é–‹æºè¶…å¿«AIå½±åƒç”¢ç”Ÿæ¨¡å‹](https://zhuanlan.zhihu.com/p/19489214543)
- [Flux](https://huggingface.co/black-forest-labs)
   - [Flux.1-canny-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-canny-dev)ï¼š[https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/)
   - [Flux.1-depth-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-Depth-dev)ï¼š[https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev/](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev/)
   - [Flux.1-fill-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-Fill-dev)ï¼š[https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/)
   - [Flux.1-redux-dev](https://huggingface.co/spaces/black-forest-labs/FLUX.1-Redux-dev)ï¼š[https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev/](https://huggingface.co/black-forest-labs/FLUX.1-Redux-dev/)
      - 2024-11-26ï¼š[Fluxå®˜æ–¹é‡ç¹ª+æ“´åœ–+é¢¨æ ¼åƒè€ƒ+ControlNet](https://mp.weixin.qq.com/s/Kj1nyJNTpoZ94JjO4FMw_g)
      - 2024-11-25ï¼š[æœ€æ–°flux_fill_inpaintæ¨¡å‹é«”é©—ã€‚](https://mp.weixin.qq.com/s/OPknDJXH1_oezSR86c_png)
- 2024-12-17ï¼š[Leffa](https://github.com/franciszzj/Leffa)ï¼š[Leffaï¼šMeta AI é–‹æºç²¾ç¢ºæ§åˆ¶äººç‰©å¤–è§€å’Œå§¿å‹¢çš„åœ–åƒç”Ÿæˆæ¡†æ¶ï¼Œåœ¨ç”Ÿæˆç©¿è‘—çš„åŒæ™‚ä¿æŒäººç‰©ç‰¹å¾µ](https://juejin.cn/post/7449325873725276196)
- 2024-11-29ï¼š[PuLID, Pure and Lightning ID Customization via Contrastive Alignment](https://github.com/ToTheBeginning/PuLID)ï¼š[https://github.com/balazik/ComfyUI-PuLID-Flux](https://github.com/balazik/ComfyUI-PuLID-Flux)
   - 2024-11-07ï¼š[æå®šComfyUI-PuLID-Fluxç¯€é»åªè¦é€™å¹¾æ­¥ï¼é™„ä¸€éµå£“ç¸®åŒ…](https://mp.weixin.qq.com/s/07BMFHaSasl7-PFtkN6_Zg)
   - 2024-10-08ï¼š[ä¸€æ–‡ææ‡‚PuLID FLUXäººç‰©æ›è‡‰&é¢¨æ ¼é·ç§»](https://mp.weixin.qq.com/s/V-2Cp8_xFnHQNFn35aGdLg)
- 2024-11-26ï¼š[MagicQuill](https://github.com/magic-quill/MagicQuill)ï¼š[https://huggingface.co/spaces/AI4Editing/MagicQuill](https://huggingface.co/spaces/AI4Editing/MagicQuill)
   - [MagicQuillï¼Œç™»ä¸ŠHuggingfaceè¶¨å‹¢æ¦œæ¦œé¦–çš„AI Påœ–ç¥å™¨](https://mp.weixin.qq.com/s/Pc3xRP8_9BxkVSRNznkplw)
- 2024-11-26ï¼š[OOTDiffusion](https://github.com/levihsu/OOTDiffusion)ï¼š[https://huggingface.co/spaces/levihsu/OOTDiffusion](https://huggingface.co/spaces/levihsu/OOTDiffusion)
   - [é–‹æºAIæ›è£ç¥å™¨OOTDiffusion](https://mp.weixin.qq.com/s/B2rNCjJLo8coYzoHGPnVaw)
- 2024-11-24ï¼š[Comfyui Impact Pack](https://github.com/ltdrdata/ComfyUI-Impact-Pack)
   - [Comfyui æœ€å¼·è‡‰éƒ¨ä¿®å¾©å·¥å…·Impact Pack](https://mp.weixin.qq.com/s/hNQ9BfdGbRQ_Osus-yMJWg)
- 2024-11-05ï¼š[ComfyUI OmniGen @ åŒ—äº¬äººå·¥æ™ºæ…§ç ”ç©¶é™¢](https://github.com/AIFSH/OmniGen-ComfyUI)ï¼š[https://huggingface.co/spaces/Shitao/OmniGen](https://huggingface.co/spaces/Shitao/OmniGen)
   - [ComfyUI å½±åƒç”Ÿæˆæ¨¡å‹OmniGenï¼Œäººç‰©ä¸€è‡´æ€§è™•ç†çš„ä¹Ÿå¤ªå¥½äº†](https://mp.weixin.qq.com/s/msGK0FmNs3T3jbUBHfR9DA)
   - [å…¨èƒ½å½±åƒç”Ÿæˆæ¨¡å‹OmniGenï¼šå‘Šåˆ¥ControlNetã€ipadapterç­‰æ’ä»¶ï¼Œåƒ…æ†‘æç¤ºå³å¯æ§åˆ¶å½±åƒç”Ÿæˆèˆ‡ç·¨è¼¯](https://mp.weixin.qq.com/s/48HmqRGBOK1uBdzlprdKSA)


## Digital Human (è™›æ“¬æ•¸å­—äºº)
- [HeyGem](https://github.com/GuijiAI/HeyGem.ai)ï¼š[é–‹æºæ•¸ä½äººå…‹éš†ç¥å™¨](https://zhuanlan.zhihu.com/p/29274862393)
- [Duix](https://github.com/GuijiAI/duix.ai)ï¼š[å…¨çƒé¦–å€‹çœŸäººæ•¸ä½äººï¼Œé–‹æºäº†](https://zhuanlan.zhihu.com/p/716583514)
- [Linly-Talker](https://github.com/Kedreamix/Linly-Talker)ï¼šan intelligent AI system that combines large language models (LLMs) with visual models to create a novel human-AI interaction method. 
- [EchoMimicV2](https://github.com/antgroup/echomimic_v2)ï¼š[CVPR 2025] EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation
- [Hallo3](https://github.com/fudan-generative-vision/hallo3)ï¼š[CVPR 2025] Highly Dynamic and Realistic Portrait Image Animation with Diffusion Transformer Networks
- [MimicTalk](https://github.com/yerfor/MimicTalk)ï¼š[NeurIPS 2024] MimicTalk: Mimicking a personalized and expressive 3D talking face in minutes
- [JoyGen](https://github.com/JOY-MM/JoyGen)ï¼šAudio-Driven 3D Depth-Aware Talking-Face Video Editing
- [Latentsync](https://github.com/bytedance/LatentSync)
- [MuseTalk](https://github.com/TMElyralab/MuseTalk)

## Optical Character Recognition (å…‰å­¸æ–‡å­—è­˜åˆ¥)
**[é‡å°ç‰©ä»¶æˆ–å ´æ™¯å½±åƒé€²è¡Œåˆ†æèˆ‡åµæ¸¬](https://www.twman.org/AI/CV)**
- 2025-03-05ï¼š[PP-DocBee](https://github.com/PaddlePaddle/PaddleMIX/tree/develop/deploy/ppdocbee)ï¼š[ç™¾åº¦æ¨å‡ºæ–‡ä»¶å½±åƒç†è§£PP-DocBee](https://zhuanlan.zhihu.com/p/28715553656)
- 2025-03-03ï¼š[olmocr](https://github.com/allenai/olmocr)ï¼š[ğŸš€æœ¬åœ°éƒ¨ç½²æœ€å¼ºOCRå¤§æ¨¡å‹olmOCRï¼æ”¯æŒç»“æ„åŒ–ç²¾å‡†æå–å¤æ‚PDFæ–‡ä»¶å†…å®¹ï¼](https://www.aivi.fyi/llms/deploy-olmOCR)
- 2025-02-05ï¼š[MinerU](https://github.com/opendatalab/MinerU)ï¼š[å°‡PDFè½‰æ›ç‚ºæ©Ÿå™¨å¯è®€æ ¼å¼çš„ç¥å™¨](https://mp.weixin.qq.com/s/ci5wp6gICTCtaRZfn5yWUQ)
- 2024-12-15ï¼š[markitdown](https://github.com/microsoft/markitdown)
- 2024-09-22ï¼š[OCR2.0æ—¶ä»£-GOTæ¥å•¦ï¼](https://mp.weixin.qq.com/s/W-Ult-F3pU6Wvx3fHEN8yA)
- 2024-09-11ï¼š[GOT-OCR-2.0æ¨¡å‹å¼€æº](https://mp.weixin.qq.com/s/rQL-Q0TGhT6e8Ti4zZalrg)
- 2024-08-20ï¼š[è¬ç‰©çš†å¯AIåŒ–ï¼å‰›é–‹æºå°±æœ‰12000äººåœè§€çš„OCR æƒæPDF é–‹æºå·¥å…·ï¼é‚„å¯è½‰æ›ç‚ºMarkDownï¼](https://www.53ai.com/news/MultimodalLargeModel/2024082059736.html)
- [advancedliteratemachinery/OCR/OmniParser](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/OmniParser)
- 2024-10-29ï¼š[Alibabaå‡ºå“:OmniParseré€šç”¨æ–‡æª”è¤‡é›œå ´æ™¯ä¸‹OCRæŠ½å–](https://mp.weixin.qq.com/s/_1Aatpna7poIVRhfYk4aAQ)
- [RapidOCR](https://github.com/RapidAI/RapidOCR/blob/main/docs/README_zh.md)
- [12å€‹æµè¡Œçš„é–‹æºå…è²»OCRé …ç›®](https://mp.weixin.qq.com/s/7EuhnQedAX6injBL_Dg_sQ)
- [ç”¨PaddleOCRçš„PPOCRLabelä¾†å¾®èª¿é†«ç™‚è¨ºæ–·æ›¸å’Œæ”¶æ“š](https://blog.twman.org/2023/07/wsl.html)
- [TableStructureRec: è¡¨æ ¼çµæ§‹è¾¨è­˜æ¨ç†åº«ä¾†äº†](https://zhuanlan.zhihu.com/p/668484933)ï¼šhttps://github.com/RapidAI/TableStructureRec
