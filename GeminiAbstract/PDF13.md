---
layout: default
title: Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101
---

<p align="center">
  <strong>Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101</strong>  
</p>
<p align="center">
  AI是一條孤獨且充滿惶恐及未知的旅程，花俏絢麗的收費課程或活動絕非通往成功的捷徑。<br>
  衷心感謝當時來自不同單位的AI同好參與者實名分享的寶貴經驗；如欲移除資訊還請告知。<br>
  由 <a href="https://www.twman.org/" target="_blank">TonTon Huang Ph.D.</a> 發起，及其當時任職公司(台灣雪豹科技)無償贊助場地及茶水點心。<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="180"></a>
    <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 100px !important;width: 180px !important;" ></a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101" target="_blank">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/" target="_blank">Facebook</a> |
  <a href="https://deep-learning-101.github.io/"> 回 GitHub Pages</a> |
  <a href="http://DeepLearning101.TWMAN.ORG" target="_blank">網站</a> |
  <a href="https://huggingface.co/DeepLearning101" target="_blank">Hugging Face Space</a>
</p>

# 第十三章 線性因子模型

<a href="https://www.youtube.com/watch?v=zVENYs30Ny4&" target="_blank" rel="noopener noreferrer"><i class="fab fa-youtube mr-1"></i>2017/08/11, Linear Factor Models @ Deep Learning Book Chapter 13</a><br>

**重點摘要:**
許多深度學習的研究都涉及將輸入構建為機率模型 `p_model(x)`。原則上，給定任何其他變量的情況下，這樣的模型可以使用機率推斷來預測環境中的任何變量。許多這樣的模型還具有潛變量 `h`，其中 `p_model(x) = E_h p_model(x|h)`。這些潛變量提供了表示數據的另一種方式。我們在深度前饋網路和循環網路中已經發現，基於潛變量的分布式表示繼承了表示學習的所有優點。
本章描述了一些基於潛變量的最簡單的機率模型：**線性因子模型 (linear factor model)**。這些模型有時被用來作為混合模型的組成模塊。這些約束模型考慮輸入數據的哪些部分需要被優先複製，因此它往往能學習到數據的有用特性。同時，也介紹了構建生成模型所需的許多基本方法，在此基礎上更先進的深度模型也將得到進一步擴展。
線性因子模型通過隨機線性解碼器函數來定義，該函數通過對 `h` 的線性變換以及添加噪聲來生成 `x`。
`h ~ p(h)` (公式 13.1)
`x = Wh + b + noise` (公式 13.2)
其中 `p(h)` 是一個因子分布，滿足 `p(h) = Π_i p(h_i)`，所以易於從其中採樣。噪聲通常是對角化的（在維度上是獨立的）且服從高斯分布。


**Q:** 什麼是線性因子模型的基本思想？它如何描述數據的生成過程？

**A:** 線性因子模型的基本思想是假設觀測數據 `x` 是由一組潛在的、未觀察到的因子 `h` 通過一個線性變換，再加上一些噪聲生成的。
    數據的生成過程可以描述為：
    1.  首先從一個先驗分布 `p(h)` 中抽取潛在因子 `h`。這個先驗分布通常假設是因子化的，即 `p(h) = Π_i p(h_i)`，意味著潛在因子之間是先驗獨立的。
    2.  然後，觀測數據 `x` 由潛在因子 `h` 通過一個線性變換 `Wh + b` 生成，並疊加上一些噪聲：`x = Wh + b + noise`。其中 `W` 是因子載荷矩陣，`b` 是偏置向量。

**Q:** 在線性因子模型中，潛在因子 `h` 的先驗分布 `p(h)` 通常有什麼特性？噪聲項通常假設服從什麼分布？

**A:**
*   **潛在因子 `h` 的先驗分布 `p(h)`:** 通常假設是因子化的，即 `p(h) = Π_i p(h_i)`，表示不同的潛在因子是先驗獨立的。每個 `p(h_i)` 通常是一個簡單的分布，例如標準正態分布。
*   **噪聲項:** 通常假設是獨立的（在 `x` 的不同維度上），並且服從高斯分布，例如均值為零、協方差為對角矩陣的高斯噪聲。

**Q:** 線性因子模型在機器學習中扮演什麼角色？

**A:** 線性因子模型是許多更複雜的機率模型（包括一些深度學習模型）的基礎。它們可以用於：
1.  **降維:** 如果潛在因子 `h` 的維度低於觀測數據 `x` 的維度，模型可以學習到數據的低維表示。
2.  **特徵學習:** 潛在因子 `h` 可以被視為從數據中提取出的潛在特徵。
3.  **密度估計:** 模型定義了觀測數據 `x` 的一個機率分布。
4.  **作為混合模型的組件:** 線性因子模型可以作為更複雜的混合模型的組成部分。

---

## 13.1 機率 PCA 和因子分析

**重點摘要:**
*   **機率 PCA (Probabilistic PCA, PPCA):** 與標準 PCA 不同，PPCA 是一個生成模型。它假設潛在因子 `h` 服從標準正態分布 `N(h; 0, I)`，觀測數據 `x` 在給定 `h` 時服從高斯分布 `x ~ N(x; Wh + b, σ^2I)` (公式 13.5)，即噪聲是各向同性的高斯噪聲。
*   **因子分析 (Factor Analysis, FA):** 與 PPCA 非常相似，但允許噪聲的協方差矩陣是任意的對角矩陣 `ψ = diag(σ^2)` (公式 13.4)，即每個觀測維度可以有不同的噪聲方差。潛在因子 `h` 仍然服從標準正態分布 `N(h; 0, I)`。FA 假設觀測數據 `x` 在給定 `h` 時，其各個維度 `x_i` 是條件獨立的。
這兩種模型都假設了潛在變量和噪聲的高斯性。學習這些模型的參數（如 `W`, `b`, `σ^2` 或 `ψ`）通常可以使用期望最大化 (EM) 算法。


**Q:** 機率 PCA (PPCA) 和因子分析 (FA) 在模型假設上有什麼共同點和主要區別？

**A:**
*   **共同點:**
    1.  兩者都是線性因子模型。
    2.  兩者都假設潛在因子 `h` 服從標準正態分布，即 `h ~ N(0, I)`。
    3.  兩者都假設觀測數據 `x` 是由潛在因子 `h` 的線性變換加上高斯噪聲生成的。
*   **主要區別:** 區別在於對噪聲協方差矩陣的假設：
    1.  **PPCA:** 假設噪聲是各向同性的 (isotropic) 高斯噪聲，即噪聲的協方差矩陣是 `σ^2I`，其中 `σ^2` 是一個標量，`I` 是單位矩陣。這意味著所有觀測維度上的噪聲方差都相同。
    2.  **因子分析 (FA):** 假設噪聲的協方差矩陣是對角的，但不一定是各向同性的，即 `ψ = diag(σ_1^2, σ_2^2, ..., σ_n^2)`。這意味著每個觀測維度 `x_i` 可以有其自身的噪聲方差 `σ_i^2`，但不同維度之間的噪聲是獨立的。

**Q:** 在因子分析中，「觀測數據的條件獨立性」指的是什麼？

**A:** 指的是在給定潛在因子 `h` 的條件下，觀測數據 `x` 的各個維度 `x_i` 之間是相互獨立的。這是因為因子分析假設噪聲的協方差矩陣 `ψ` 是對角的，這意味著噪聲在不同維度上是不相關的。由於 `x = Wh + b + noise`，並且 `h` 是給定的，所以 `x` 的各維度之間的相關性完全由 `Wh` 決定，而噪聲的獨立性保證了在給定 `h` 之後，`x_i` 僅依賴於 `h` 和其自身的噪聲，與其他 `x_j` (j≠i) 的噪聲無關。

**Q:** 學習 PPCA 和因子分析模型的參數通常使用什麼算法？

**A:** 學習這些模型的參數（例如，因子載荷矩陣 `W`、偏置 `b` 以及噪聲方差參數 `σ^2` 或 `ψ`）通常使用**期望最大化 (Expectation-Maximization, EM)** 算法。EM 算法是一種迭代算法，用於在具有潛變量的機率模型中尋找參數的最大概似估計。

---

## 13.2 獨立成分分析

**重點摘要:**
獨立成分分析 (Independent Component Analysis, ICA) 是一種線性因子模型，其目標是將觀測到的多維信號分解為若干個統計上**獨立的非高斯**源信號的線性混合。與 PCA 和因子分析不同，ICA 假設潛在因子 `h`（即源信號）是**非高斯的且相互獨立**。觀測數據 `x` 仍然被建模為 `x = Wh`（這裡忽略偏置和可加性噪聲以簡化，但也可以包含）。ICA 的核心在於，只有當源信號是非高斯時，才能唯一地從混合信號中分離出它們（除了尺度和順序的不確定性）。
學習 ICA 模型通常涉及到最大化觀測數據的概似，或者等價地最大化某種衡量源信號獨立性的度量（如負熵或峰度）。與 PCA 不同，ICA 尋找的不是使方差最大化的方向，而是使輸出的獨立成分的非高斯性最大化的方向。非線性 ICA (NICE) 是 ICA 的一個非線性擴展。


**Q:** 獨立成分分析 (ICA) 與 PCA 和因子分析在對潛在因子（源信號）的假設上有何根本不同？

**A:** 根本不同在於對潛在因子 `h` 的分布假設：
*   **PCA 和因子分析:** 假設潛在因子 `h` 服從高斯分布（通常是標準正態分布）。
*   **ICA:** 假設潛在因子 `h`（即獨立成分或源信號）是**非高斯的**，並且是**統計上相互獨立的**。

**Q:** 為什麼 ICA 要求源信號是非高斯的？

**A:** 根據中心極限定理，多個獨立隨機變量的和的分布趨向于高斯分布，無論原始變量的分布是什麼。如果源信號本身就是高斯的，那麼它們的任意線性混合仍然是高斯的。在這種情況下，有無限多種方式可以將混合信號分解為高斯源信號的線性組合，從而無法唯一地恢復原始的獨立源信號。
    只有當源信號是非高斯時，它們的線性混合才會保留一些非高斯性，使得我們可以通過尋找最大化非高斯性的投影方向來分離這些獨立成分。

**Q:** ICA 的目標是什麼？它試圖從觀測數據中恢復什麼？

**A:** ICA 的目標是將觀測到的多維數據 `x` 分解為一組統計上相互獨立的潛在源信號 `h` 的線性混合。它試圖從混合信號 `x` 中恢復出這些原始的、獨立的源信號 `h` 以及混合矩陣 `W`（或其逆，分離矩陣）。

**Q:** 學習 ICA 模型常用的準則是什麼？

**A:** 學習 ICA 模型常用的準則包括：
1.  **最大化概似:** 假設源信號 `h_i` 的非高斯密度函數 `p(h_i)` 已知或被參數化，然後最大化觀測數據 `x` 的對數概似 `log p(x)`。
2.  **最大化非高斯性:** 由於高斯變量的熵在所有具有相同方差的變量中是最大的，可以通過最大化某種衡量源信號非高斯性的度量來實現分離。常用的度量包括：
    *   **負熵 (Negentropy):** 衡量一個變量與具有相同協方差矩陣的高斯變量之間的熵差。
    *   **峰度 (Kurtosis):** 四階矩，用於衡量分布的「尖峭」程度。超高斯分布（比高斯更尖）的峰度為正，亞高斯分布（比高斯更平坦）的峰度為負。
3.  **最小化互信息 (Mutual Information):** 最小化估計出的源信號之間的互信息，以使它們盡可能獨立。

---

## 13.3 慢特徵分析

**重點摘要:**
慢特徵分析 (Slow Feature Analysis, SFA) 是一種從時間信號中學習不變特徵的線性因子模型。其核心思想（慢度原則, slowness principle）是，場景中描述物體的單個量或重要特徵的變化通常比構成場景的原始感官輸入（如像素值）的變化慢得多。SFA 旨在找到輸入信號的線性變換 `f(x^(t)))`，使得輸出的特徵 `f(x^(t)))` 隨時間變化盡可能緩慢，同時滿足一些約束條件（如輸出零均值、單位方差以及不同特徵之間去相關）。
其目標函數通常是最小化輸出特徵在相鄰時間步之間的平方差的期望 (公式 13.8)，同時滿足約束 (公式 13.9, 13.10, 13.11)。SFA 可以通過廣義特徵值問題來求解。它可以被看作是一種學習時間上緩慢變化的不變表示的方法。


**Q:** 什麼是慢特徵分析 (SFA)？它的核心思想（慢度原則）是什麼？

**A:** 慢特徵分析 (SFA) 是一種無監督學習算法，旨在從快速變化的輸入時間序列中提取出變化緩慢的特徵。
    其核心思想是**慢度原則 (slowness principle)**：在一個動態環境中，描述環境本質屬性或重要對象的特徵通常比原始感官輸入（例如，視頻中的像素值）隨時間變化得更緩慢。SFA 的目標就是找到輸入信號的這樣一些變換，使得變換後的特徵隨時間變化盡可能慢。

**Q:** SFA 的目標函數通常是什麼形式？它需要滿足哪些約束條件？

**A:** SFA 的目標函數通常是最小化輸出特徵 `f(x^(t)))_i` 在相鄰時間步 `t` 和 `t+1` 之間的平方差的期望（對於每個特徵 `i`）：
`min_θ E_t[ (f(x^(t+1);θ)_i - f(x^(t);θ)_i)^2 ]` (公式 13.8)
    同時，為了得到有意義且非平凡的解，SFA 通常需要滿足以下約束條件：
    1.  **零均值:** `E_t[ f(x^(t);θ)_i ] = 0` (公式 13.9)
    2.  **單位方差:** `E_t[ (f(x^(t);θ)_i)^2 ] = 1` (公式 13.10)
    3.  **去相關 (Decorrelation):** 不同特徵之間是不相關的，`E_t[ f(x^(t);θ)_i f(x^(t);θ)_j ] = 0`  對於 `i ≠ j` (公式 13.11)
    這些約束確保了學習到的特徵是有信息的，並且是按其「慢度」排序的。

**Q:** SFA 學習到的特徵具有什麼樣的特性？它在什麼樣的場景中可能有用？

**A:** SFA 學習到的特徵具有**時間上的緩慢變化性**和**對快速變化的輸入的不變性**。它們捕捉了輸入信號中隨時間持續存在的、相對穩定的方面。
    SFA 在以下場景中可能有用：
    1.  **從視覺輸入中學習不變特徵:** 例如，從一個移動物體的視頻中學習該物體的身份（變化緩慢）而不是其精確位置或姿態（變化較快）。
    2.  **機器人導航:** 學習環境中穩定不變的地標。
    3.  **語音處理:** 提取語音信號中變化較慢的語義信息。
    4.  **任何需要從快速變化的時間序列中提取緩慢變化的潛在狀態或不變表示的任務。**

---

## 13.4 稀疏編碼

**重點摘要:**
稀疏編碼 (Sparse Coding) (Olshausen and Field, 1996) 是一種線性因子模型，已被廣泛研究作為一種無監督特徵學習和特徵提取機制。其核心思想是，輸入數據 `x` 可以由一個「超完備」的基向量（字典）的稀疏線性組合來表示。觀測模型通常是 `x = Wh + noise` (公式 13.12)，但與 PCA 或因子分析不同，潛在編碼 `h` 的先驗 `p(h)` 被選擇為能夠誘導稀疏性的分布，例如 Laplace 分布 `p(h_i) ∝ exp(-λ|h_i|)` (公式 13.13) 或 Student-t 分布。
學習稀疏編碼模型通常涉及兩個交替的步驟：
1.  **推斷潛在編碼 `h`:** 給定字典 `W` 和輸入 `x`，找到最優的稀疏編碼 `h*`。這通常通過最小化一個包含重構誤差項和稀疏懲罰項的目標函數來實現，例如 `argmin_h ||x - Wh||^2 + λ||h||_1` (公式 13.18)。
2.  **更新字典 `W`:** 給定稀疏編碼 `h`，更新字典 `W` 以更好地重構數據。
由於推斷 `h` 的步驟（即使對於給定的 `W`）通常沒有閉式解（因為稀疏先驗的非高斯性），稀疏編碼的學習比 PCA 更複雜。稀疏編碼學習到的字典基通常類似於視覺皮層 V1 區域簡單細胞的感受野，能夠有效地表示自然圖像的局部結構。


**Q:** 什麼是稀疏編碼？它的核心思想是什麼？

**A:** 稀疏編碼是一種表示學習方法，其核心思想是找到一組「超完備」的基向量（稱為字典 `W`），使得輸入數據 `x` 可以被表示為這些基向量的一個**稀疏**線性組合。也就是說，對於每個輸入 `x`，其對應的編碼 `h`（即線性組合的係數）中的大部分元素都應該是零或接近零，只有少數元素具有顯著的非零值。觀測模型通常是 `x ≈ Wh`。

**Q:** 稀疏編碼模型中的潛在編碼 `h` 的先驗分布 `p(h)` 與 PCA 或因子分析有何不同？它通常選擇什麼樣的分布？

**A:**
*   **PCA/因子分析:** 潛在編碼 `h` 的先驗通常假設為高斯分布（通常是標準正態分布），這種分布是稠密的，不誘導稀疏性。
*   **稀疏編碼:** 潛在編碼 `h` 的先驗被選擇為能夠誘導稀疏性的分布。常用的選擇包括：
    *   **Laplace 分布 (L1 先驗):** `p(h_i) ∝ exp(-λ|h_i|)`。這種先驗在零點處有一個尖峰，傾向於使許多 `h_i` 的值恰好為零。
    *   **Student-t 分布:** 這是一種重尾分布，也具有促進稀疏性的特性。
    *   其他在零點附近具有高機率密度、尾部較重的分布。

**Q:** 學習稀疏編碼模型通常包含哪兩個主要步驟？為什麼說它的學習比 PCA 更複雜？

**A:** 學習稀疏編碼模型通常是一個迭代的過程，包含兩個交替的步驟：
1.  **推斷稀疏編碼 `h` (Inference/Sparse Coding Step):** 給定當前的字典 `W` 和一個輸入數據點 `x`，找到最優的稀疏編碼 `h*`，使得 `x` 可以通過 `Wh*` 很好地重構，同時 `h*` 本身是稀疏的。這通常通過最小化一個目標函數來實現，該目標函數包含重構誤差項（如 `||x - Wh||^2`）和一個稀疏懲罰項（如 `λ||h||_1`）。
2.  **更新字典 `W` (Dictionary Update Step):** 給定當前所有數據點的稀疏編碼 `H`，更新字典 `W`，使其能夠最好地從這些稀疏編碼中重構原始數據。
    它的學習比 PCA 更複雜，主要是因為**推斷稀疏編碼 `h` 的步驟通常沒有閉式解**（不像 PCA 中可以直接通過特徵值分解得到主成分）。由於稀疏先驗（如 Laplace 先驗）的非二次型特性，最小化關於 `h` 的目標函數通常需要使用迭代的數值優化算法（如迭代軟閾值算法 ISTA，或者基於 LARS/OMP 的方法）。

**Q:** 稀疏編碼學習到的字典基（或特徵）通常具有什麼樣的特性？一個經典的例子是什麼？

**A:** 稀疏編碼學習到的字典基通常是**局部的、定向的、帶通的 (localized, oriented, and bandpass)**，類似於邊緣檢測器或 Gabor 濾波器。它們能夠有效地捕捉輸入數據中的局部結構。
    一個經典的例子是，當在自然圖像塊上訓練稀疏編碼時，學習到的字典基向量（或特徵）看起來非常像哺乳動物初级視覺皮層 (V1) 中簡單細胞的感受野。

---

## 13.5 PCA 的流形解釋

**重點摘要:**
線性因子模型，包括 PCA 和因子分析，可以被理解為學習一個數據流形。PCA 可以被視為將高維數據投影到一個低維的線性子空間（主子空間），這個子空間可以被看作是對數據所在流形的一個局部線性近似。
*   **編碼器:** `h = f(x) = W^T(x-μ)` (公式 13.19)
*   **解碼器:** `x̂ = g(h) = Wh + μ` (公式 13.20)
PCA 最小化重構誤差 `E[||x - x̂||^2]` (公式 13.21)。這個重構誤差等於被丟棄的特徵值之和 `Σ_{i=d+1}^D λ_i` (公式 13.23)，其中 `d` 是主子空間的維度。
圖 13.3 展示了平坦的高斯「薄餅」如何近似一個低維流形附近的機率密度。PCA 找到了最好地擬合這個「薄餅」的低維線性子空間。


**Q:** 如何從流形學習的角度來解釋主成分分析 (PCA)？

**A:** 從流形學習的角度來看，PCA 可以被視為試圖找到一個能夠最好地（在均方誤差意義下）近似數據所在流形的**低維線性子空間（或仿射子空間，如果考慮均值）**。
    如果數據點大致分佈在一個嵌入在高維空間中的、相對「平坦」的低維流形上，那麼 PCA 找到的主成分子空間就是對這個流形的一個線性逼近。編碼過程是將數據點投影到這個子空間上，而解碼過程是從這個子空間上的表示重構回原始高維空間中的點（重構點將位於這個線性子空間內）。

**Q:** 在 PCA 中，編碼器和解碼器是如何定義的？

**A:**
*   **編碼器 (Encoder) `f(x)`:** 將原始數據點 `x` 投影到由前 `d` 個主成分張成的主子空間上，得到低維表示 `h`。如果 `W` 的列是按特徵值降序排列的主成分向量，`μ` 是數據的均值，則編碼器可以表示為：
    `h = f(x) = W_d^T (x - μ)`
    其中 `W_d` 是包含前 `d` 個主成分的矩陣。
*   **解碼器 (Decoder) `g(h)`:** 從低維表示 `h` 重構回原始數據空間中的點 `x̂`。重構點位於由主成分張成的子空間中：
    `x̂ = g(h) = W_d h + μ`

**Q:** PCA 的重構誤差與數據的協方差矩陣的特徵值有什麼關係？

**A:** PCA 的均方重構誤差等於數據協方差矩陣中那些被丟棄的（較小的）特徵值之和。如果我們保留了 `d` 個主成分（對應最大的 `d` 個特徵值 `λ_1, ..., λ_d`），並丟棄了剩餘的 `D-d` 個主成分（對應較小的特徵值 `λ_{d+1}, ..., λ_D`），那麼均方重構誤差為：
`E[||x - x̂||^2] = Σ_{i=d+1}^D λ_i` (公式 13.23)
這意味著 PCA 通過保留方差最大的方向（對應最大的特徵值）來最小化重構誤差。

**Q:** 為什麼說線性因子模型是「最簡單的生成模型和學習數據表示的最簡單模型」？

**A:** 從某種程度上說，線性因子模型（如 PCA、因子分析）可以被視為最簡單的生成模型，因為它們對潛在因子和觀測數據之間的關係（線性變換）以及潛在因子和噪聲的分布（通常是高斯）都做了非常簡單的假設。它們也是學習數據表示的最簡單模型，因為它們主要捕捉數據的線性和二階統計特性（如協方差）。
    儘管簡單，它們可以作為更複雜的非線性模型（如自編碼器網路和深度機率模型）的基礎和起點，這些更強大和更靈活的模型可以從線性因子模型擴展而來。

---

希望這些詳細的摘要和Q&A對您有所幫助！