---
layout: default
title: Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101
---

<p align="center">
  <strong>Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101</strong>  
</p>
<p align="center">
  AI是一條孤獨且充滿惶恐及未知的旅程，花俏絢麗的收費課程或活動絕非通往成功的捷徑。<br>
  衷心感謝當時來自不同單位的AI同好參與者實名分享的寶貴經驗；如欲移除資訊還請告知。<br>
  由 <a href="https://www.twman.org/" target="_blank">TonTon Huang Ph.D.</a> 發起，及其當時任職公司(台灣雪豹科技)無償贊助場地及茶水點心。<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="180"></a>
    <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 100px !important;width: 180px !important;" ></a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101" target="_blank">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/" target="_blank">Facebook</a> |
  <a href="https://deep-learning-101.github.io/"> 回 GitHub Pages</a> |
  <a href="http://DeepLearning101.TWMAN.ORG" target="_blank">網站</a> |
  <a href="https://huggingface.co/DeepLearning101" target="_blank">Hugging Face Space</a>
</p>

# 第十六章 深度學習中的結構化機率模型

<a href="https://www.youtube.com/watch?v=n0rBS3sAqI0" target="_blank" rel="noopener noreferrer"><i class="fab fa-youtube mr-1"></i>017/10/27, Structured Probabilistic Models @ Deep Learning Book Chapter 16</a><br>


**重點摘要:**
深度學習為研究者提供了許多建模方式，用於設計和描述算法。其中一種形式是基於**結構化機率模型 (structured probabilistic model)** 的思想。本章旨在理解如何使用圖模型作為描述機率模型中許多變數之間依賴關係的語言，以及如何將這些思想應用於深度學習。
結構化機率模型使用圖來描述機率分佈中隨機變數之間的直接相互作用，從而描述一個完整的機率分佈。圖中的節點代表隨機變數，邊代表直接相互作用。由於模型結構是由圖定義的，所以這些模型也常被稱為**圖模型 (graphical model)**。
本章將介紹圖模型中幾個核心方法的基本背景，並重點描述已被證明對深度學習社群最有用的觀點。主要內容包括：描述模型結構、學習算法和推斷過程。首先介紹構建大規模模型時面臨的挑戰，然後介紹如何使用圖來描述機率分佈的結構。


**Q:** 什麼是結構化機率模型？它使用什麼工具來描述變數間的相互作用？

**A:** 結構化機率模型是一種使用圖 (graph) 來描述機率分佈中多個隨機變數之間直接相互作用的建模方式。圖中的節點代表隨機變數，邊代表這些變數之間的直接相互作用。

**Q:** 為什麼結構化機率模型也被稱為圖模型？

**A:** 因為這些模型的結構是由一個圖來定義和可視化的，圖清晰地展示了變數之間的依賴關係。

**Q:** 本章將涵蓋圖模型的哪些核心方面？

**A:** 本章將涵蓋圖模型的核心方面，包括：
1.  描述模型結構的方法。
2.  相關的學習算法。
3.  推斷過程。
4.  構建大規模模型時的挑戰。
5.  如何使用圖來描述機率分佈的結構。

---

## 16.1 非結構化建模的挑戰

**重點摘要:**
深度學習的目標是使得機器能夠學習解決許多人工智能中需要解決的挑戰，這也意味著它們能夠理解具有豐富結構的高維數據。例如，自然圖片、語音信號和包含許多詞和句點的文本。
當我們嘗試用機率模型處理這些高維數據時，會遇到一些非結構化建模的固有挑戰：
1.  **估計密度函數:** 給定一個輸入 `x`，模型需要返回對數據生成真實密度函數 `p(x)` 的估計。這需要處理高維空間，即使只量化到很少的級別，可能狀態的數量也會非常巨大。
2.  **去噪:** 給定受損的觀察輸入 `x̃`，模型返回一個對原始真實 `x` 的估計。這需要從大量可能的原始 `x` 中去除噪聲，並理解哪些部分是噪聲，哪些是信號。
3.  **缺失值的填補:** 給定 `x` 的某些元素作為觀察值，模型被要求返回一個或一些未觀察到的元素的估計或機率分佈。這需要模型理解變數間的依賴關係。
4.  **採樣:** 模型從 `p(x)` 中抽取新的樣本。這需要模型能夠捕捉數據的整體結構和多樣性。

這些任務在沒有結構化假設的情況下都非常困難，主要原因包括：
*   **內存:** 存儲高維分佈的表格形式表示需要巨大的內存空間 (維數災難)。
*   **統計的高效性:** 參數數量增加時，所需的訓練數據量也相應增加，以準確地估計這些參數。
*   **運行時間 (推斷):** 許多操作（如邊緣化、條件化）在高維空間中計算成本極高。
*   **運行時間 (採樣):** 從複雜的高維分佈中採樣通常很困難。

結構化機率模型通過顯式地描述變數子集所產生的每一種可能的相互作用類型，來應對這些挑戰。這使得模型可以用較少數量的參數對複雜依賴關係進行建模和計算。


**Q:** 在處理高維數據時，非結構化機率建模面臨哪些主要的挑戰？

**A:** 主要挑戰包括：
1.  **密度估計的困難:** 在高維空間中，準確估計機率密度函數 `p(x)` 非常困難，因為狀態空間巨大。
2.  **去噪的困難:** 從損壞的輸入中恢復原始信號需要模型理解信號和噪聲的複雜特性。
3.  **缺失值填補的困難:** 準確填補缺失值需要模型理解變數之間的依賴關係。
4.  **採樣的困難:** 從複雜的高維分佈中生成新的、多樣化的樣本具有挑戰性。

**Q:** 這些挑戰背後的根本原因是什麼？（即維數災難的體現）

**A:** 根本原因可以歸結為「維數災難」的幾個方面：
1.  **內存開銷:** 存儲高維空間的完整機率表（如果變數是離散的）需要指數級的內存。
2.  **統計高效性差:** 隨著維度（或模型參數數量）的增加，需要指數級增長的訓練數據才能可靠地估計模型參數。
3.  **運行時間開銷 (推斷):** 許多推斷操作（如計算邊緣機率或條件機率）在高維空間中計算複雜度極高。
4.  **運行時間開銷 (採樣):** 從高維的複雜分佈中直接採樣通常是不可行的。

**Q:** 結構化機率模型如何幫助應對這些挑戰？

**A:** 結構化機率模型通過對變數之間的依賴關係進行顯式的、通常是稀疏的假設（用圖結構表示），來應對這些挑戰。它允許模型用更少的參數來表示複雜的依賴關係，從而減少了內存需求、提高了統計效率，並可能簡化推斷和採樣的計算過程。

---

## 16.2 使用圖描述模型結構

**重點摘要:**
結構化機率模型使用圖（節點表示隨機變數，邊表示直接相互作用）來描述機率分佈的結構。圖模型可以大致分為兩類：有向圖模型（貝葉斯網路）和無向圖模型（馬爾可夫隨機場）。

---

### 16.2.1 有向模型

**重點摘要:**
有向圖模型 (Directed Graphical Model)，也稱為信念網路 (Belief Network) 或貝葉斯網路 (Bayesian Network)，使用有向無環圖 (DAG) 來表示變數之間的因果關係或條件依賴關係。圖中的箭頭從父節點指向子節點，表示子節點的機率分佈直接依賴於其父節點。
一個有向圖模型 `G` 中所有變數的聯合機率分佈可以分解為每個變數 `x_i` 在其父節點 `Pa_G(x_i)` 條件下的局部條件機率分佈 (local conditional probability distribution) 的乘積：
`p(x) = Π_i p(x_i | Pa_G(x_i))` (公式 16.1)
這種分解極大地簡化了高維聯合機率分佈的表示和計算，因為它將全局依賴分解為局部依賴。例如，在賽跑的例子中 (圖 16.2)，Bob 的完成時間 `t_1` 只依賴於 Alice 的完成時間 `t_0`，Carol 的完成時間 `t_2` 只依賴於 Bob 的完成時間 `t_1`，則聯合機率 `p(t_0, t_1, t_2) = p(t_0)p(t_1|t_0)p(t_2|t_1)` (公式 16.2)。這種結構大大減少了需要存儲和計算的參數數量。


**Q:** 什麼是有向圖模型（或貝葉斯網路）？它使用什麼樣的圖結構？

**A:** 有向圖模型是一種結構化機率模型，它使用有向無環圖 (Directed Acyclic Graph, DAG) 來表示一組隨機變數之間的條件依賴關係。圖中的節點代表隨機變數，有向邊（箭頭）從父節點指向子節點，表示子節點的機率直接依賴於其父節點。

**Q:** 有向圖模型如何分解聯合機率分佈？這個分解公式是什麼？

**A:** 有向圖模型將所有變數的聯合機率分佈分解為每個變數在其父節點條件下的局部條件機率分佈的乘積。如果 `Pa_G(x_i)` 表示變數 `x_i` 在圖 `G` 中的父節點集合，則聯合機率為：
`p(x) = Π_i p(x_i | Pa_G(x_i))` (公式 16.1)

**Q:** 相比於完全聯合機率表，有向圖模型的分解表示有什麼優勢？

**A:** 主要優勢在於**參數數量的減少**和**計算效率的提高**。
*   **參數減少:** 它只需要為每個變數指定其關於其直接父節點的條件機率表，而不需要存儲涵蓋所有變數組合的完整聯合機率表。如果父節點數量遠小於總變數數量，參數數量會大大減少。
*   **計算效率:** 許多推斷和學習算法可以利用這種分解結構來進行更高效的計算。例如，計算邊緣機率或條件機率可以通過局部計算和消息傳遞來實現，而不是對整個聯合分佈進行操作。

---

### 16.2.2 無向模型

**重點摘要:**
無向圖模型 (Undirected Graphical Model)，也稱為馬爾可夫隨機場 (Markov Random Field, MRF) 或馬爾可夫網路 (Markov Network)，使用無向圖來表示變數之間的對稱相互作用。與有向模型不同，邊沒有方向性，表示變數之間存在關聯，但不直接指定因果方向。
無向模型更適合表示某些情況，例如當相互作用本質上是對稱的，或者很難確定因果方向時。例如，考慮室友、你和同事之間是否生病的例子，疾病的傳播是雙向的。
在這種情況下，如果你的室友和你的同事都會影響你是否生病，你也會影響他們，那麼用無向圖更合適。一個重要的概念是**團 (clique)**，即圖中任意兩個節點之間都有邊連接的節點子集。無向模型的聯合機率分佈通常通過定義在圖的團上的**因子 (factor)** 或**勢函數 (potential function)** `φ(C)` 的乘積來參數化：
`p̃(x) = Π_{C∈G} φ(C)` (公式 16.3)
這裡 `p̃(x)` 是一個未歸一化的機率度量，需要除以一個歸一化常數（配分函數）才能得到有效的機率分佈。


**Q:** 什麼是無向圖模型（或馬爾可夫隨機場）？它與有向圖模型在邊的表示上有何不同？

**A:** 無向圖模型是一種結構化機率模型，它使用無向圖來表示一組隨機變數之間的相互作用或關聯。與有向圖模型不同，無向圖模型中的邊沒有方向，表示變數之間存在對稱的依賴關係，而不指定因果方向。

**Q:** 在什麼情況下，使用無向圖模型比有向圖模型更合適？

**A:** 當變數之間的相互作用本質上是對稱的，或者很難或不自然地指定因果方向時，使用無向圖模型更合適。例如，在描述物理系統（如伊辛模型中相鄰自旋的相互作用）或某些圖像模型（如像素鄰域的相互作用）時，無向模型更為自然。書中舉的例子是室友、你和同事之間疾病傳播的相互影響。

**Q:** 無向圖模型的聯合機率分佈通常如何通過「團」和「因子（勢函數）」來定義？

**A:** 在無向圖模型中，聯合機率分佈（通常是未歸一化的形式 `p̃(x)`）被定義為圖中所有（通常是最大）團 (cliques) 上的勢函數 (potential functions) 或因子 (factors) `φ(C)` 的乘積：
`p̃(x) = Π_{C∈G} φ(C)` (公式 16.3)
其中，一個團 `C` 是圖中節點的一個子集，該子集中的任意兩個節點之間都有一條邊。勢函數 `φ(C)` 是定義在團 `C` 所包含的變數上的非負函數，它衡量了這些變數處於特定狀態組合的「相容性」或「能量」。

---

### 16.2.3 配分函數

**重點摘要:**
無向圖模型通過因子乘積定義的 `p̃(x)` 通常不是一個歸一化的機率分佈（即其總和或積分不為1）。為了得到一個有效的機率分佈 `p(x)`，我們需要將其除以一個歸一化常數，稱為**配分函數 (partition function)** `Z`：
`p(x) = (1/Z) p̃(x)` (公式 16.4)
其中 `Z = ∫ p̃(x) dx` (對於連續變數) 或 `Z = Σ_x p̃(x)` (對於離散變數) (公式 16.5)。
配分函數 `Z` 通常是難以計算的，因為它涉及到對所有可能狀態的求和或積分。這使得無向模型的學習和推斷（特別是計算邊緣機率或條件機率）比有向模型更具挑戰性。即使對於一個簡單的二次勢函數 `φ(x) = x^2`，如果沒有對 `x` 的範圍做限制，`Z` 也可能發散 (公式 16.6)。


**Q:** 什麼是配分函數 (partition function) `Z`？為什麼在無向圖模型中通常需要它？

**A:** 配分函數 `Z` 是一個歸一化常數，定義為無向圖模型中所有團勢函數乘積 `p̃(x)` 在所有可能狀態 `x` 上的總和（對於離散變數）或積分（對於連續變數）。
在無向圖模型中通常需要它，因為通過團勢函數乘積定義的 `p̃(x)` 本身並不保證是一個有效的機率分佈（即其總和或積分不一定等於1）。將 `p̃(x)` 除以 `Z` 才能得到一個有效的、歸一化的機率分佈 `p(x) = (1/Z)p̃(x)`。

**Q:** 為什麼計算配分函數 `Z` 通常很困難？這對無向模型的應用帶來了什麼挑戰？

**A:** 計算配分函數 `Z` 通常很困難，因為它涉及到對模型中所有變數的所有可能狀態組合進行求和或積分。如果變數數量很多，或者每個變數的狀態空間很大，那麼這個求和或積分的計算量會呈指數級增長，變得難以處理 (intractable)。
這對無向模型的應用帶來了以下挑戰：
1.  **學習困難:** 許多學習算法（如最大概似學習）的目標函數或梯度計算會涉及到配分函數 `Z` 或其導數，如果 `Z` 難以計算，這些算法就很難直接應用。
2.  **推斷困難:** 計算邊緣機率 `p(x_A)` 或條件機率 `p(x_A|x_B)` 通常也需要計算（部分的）配分函數，因此也會變得困難。
3.  **模型評估困難:** 評估模型在測試數據上的概似值需要歸一化的機率，如果 `Z` 未知，則難以進行。

---

### 16.2.4 基於能量的模型

**重點摘要:**
許多無向圖模型都採用一個方便的理論框架，即所有因子 `φ(C)` 都是嚴格正的。這允許我們使用能量函數 (energy function) `E(x)` 來定義未歸一化的機率：
`p̃(x) = exp(-E(x))` (公式 16.7)
這種模型被稱為**基於能量的模型 (Energy-Based Model, EBM)**。配分函數是所有狀態的玻爾茲曼因子 `exp(-E(x))` 之和或積分。學習的目標是修改能量函數，使得期望的狀態具有低能量，不期望的狀態具有高能量。**玻爾茲曼機 (Boltzmann Machine)** 是基於能量的模型的早期重要例子，其名稱也使得基於能量的模型有時被稱為玻爾茲曼分佈。
在無向模型中，通常將多個因子的貢獻組合到一個全局能量函數中，例如通過加法：`E(x) = Σ_i E_i(x)`，這對應於因子 `p̃_i(x) = exp(-E_i(x))` 的乘積 `p̃(x) = Π_i p̃_i(x)`。一個例子是**專家乘積 (product of experts)** 模型。


**Q:** 什麼是基於能量的模型 (EBM)？它的未歸一化機率如何通過能量函數定義？

**A:** 基於能量的模型 (EBM) 是一種無向圖模型，它通過一個稱為能量函數 `E(x)` 的標量函數來定義（未歸一化的）機率分佈。其未歸一化機率與能量的負指數成正比：
`p̃(x) = exp(-E(x))` (公式 16.7)
歸一化的機率則是 `p(x) = (1/Z)exp(-E(x))`，其中 `Z` 是配分函數。

**Q:** 在基於能量的模型中，學習的目標是什麼？能量函數的高低與狀態的期望程度有何關係？

**A:** 學習的目標是調整能量函數 `E(x)`，使得：
*   **期望的狀態（例如訓練數據中觀察到的狀態）具有較低的能量值。** 低能量對應高機率。
*   **不期望的狀態具有較高的能量值。** 高能量對應低機率。
模型試圖通過能量函數來塑造一個「能量景觀」，將機率質量集中在期望的區域。

**Q:** 玻爾茲曼機與基於能量的模型是什麼關係？

**A:** 玻爾茲曼機是基於能量的模型的早期且非常重要的一個例子。它明確地使用能量函數來定義狀態的機率。由於玻爾茲曼機的影響，`p(x) ∝ exp(-E(x))` 形式的分佈有時也被稱為玻爾茲曼分佈。

**Q:** 什麼是「專家乘積 (product of experts)」模型？它如何與能量函數的加法相聯繫？

**A:** 專家乘積模型是一種將多個簡單的「專家」模型（每個專家定義一個機率分佈或一個因子）的輸出相乘來形成一個更複雜的組合模型的方法。如果每個專家 `i` 定義一個因子 `p̃_i(x) = exp(-E_i(x))`，那麼它們的乘積 `p̃(x) = Π_i p̃_i(x)` 對應於一個總能量函數，該函數是各個專家能量函數之和：`E(x) = Σ_i E_i(x)`。每個專家可以看作是對數據施加一個軟約束，總能量低的狀態是滿足最多（或最重要）約束的狀態。

---

### 16.2.5 分離和d-分離

**重點摘要:**
圖模型的一個重要用途是判斷變數之間的條件獨立性。
*   **無向模型中的分離 (Separation):** 如果在一組給定的觀察變數 `S` 的條件下，從變數集 `A` 到變數集 `B` 的所有路徑都被 `S` 中的節點「阻斷」(block)，那麼 `A` 和 `B` 在給定 `S` 時是條件獨立的。如果 `S` 為空，則看 `A` 和 `B` 之間是否存在路徑；若不存在，則它們是無條件獨立的。
*   **有向模型中的d-分離 (d-Separation):** d-分離的概念更複雜，因為有向邊引入了方向性。一條路徑被觀察變數集 `Z` d-分離（或阻斷），如果滿足以下任一條件：
    1.  路徑包含一個鏈 `i → m → j` 或一個分叉 `i ← m → j`，且 `m` 在 `Z` 中。
    2.  路徑包含一個**對撞 (collider)** `i → m ← j`，且 `m` 不在 `Z` 中，並且 `m` 的任何後代都不在 `Z` 中。
    如果從 `A` 到 `B` 的所有路徑都被 `Z` d-分離，則 `A` 和 `B` 在給定 `Z` 時是條件獨立的。對撞結構（也稱 V-結構）會產生「相消解釋」效應：如果對撞節點 `m` 被觀察，其父節點 `i` 和 `j`（原本可能獨立）會變得相關。


**Q:** 在無向圖模型中，如何判斷兩組變數 `A` 和 `B` 在給定觀察變數集 `S` 的條件下是否條件獨立？這個概念叫什麼？

**A:** 這個概念叫做**分離 (separation)**。
如果從集合 `A` 中的任何節點到集合 `B` 中的任何節點的所有路徑都被集合 `S` 中的節點所阻斷（即每條路徑都至少包含一個 `S` 中的節點），那麼我們說 `S` 分離了 `A` 和 `B`，並且 `A` 和 `B` 在給定 `S` 時是條件獨立的，記為 `A ⊥ B | S`。

**Q:** 在有向圖模型中，判斷條件獨立性的概念是什麼？它與無向模型的分離有何不同？

**A:** 在有向圖模型中，判斷條件獨立性的概念叫做 **d-分離 (d-separation)**（d 代表方向性, directional）。
它與無向模型的分離不同，因為它需要考慮圖中邊的方向以及是否存在「對撞 (collider)」結構。

**Q:** 什麼是「對撞 (collider)」或「V-結構」？它在 d-分離的判斷中扮演什麼特殊角色？

**A:**
*   **對撞 (collider) / V-結構:** 在有向圖中，如果一個節點 `m` 同時是另外兩個節點 `i` 和 `j` 的子節點（即路徑片段形如 `i → m ← j`），那麼節點 `m` 在這條路徑上被稱為一個對撞。
*   **特殊角色:** 對撞的特殊之處在於它「阻斷」路徑的條件與非對撞節點相反：
    *   如果一個對撞節點 `m` **沒有**被觀察（即 `m` 不在條件集 `Z` 中，且 `m` 的任何後代也不在 `Z` 中），那麼包含該對撞的路徑是被阻斷的。
    *   如果一個對撞節點 `m` **被**觀察（即 `m` 在條件集 `Z` 中，或者 `m` 的某個後代在 `Z` 中），那麼包含該對撞的路徑是**通暢的**（除非路徑上還有其他阻斷點）。
    這種行為源於「相消解釋」效應：如果我們觀察到了一個共同結果（對撞節點），那麼它的兩個（或多個）獨立原因（父節點）之間就會變得相關。

---

### 16.2.6 在有向模型和無向模型中轉換

**重點摘要:**
有時需要在有向模型和無向模型之間進行轉換。
*   **從有向到無向 (Moralization):** 如果一個有向圖模型可以被完美地轉換為一個無向圖模型（即無向模型能夠表示有向模型中的所有條件獨立性），則該有向模型必須沒有「不道德結構 (immorality)」。不道德結構是指存在兩個沒有直接邊連接的父節點，它們共同指向同一個子節點（即 V-結構）。為了將有向圖轉換為無向圖，需要進行「道德化 (moralization)」操作：在所有具有相同子節點但彼此之間沒有邊連接的父節點之間添加無向邊（「聯姻」），然後將所有有向邊替換為無向邊。這樣得到的無向圖稱為道德圖 (moralized graph)。
*   **從無向到有向:** 將無向圖轉換為有向圖通常更複雜，因為需要為邊賦予方向，同時不能引入新的（有向圖特有的）條件獨立性（如 V-結構導致的獨立性）。一種方法是通過弦化 (triangulation) 將無向圖轉換為弦圖 (chordal graph)，然後可以為弦圖的邊賦予一個與完美消除序 (perfect elimination ordering) 一致的方向，從而得到一個等價的有向圖。


**Q:** 什麼是「不道德結構 (immorality)」或「V-結構」？為什麼它在有向圖向無向圖轉換時很重要？

**A:** 「不道德結構」或「V-結構」是指在有向圖中，存在兩個節點 `a` 和 `b`，它們都沒有直接的邊連接，但它們都指向同一個子節點 `c`（即 `a → c ← b`）。
它在有向圖向無向圖轉換時很重要，因為這種結構在有向圖中蘊含了特定的條件獨立性（如果 `c` 未被觀察，`a` 和 `b` 可能獨立），而這種獨立性很難用一個簡單的無向圖來直接表示。如果直接去掉箭頭，`a` 和 `b` 就會通過 `c` 連接，從而錯誤地暗示它們之間存在（邊緣）依賴。

**Q:** 什麼是「道德化 (moralization)」過程？它如何將有向圖轉換為無向圖？

**A:** 「道德化」是將有向圖轉換為無向圖的一個過程，旨在保留有向圖中的大部分（但可能不是全部）條件獨立性。它包含以下步驟：
1.  **「聯姻」父節點 (Marrying parents):** 對於圖中每一個節點，如果它有多個父節點，則在所有這些父節點之間添加無向邊，使得它們形成一個全連接的團（如果它們之間原本沒有邊的話）。
2.  **去掉箭頭 (Dropping arrows):** 將圖中所有有向邊替換為無向邊。
這樣得到的無向圖稱為道德圖 (moralized graph)。

**Q:** 將無向圖轉換為有向圖是否總是很簡單？通常需要什麼樣的圖結構才能更容易地進行這種轉換？

**A:** 將無向圖轉換為有向圖通常不簡單，甚至可能無法找到一個能夠完美表示相同條件獨立性的有向圖。
如果一個無向圖是**弦圖 (chordal graph)**（也稱為三角化圖, triangulated graph），即圖中任何長度大於等於4的環路都至少有一個弦（連接環路中不相鄰節點的邊），那麼將其轉換為等價的有向圖就相對容易。可以找到一個弦圖的完美消除序，並根據這個序為邊賦予方向，從而得到一個沒有新引入 V-結構（不道德結構）的有向圖。

---

### 16.2.7 因子圖

**重點摘要:**
因子圖 (Factor Graph) 是從無向圖模型中抽樣的另一種方法，它可以解決標準無向圖模型語法表達能力的某些局限性。在無向圖中，一個因子 `φ` 的範圍必須是圖中的一個團。因子圖通過引入額外的「因子節點」（通常用方塊表示）來明確表示每個因子，而原始模型的變數節點（通常用圓圈表示）只與那些包含它們的因子節點相連。
因子圖的優點在於它能更精確地表示因式分解的結構。例如，如果聯合機率可以分解為 `p(a,b,c,d) ∝ φ_1(a,b,c)φ_2(b,d)φ_3(c,d)`，在標準無向圖中，`b,c,d` 必須形成一個團，即使沒有直接的 `φ(b,c,d)` 因子。而在因子圖中，可以清晰地表示這種分解，而不需要暗示額外的依賴。因子圖對於推導和理解如置信傳播 (belief propagation) 等消息傳遞算法非常有用。


**Q:** 什麼是因子圖 (Factor Graph)？它與標準的無向圖模型在表示因子方面有何不同？

**A:** 因子圖是一種二分圖，包含兩種類型的節點：
1.  **變數節點 (Variable nodes):** 代表模型中的隨機變數（通常用圓圈表示）。
2.  **因子節點 (Factor nodes):** 代表聯合機率分佈中的因子（通常用方塊表示）。
    邊只存在於變數節點和因子節點之間，表示該變數是該因子作用域的一部分。
    與標準無向圖模型的主要不同在於，標準無向圖中，因子（勢函數）的作用域必須對應於圖中的一個團 (clique)。而因子圖更靈活，它明確地表示了每個因子以及該因子所依賴的變數，而不需要這些變數在原始變數圖中一定形成一個團。

**Q:** 因子圖相比於標準無向圖模型有哪些優點？

**A:**
1.  **更精確地表示因式分解:** 因子圖可以更準確地反映聯合機率分佈的真實因式分解結構，而不會像標準無向圖那樣可能引入虛假的（由於團的限制而暗示的）依賴關係。
2.  **有利於消息傳遞算法:** 因子圖的結構非常適合推導和實現如置信傳播 (belief propagation) 或和積 (sum-product) / 最大積 (max-product) 算法等消息傳遞算法，這些算法常用於圖模型中的推斷。

---

## 16.3 從圖模型中採樣

**重點摘要:**
圖模型簡化了從模型中採樣的過程。
*   **有向模型中的原始採樣 (Ancestral Sampling):** 對於有向無環圖，可以按照拓撲排序（即父節點總是在子節點之前）對變數進行採樣。首先從沒有父節點的變數的邊緣機率中採樣，然後依次對每個變數 `x_i` 從其條件機率 `p(x_i | Pa_G(x_i))`（其父節點的值已經被採樣得到）中採樣。這個過程非常快（如果每個條件分佈都易於採樣），並且非常簡單。
*   **無向模型中的採樣:** 從無向模型中採樣通常更困難，因為沒有自然的順序。如果需要精確採樣，通常需要解決推斷問題（例如，將無向圖轉換為有向圖的原始採樣，但這可能引入計算複雜性）。更常見的是使用近似採樣方法，如 Gibbs 採樣。Gibbs 採樣通過迭代地從每個變數 `x_i` 在給定所有其他變數 `x_{-i}` 的條件機率 `p(x_i | x_{-i})` 中採樣來生成樣本序列，該序列最終會收斂到模型的真實分佈。


**Q:** 什麼是原始採樣 (Ancestral Sampling)？它適用於哪種類型的圖模型？它是如何工作的？

**A:** 原始採樣是一種從有向無環圖 (DAG) 模型中生成精確樣本的方法。
*   **適用模型:** 有向無環圖模型。
*   **工作原理:**
    1.  對圖中的所有節點進行拓撲排序，使得每個節點的所有父節點都在它自己之前出現。
    2.  按照這個拓撲順序遍歷節點。
    3.  對於當前節點 `x_i`，如果它沒有父節點，則從其邊緣機率分佈 `p(x_i)` 中採樣得到其值。
    4.  如果它有父節點 `Pa_G(x_i)`（這些父節點的值已經在本過程的前面步驟中被採樣得到），則從條件機率分佈 `p(x_i | Pa_G(x_i))` 中採樣得到 `x_i` 的值。
    重複這個過程直到所有節點都被採樣，就得到了一個來自聯合機率分佈 `p(x)` 的樣本。

**Q:** 為什麼從無向圖模型中進行精確採樣通常比有向圖模型更困難？無向模型常用的近似採樣方法是什麼？

**A:** 從無向圖模型中進行精確採樣更困難，因為無向圖沒有像有向圖那樣自然的拓撲排序，無法直接應用原始採樣那樣的逐步生成過程。計算聯合分佈或邊緣/條件分佈通常需要處理難解的配分函數。
    無向模型常用的近似採樣方法是**馬爾可夫鏈蒙特卡羅 (MCMC)** 方法，其中最著名和常用的一種是 **Gibbs 採樣**。

**Q:** Gibbs 採樣是如何從無向圖模型中生成樣本的？

**A:** Gibbs 採樣是一種迭代的 MCMC 方法。它從一個任意的初始狀態開始，然後重複以下步驟：
1.  選擇模型中的一個變數 `x_i`（可以按順序或隨機選擇）。
2.  固定所有其他變數 `x_{-i}` 的當前值。
3.  從條件機率分佈 `p(x_i | x_{-i})` 中抽取一個新的值作為 `x_i` 的更新值。
經過足夠多的迭代（burn-in 期間）後，鏈的狀態可以被認為是近似來自目標聯合機率分佈 `p(x)` 的樣本。

---

## 16.4 結構化建模的優勢

**重點摘要:**
結構化機率模型的主要優點是它們能夠顯著表示機率分佈、學習和推斷的成本。
*   **成本降低:** 通過對變數間的直接相互作用做出稀疏假設，模型參數數量和計算複雜度可以大大降低。
*   **知識的融入:** 允許我們將關於數據的現有知識或關於學習者應如何處理數據的先驗假設明確地編碼到模型結構中。這對於處理非易獲取或稀疏數據的領域尤其有益。
*   **模塊化和可解釋性:** 模型結構清晰，易於理解和調試。可以設計、分析和評估模型的不同組件。
*   **高效的算法:** 可以利用圖結構來開發高效的學習和推斷算法，如置信傳播。


**Q:** 使用結構化機率模型（圖模型）相比於非結構化模型有哪些主要的優勢？

**A:** 主要優勢包括：
1.  **降低成本:**
    *   **參數數量減少:** 通過對變數間的依賴關係進行稀疏假設（即圖中邊較少），可以用更少的參數來表示複雜的聯合機率分佈。
    *   **計算複雜度降低:** 許多學習和推斷算法可以利用圖的結構來分解計算，從而降低運行時間。
2.  **融入先驗知識:** 允許研究者將關於問題領域的先驗知識（例如變數之間的因果關係或條件獨立性）直接編碼到模型的圖結構中。
3.  **更好的泛化能力:** 由於參數較少和結構化假設，模型可能更容易從有限的數據中學習並泛化到未見過的數據。
4.  **可解釋性和模塊化:** 圖結構提供了一種直觀的方式來理解模型中變數之間的關係，使得模型更易於解釋和調試。模型的不同部分可以被視為模塊進行單獨設計和分析。
5.  **促進高效算法的開發:** 圖結構為開發專門的、高效的學習和推 fonctionnalités算法（如消息傳遞算法）提供了基礎。

---

## 16.5 學習依賴關係

**重點摘要:**
一個好的生成模型需要準確地捕獲觀察到的「可見」變數 `v` 上的分佈。通常通過引入潛在或「隱藏」變數 `h` 來建模 `v` 之間的複雜依賴關係。學習 `v` 和 `h` 之間的連接以及 `h` 之間的連接的結構（即學習圖的邊）本身就是一個重要的研究領域，稱為**結構學習 (structure learning)**。
Koller and Friedman (2009) 提供了對該領域不偏不倚的參考。大多數結構學習技術基於一個貪婪搜索的形式，它們提出一種結構，然後評估該結構對數據的擬合程度，並重複此過程。
然而，在深度學習中，通常不直接學習圖的稀疏連接結構，而是使用全連接的層，並依賴於參數的正則化（如 L1 稀疏性）或激活的稀疏性來間接實現類似的效果。潛變數 `h` 除了顯式地建模 `v` 的依賴關係外，還有其他優勢，例如它們提供了 `v` 的表示，可以執行降維，或者使 `p(v)` 的計算更易處理。


**Q:** 在生成模型中，引入潛變量 (hidden variables) `h` 的主要目的是什麼？

**A:** 引入潛變量 `h` 的主要目的是：
1.  **建模可見變數 `v` 之間的複雜依賴關係:** 潛變數可以作為中介，捕獲 `v` 中不同元素之間的高階的、非直接的相互作用。
2.  **學習數據的抽象表示:** 潛變數可以學習到數據的更緊湊、更抽象的特徵表示。
3.  **降維:** 如果潛變數的維度低於可見變數，可以實現降維。
4.  **簡化模型:** 有時引入潛變數可以使得可見變數的邊緣機率 `p(v)`（通過對 `h` 積分或求和得到 `p(v) = Σ_h p(v,h)`）具有更簡單或更易處理的形式。

**Q:** 什麼是「結構學習 (structure learning)」？它在深度學習的背景下是如何處理的？

**A:** 「結構學習」是指從數據中學習圖模型的圖結構本身（即哪些節點之間應該有邊）。傳統的結構學習方法通常涉及一個搜索過程，評估不同圖結構對數據的擬合程度。
    在深度學習的背景下，儘管原則上可以進行結構學習，但更常見的做法是使用預先定義好的、通常是層狀的、稠密連接（或卷積連接）的結構，然後依賴於以下方式來間接實現結構的簡化或特徵的選擇：
    *   **參數正則化:** 例如 L1 正則化可以誘導權重稀疏，相當於移除了某些邊。
    *   **激活函數:** 例如 ReLU 可以導致激活稀疏。
    *   **網絡架構設計:** 例如卷積層本身就隱含了局部連接的假設。
    深度學習更側重於學習參數，而不是顯式地學習稀疏的圖連接。

---

## 16.6 推斷和近似推斷

**重點摘要:**
解決變數之間相互依賴的問題是機率模型中的一個主要方式。在潛變數模型中，我們通常需要提取觀察變數 `v` 的特徵 `h = E[h|v]`，或者需要解決涉及最大化 `log p(v) = log Σ_h p(v,h)` 的學習任務。這些都需要計算後驗機率 `p(h|v)`。
計算 `p(h|v)` 通常是**推斷 (inference)** 問題的核心。不幸的是，對於大多數有趣的深度學習模型來說，精確推斷（計算精確的 `p(h|v)`）是難以處理的，因為需要對 `h` 的所有可能配置求和或積分，這在計算上是 #P-hard 的。因此，需要**近似推斷 (approximate inference)** 的方法。本章後續和第十九章將詳細討論這些方法。


**Q:** 在潛變數模型中，「推斷 (inference)」通常指的是什麼核心計算？

**A:** 推斷的核心計算通常是指計算潛變數 `h` 在給定觀察變數 `v` 下的後驗機率分佈 `p(h|v)`，或者計算該後驗分佈下的某些期望值（如 `E[h|v]`）。

**Q:** 為什麼對於許多深度學習模型來說，精確推斷是難以處理的 (intractable)？

**A:** 因為計算後驗 `p(h|v) = p(v,h) / p(v) = p(v,h) / Σ_h' p(v,h')` 需要計算分母中的邊緣機率 `p(v)`，這涉及到對所有可能的潛變數配置 `h'` 進行求和或積分。如果潛變數空間很大或結構複雜，這個求和/積分的計算量會呈指數級增長，使得精確計算變得不可行。這個問題通常是 #P-hard 的。

**Q:** 當精確推斷不可行時，我們需要採用什麼樣的方法？

**A:** 我們需要採用**近似推斷 (approximate inference)** 的方法。這些方法旨在以可接受的計算成本找到對真實後驗 `p(h|v)` 或其相關量的良好近似。

---

## 16.7 結構化機率模型的深度學習方法

**重點摘要:**
深度學習並不是憑空發明圖模型。在深度學習中，我們通常使用圖模型來指導設計決策，導致與傳統模型有顯著不同的風格。
深度學習並不總是涉及到圖模型的「深度」。潛變數 `h` 到可見變數 `v` 的路徑長度決定了推斷的深度。我們常說的「深度」是指計算圖的深度，或者用於定義單個條件機率分佈的函數逼近器的深度。
深度學習通常基於利用分佈式表示（即許多潛變數同時激活）和學習多個抽象層次。
傳統的圖模型通常包含少量（相對於數據維度）的潛變數，且這些潛變數之間的相互作用是非線性且複雜的。深度學習模型通常使用大量的潛變數，這些潛變數之間的直接相互作用可能更簡單。
深度學習在設計圖模型的方式上，常常不那麼強調潛變數的稀疏性，而是利用潛變數之間更簡單的、通常是層次的依賴結構。
一個區別是圖模型通常有大量的單元都連接到相同的幾個單元，而傳統圖模型通常每個單元只連接到少量鄰居。
**置信傳播 (belief propagation)** 及其變體（如 loopy belief propagation）是圖模型中常用的近似推斷算法。

---

### 16.7.1 實例：受限玻爾茲曼機

**重點摘要:**
受限玻爾茲曼機 (Restricted Boltzmann Machine, RBM)，有時也稱為 harmonium，是圖模型如何用於深度學習的一個典型例子。RBM 本身不是一個深層模型，它只有一層潛變數，但它構成了學習更深層模型的組件。RBM 是基於能量的模型，其能量函數為：
`E(v,h) = -b^T v - c^T h - v^T W h` (公式 16.10)
其中 `b, c` 是偏置，`W` 是權重。RBM 的關鍵結構限制是可見單元 `v` 和隱藏單元 `h` 之間構成一個二分圖，即層內沒有連接，所有連接都在層間。
這個限制使得 RBM 具有良好的性質：
*   `p(h|v) = Π_i p(h_i|v)` (公式 16.11)
*   `p(v|h) = Π_j p(v_j|h)` (公式 16.12)
即給定一層，另一層的單元是條件獨立的。這使得 Gibbs 採樣非常高效（可以並行更新一層的所有單元），並且能量函數關於參數的梯度易於計算 (公式 16.15)。這些高效的 Gibbs 採樣和梯度計算使得訓練過程非常方便。RBM 能夠學習到數據 `v` 的良好表示 `h`（通過 `E_{h~p(h|v)}[h]`）。


**Q:** 什麼是受限玻爾茲曼機 (RBM)？它的圖結構有什麼關鍵的限制？

**A:** 受限玻爾茲曼機 (RBM) 是一種無向圖模型（也是一種基於能量的模型），它包含一層可見單元 `v` 和一層隱藏單元 `h`。
    其關鍵的結構限制是：可見單元內部沒有連接，隱藏單元內部也沒有連接。所有的連接都只存在於可見單元和隱藏單元之間，形成一個二分圖結構。

**Q:** RBM 的能量函數是什麼形式？

**A:** RBM 的能量函數通常定義為：
`E(v,h) = -b^T v - c^T h - v^T W h` (公式 16.10)
其中 `v` 是可見單元向量，`h` 是隱藏單元向量，`b` 和 `c` 分別是可見層和隱藏層的偏置向量，`W` 是連接可見層和隱藏層的權重矩陣。

**Q:** RBM 的二分圖結構帶來了哪些重要的計算優勢？

**A:** RBM 的二分圖結構導致了以下重要的計算優勢：
1.  **條件獨立性:**
    *   給定可見單元 `v`，所有隱藏單元 `h_i` 之間是條件獨立的，即 `p(h|v) = Π_i p(h_i|v)`。
    *   給定隱藏單元 `h`，所有可見單元 `v_j` 之間是條件獨立的，即 `p(v|h) = Π_j p(v_j|h)`。
2.  **高效的 Gibbs 採樣:** 由於上述條件獨立性，可以非常高效地進行塊 Gibbs 採樣。在給定 `v` 時，可以並行地對所有 `h_i` 進行採樣；在給定 `h` 時，可以並行地對所有 `v_j` 進行採樣。
3.  **易於計算的梯度:** 能量函數關於模型參數（權重和偏置）的梯度形式簡單，易於計算（例如 `∂E(v,h)/∂W_{ij} = -v_i h_j`）。

**Q:** RBM 如何用於學習數據的表示？

**A:** RBM 可以通過學習模型參數（權重 `W` 和偏置 `b, c`）來捕獲輸入數據 `v` 中的統計規律。一旦模型訓練完成，對於給定的輸入 `v`，可以計算隱藏單元激活的期望 `E_{h~p(h|v)}[h]`，這個期望值可以被視為輸入數據 `v` 的一個（通常是更抽象的、非線性的）特徵表示。這些表示可以用於後續的任務，如分類或初始化更深層的網絡。

---

希望這些詳細的摘要和Q&A對您有所幫助！