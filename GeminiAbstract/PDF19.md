## 第十九章 近似推斷 (開篇引言)

2018-03-16

Approximate Inference

[https://www.youtube.com/watch?v=YeCDY_wsojA](https://www.youtube.com/watch?v=YeCDY_wsojA)

**重點摘要:**
許多機率模型難以訓練的核心原因在於推斷困難，特別是在深度學習中，當我們有一系列可見變數 `v` 和一系列潛變數 `h` 時，計算後驗機率 `p(h|v)` 或其期望值往往是必需的，但計算上卻非常困難。即使是僅含單層隱藏變數的簡單模型（如受限玻爾茲曼機、機率PCA）可能設計成易於計算 `p(h|v)`，但大多數具有多層隱藏變數的圖模型的後驗分佈都難以處理，精確推斷算法通常需要指數級的運行時間。本章旨在介紹解決這些難處理推斷問題的技巧，並為後續第二十章中更複雜模型（如深度信念網路、深度玻爾茲曼機）的訓練奠定基礎。推斷困難通常源於結構化圖模型中潛變數之間的相互作用，這些相互作用可能是無向模型的直接連接，或有向模型中共父節點導致的「相消解釋」效應。


**Q:** 在深度學習的機率模型中，什麼是「推斷困難」通常指的是什麼？為什麼它會導致模型訓練困難？

**A:** 「推斷困難」通常是指難以計算潛變數 `h` 在給定可見變數 `v` 下的後驗機率 `p(h|v)`，或者計算該後驗機率下的某些期望值。由於許多學習算法（如最大概似學習）在其更新步驟中需要這些量，如果推斷困難，則模型參數的學習也會變得困難或不可行。

**Q:** 造成深度學習中推斷問題難以處理的根本原因是什麼？請舉例說明。

**A:** 根本原因通常源於結構化圖模型中潛變數之間的相互作用。這些相互作用可以是：
1.  **無向模型的直接相互作用：** 例如在半受限玻爾茲曼機中，潛變數之間存在直接連接，使得後驗分佈難以處理（如圖19.1左）。
2.  **有向模型中的「相消解釋」(explaining away) 作用：** 當多個潛變數是同一個可見變數的共同父節點時，觀察到可見變數會在其父節點之間引入依賴性（如圖19.1右）。

**Q:** 本章介紹的近似推斷技巧主要應用於哪些類型的機率模型？

**A:** 這些技巧主要應用於那些後驗分佈 `p(h|v)` 難以精確計算的機率模型，特別是具有多層潛變數的深度模型，如深度信念網路和深度玻爾茲曼機。即使是一些單層模型，如稀疏編碼，也可能存在推斷困難的問題。

---

## 19.1 把推斷視作優化問題

**重點摘要:**
精確推斷問題可以轉化為一個優化問題，這為開發近似推斷算法提供了思路。假設我們有一個包含可見變數 `v` 和潛變數 `h` 的機率模型，我們希望計算觀察數據的對數概似 `log p(v;θ)`。當直接邊緣化消去 `h` 很困難時，我們可以轉而計算 `log p(v;θ)` 的一個下界，稱為證據下界 (Evidence Lower Bound, ELBO)，也叫負變分自由能。ELBO 定義為：
`L(v, θ, q) = log p(v; θ) - D_KL(q(h|v) || p(h|v; θ))` (公式 19.1)
其中 `q(h|v)` 是關於 `h` 的一個任意機率分佈。由於 KL 散度非負，`L` 總是小於等於 `log p(v;θ)`，當且僅當 `q(h|v)` 完全等於真實後驗 `p(h|v;θ)` 時取等號。
通過代數運算，ELBO 可以重寫為更簡潔的形式：
`L(v, θ, q) = E_{h~q}[log p(h,v;θ)] - E_{h~q}[log q(h|v)] = E_{h~q}[log p(h,v;θ)] + H(q)` (公式 19.6, 19.7)
其中 `H(q)` 是分佈 `q` 的熵。因此，推斷問題可以看作是尋找一個分佈 `q` 來最大化 ELBO `L` 的過程。如果能在包含真實後驗 `p(h|v)` 的函數族中完美優化，就能實現精確推斷。近似推斷則是通過限定 `q` 的形式或使用不徹底的優化方法來高效地（但可能粗略地）提升 `L`。


**Q:** 什麼是證據下界 (ELBO)？它與真實的對數概似 `log p(v;θ)` 有什麼關係？

**A:** 證據下界 (ELBO)，記為 `L(v, θ, q)`，是真實對數概似 `log p(v;θ)` 的一個下界。它們之間的關係是 `L(v, θ, q) = log p(v; θ) - D_KL(q(h|v) || p(h|v; θ))`，其中 `q(h|v)` 是一個任意的近似後驗分佈，`D_KL` 是 KL 散度。因為 KL 散度總是大于或等于零，所以 `L(v, θ, q)` 總是小于或等于 `log p(v;θ)`。

**Q:** ELBO 的另一種常見表達式是什麼？它包含哪些項？

**A:** ELBO 的另一種常見表達式是 `L(v, θ, q) = E_{h~q(h|v)}[log p(h,v;θ)] + H(q(h|v))` (公式 19.7)。
它包含兩項：
1.  `E_{h~q(h|v)}[log p(h,v;θ)]`: 在近似後驗 `q` 下，模型聯合機率 `p(h,v;θ)` 的對數的期望。
2.  `H(q(h|v))`: 近似後驗分佈 `q` 的熵。

**Q:** 如何將推斷問題轉化為一個優化問題？這個優化問題的目標是什麼？

**A:** 可以將推斷問題（即計算或近似 `p(h|v)`）看作是尋找一個近似分佈 `q(h|v)` 使得證據下界 `L(v, θ, q)` 最大化的過程。因為最大化 `L` 等價于最小化 `q(h|v)` 與真實後驗 `p(h|v;θ)` 之間的 KL 散度。如果能夠找到使 `L` 達到其理論上界（即 `log p(v;θ)`）的 `q`，那麼這個 `q` 就是真實的後驗分佈。

---

## 19.2 期望最大化

**重點摘要:**
期望最大化 (Expectation-Maximization, EM) 算法是一種常見的用於在潛變數模型中最大化證據下界 `L` 的訓練算法。EM 算法並非近似推斷算法，而是能夠學到近似後驗的算法。它通過交替迭代以下兩步直至收斂：
1.  **E步 (Expectation step):** 固定當前模型參數 `θ^(old)`，計算（或定義）潛變數的後驗機率（或近似後驗）`q(h|v) = p(h|v; θ^(old))`。在這個定義下，`q` 在當前參數 `θ^(old)` 下是最優的，最大化了 `L` 中與 `q` 相關的部分。
2.  **M步 (Maximization step):** 固定分佈 `q`，通過優化模型參數 `θ` 來最大化 `L(v, θ, q)`（通常是對所有訓練樣本的 `L` 求和），即 `Σ_i L(v^(i), θ, q)` (公式 19.8)。
EM 算法可以看作是通過坐標上升來最大化 `L`：E步優化 `q`，M步優化 `θ`。基於潛變數模型的隨機梯度上升可以視為 EM 的特例，其中 M 步只執行一次梯度操作。儘管 E 步使用精確推斷，但 M 步假設一個 `q` 可以被所有 `θ` 值共享，當 M 步遠離 E 步的 `θ^(old)` 時，`L` 和真實 `log p(v)` 之間會出現差距，但下一個 E 步會彌合這個差距。EM 算法的一個關鍵性質是，當模型參數 `θ` 改變時，我們仍然可以使用舊的（E步計算的）`q` 分佈，這在傳統機器學習中推導 M 步更新時很有用，但在深度學習中由於模型複雜性，M 步的簡單解析解較少見。


**Q:** 期望最大化 (EM) 算法的兩個主要步驟是什麼？它們各自的目標是什麼？

**A:** EM 算法包含兩個主要步驟：
1.  **E步 (Expectation step):** 在這一步，固定當前的模型參數 `θ^(old)`，計算或設置潛變數 `h` 的（近似）後驗分佈 `q(h|v)`。對於標準 EM，`q(h|v)` 被設為真實的後驗 `p(h|v; θ^(old))`。這一步的目標是基於當前模型對潛變數做出最好的推斷。
2.  **M步 (Maximization step):** 在這一步，固定 E 步得到的潛變數分佈 `q(h|v)`，然後調整模型參數 `θ` 以最大化證據下界 `L(v, θ, q)`（通常是對所有數據點的 `L` 的總和）。

**Q:** EM 算法與坐標上升法有何關聯？

**A:** EM 算法可以被看作是一種坐標上升法，用於最大化證據下界 `L(v, θ, q)`。E 步通過更新 `q` 來最大化 `L`（固定 `θ`），而 M 步通過更新 `θ` 來最大化 `L`（固定 `q`）。通過交替執行這兩步，`L` 的值會單調不減地增加，直至收斂。

**Q:** 在深度學習的背景下，EM 算法的 M 步與傳統機器學習中的 M 步有何不同？

**A:** 在傳統機器學習中，對於某些模型族，M 步有時可以通過解析解直接完成，即在給定 `q` 的情況下找到最優的 `θ`。然而，在深度學習中，模型通常非常複雜（例如由深度神經網路參數化），導致很難在 M 步中找到一個簡單的解析解來獲得最優的 `θ`。因此，深度學習中的 M 步通常涉及使用梯度下降等迭代優化方法來部分地或完全地最大化 `L`。

---

## 19.3 最大後驗推斷和稀疏編碼

**重點摘要:**
推斷不僅指計算完整的後驗分佈 `p(h|v)`，也可以指計算潛變數的最可能值 `h* = argmax_h p(h|v)`，這被稱為最大後驗 (Maximum A Posteriori, MAP) 推斷。MAP 推斷本身是精確計算 `h*`，但如果我們將其視為產生一個近似後驗分佈 `q` 的過程，那麼它是一種近似推斷，因為它提供的 `q` 並非最優。具體來說，如果我們將 `q` 限定為狄拉克 (Dirac) 分佈 `q(h|v) = δ(h-μ)` (公式 19.11)，那麼最大化 ELBO `L(v,θ,q)` 就等價於解決 `μ* = argmax_μ log p(h=μ, v;θ)` (公式 19.12)，這與 MAP 推斷問題 `h* = argmax_h p(h|v)` (公式 19.13) 本質相同（因為 `p(v)` 相對於 `h` 或 `μ` 是常數）。
MAP 推斷在深度學習中被廣泛應用於稀疏編碼模型。稀疏編碼是一種線性因子模型，其潛變數 `h` 具有誘導稀疏性的先驗（如可分解的 Laplace 先驗 `p(h_i) = (λ/2)exp(-λ|h_i|)` (公式 19.14)），可見變數 `v` 由 `h` 的線性變換加高斯噪聲生成 `p(v|h) = N(v; Wh+b, β⁻¹I)` (公式 19.15)。由於潛變數之間的「相消解釋」效應（當 `v` 被觀察時，所有潛變數都在一個大的團中），後驗 `p(h|v)` 難以計算。因此，通常不使用精確的最大概似學習，而是通過 MAP 推斷得到 `h`，然後最大化以該 `h` 為中心的 Dirac 分佈所定義的 ELBO 來學習模型參數 `W` 和 `b`。這相當於最小化一個目標函數，如 `J(H,W) = Σ_{i,j} λ|H_{i,j}| + (β/2) Σ_{i,j} (V - HW^T)_{i,j}^2` (公式 19.16，此處為簡化形式)。這個最小化問題可以通過交替優化 `H` 和 `W` 來解決。


**Q:** 什麼是最大後驗 (MAP) 推斷？它與計算完整的後驗分佈 `p(h|v)` 有何不同？

**A:** MAP 推斷是計算潛變數 `h` 的最可能值（即眾數）`h* = argmax_h p(h|v)`。它只給出了一個點估計，而不是像 `p(h|v)` 那樣給出了潛變數所有可能值的完整機率分佈。

**Q:** 如何將 MAP 推斷視為一種近似推斷方法？它對應於選擇什麼樣的近似後驗分佈 `q`？

**A:** 如果我們將 MAP 推斷的結果 `h*` 用來定義一個近似後驗分佈，那麼這種近似後驗分佈是一個集中在 `h*` 點的狄拉克 (Dirac) δ 函數，即 `q(h|v) = δ(h-h*)`。從最大化 ELBO 的角度看，MAP 推斷是在所有可能的狄拉克分佈族中尋找最優的 `q`。由於狄拉克分佈的熵的微分趨近于負無窮，這使得 ELBO 界變得無限松，所以 MAP 推斷通常不被認為是好的變分近似。

**Q:** 在稀疏編碼模型中，為什麼通常使用 MAP 推斷而不是嘗試計算完整的後驗 `p(h|v)`？

**A:** 在稀疏編碼模型中，當可見變數 `v` 被觀察到時，由於「相消解釋」效應，所有潛變數 `h_i` 之間都變得相關，形成一個大的團。如果潛變數的先驗不是高斯分佈（例如常用的 Laplace 稀疏先驗），那麼這個後驗 `p(h|v)` 通常非常複雜，難以計算或表達。因此，計算量更小的 MAP 推斷成為一種實用的替代方案。

**Q:** 使用 MAP 推斷學習稀疏編碼模型的參數，其目標函數通常是什麼形式？

**A:** 其目標函數通常包含兩部分：一部分是與潛變數稀疏性先驗相關的懲罰項（例如 `L1` 范數 `Σ λ|H_{i,j}|`），另一部分是重構誤差項（例如平方誤差 `Σ (V - HW^T)_{i,j}^2`）。學習過程是最小化這個組合的目標函數 `J(H,W)` (公式 19.16)。

---

## 19.4 變分推斷和變分學習

**重點摘要:**
變分學習的核心思想是在一個受約束的分佈族（關於 `q`）上最大化證據下界 `L`。選擇這個分佈族時需要考慮計算 `E_q[log p(h,v)]` 的難易度。
*   **均值場 (Mean-field) 方法:** 一種常用的變分學習方法是假設 `q` 是一個因子分佈，即 `q(h|v) = Π_i q(h_i|v)` (公式 19.17)。這被稱為均值場近似。更一般地，可以通過選擇 `q` 的圖模型結構來靈活決定近似程度，這被稱為結構化變分推斷。
變分方法的優點是不需要為 `q` 設定特定的參數化形式，而是設定其分解方式，然後通過優化問題找出在這些限制下最優的機率分佈。對於離散潛變數，這通常轉化為優化有限個變數；對於連續潛變數，則需要使用變分法 (calculus of variations) 來解決函數空間上的優化問題。
當使用 `q` 來擬合 `p` 時，最小化 `D_KL(q||p)` (變分推斷的目標) 和最小化 `D_KL(p||q)` (最大概似學習的目標) 有不同的偏好。前者傾向于找到一個 `q`，使得在真實後驗 `p` 機率低的地方 `q` 的機率也低；後者傾向于找到一個模型，使得在數據點 `p_data` 機率高的地方模型機率也高 (參見圖 3.6)。

---

### 19.4.1 離散型潛變數

**重點摘要:**
對於離散潛變數，變分推斷相對直接。通常假設 `q` 的每個因子（例如，在均值場假設下是 `q(h_i|v)`）由一些離散狀態的可查詢表格定義。最簡單情況下，如果 `h` 是二值的且採用均值場假定，可以用一個向量 `ĥ`（其中 `ĥ_i = q(h_i=1|v)`）來參數化 `q`。優化這些參數 `ĥ_i` 可以通過標準優化算法（如梯度下降）解決。為了追求速度，通常使用特殊設計的優化算法，如不動點方程求解 `∂L/∂ĥ_i = 0` (公式 19.18)。在二值稀疏編碼模型中，輸入 `v` 由 `m` 個二值隱藏單元 `h_i`（其先驗為 `p(h_i=1)=σ(b_i)`）的線性組合加高斯噪聲生成 `p(v|h) = N(v; Wh, β⁻¹I)` (公式 19.19, 19.20)。通過均值場近似 `q(h|v) = Π_i q(h_i|v)`，可以推導出 ELBO `L` (公式 19.36)。


**Q:** 在對離散潛變數進行均值場變分推斷時，近似後驗 `q` 通常如何表示？

**A:** 通常，近似後驗 `q(h|v)` 被假定為可以分解為各個潛變數 `h_i` 的邊緣分佈的乘積，即 `q(h|v) = Π_i q(h_i|v)`。如果 `h_i` 是離散的，那麼 `q(h_i|v)` 本身可以由一個表示 `h_i` 取各個離散值的機率的表格來定義。例如，如果 `h_i` 是二值的，`q(h_i=1|v)` 可以用一個參數 `ĥ_i` 表示。

**Q:** 在均值場近似下，如何優化近似後驗 `q` 的參數（例如 `ĥ_i`）？

**A:** 一種常見的方法是通過求解不動點方程。對證據下界 `L` 關於每個參數 `ĥ_i` 求偏導，並令其等於零，即 `∂L/∂ĥ_i = 0` (公式 19.18)。然後反覆迭代更新每個 `ĥ_i` 直到滿足收斂準則。

**Q:** 在推導二值稀疏編碼模型的變分學習方程時，均值場近似帶來了什麼簡化？

**A:** 均值場近似 `q(h|v) = Π_i q(h_i|v)` 使得原本複雜的聯合後驗的期望計算可以分解為對各個獨立的 `q(h_i|v)` 的期望計算。這使得證據下界 `L` 可以被表示為少量簡單的代數運算（如公式 19.36 所示），從而使其易於處理和優化。

---

### 19.4.2 變分法

**重點摘要:**
變分法 (calculus of variations) 是處理函數的函數（稱為泛函, functional）優化問題的數學工具。正如我們對普通函數求偏導找到臨界點一樣，我們可以使用泛函導數（或變分導數）找到使泛函取極值的函數。一個關鍵公式是 `δ/δf(x) ∫ g(f(x'),x')dx' = ∂g(f(x),x)/∂f(x)` (公式 19.46 的簡化理解，假設 `g` 僅依賴 `f(x)` 在 `x` 點的值)。通過尋找泛函導數為零的函數來優化泛函。例如，要找到具有最大微分熵且滿足均值和方差約束的機率密度函數 `p(x)`，可以構造拉格朗日泛函 `L[p]` (公式 19.50)，並令其關於 `p(x)` 的泛函導數為零 (公式 19.52)，解得 `p(x)` 的形式為高斯分佈 (公式 19.54)。


**Q:** 什麼是泛函 (functional)？什麼是泛函導數 (functional derivative) 或變分導數 (variational derivative)？

**A:**
*   **泛函:** 函數的函數，即輸入是一個函數，輸出是一個標量值的映射。例如，熵 `H[p]` 是一個泛函，因為它的輸入是機率密度函數 `p(x)`，輸出是一個實數。
*   **泛函導數/變分導數:** 類似於普通函數的偏導數，它衡量泛函在其輸入函數發生微小變化時的變化率。記為 `δJ[f]/δf(x)`，表示泛函 `J` 關於其輸入函數 `f` 在點 `x` 處的函數值的導數。

**Q:** 變分法在尋找最優機率分佈（如最大熵分佈）中有什麼作用？

**A:** 變分法提供了一種系統性的方法來尋找滿足某些約束條件下使某個泛函（如熵）最大化或最小化的函數。通過構造一個包含約束的拉格朗日泛函，然後令該泛函關於待求函數的泛函導數為零，就可以導出該最優函數所應滿足的方程或其函數形式。例如，可以通過變分法證明在給定均值和方差的情況下，高斯分佈是具有最大熵的連續機率分佈。

---

### 19.4.3 連續型潛變數

**重點摘要:**
當圖模型包含連續型潛變數時，仍然可以使用均值場近似 `q(h|v) = Π_i q(h_i|v)` (公式 19.55) 並通過最大化 `L` 來進行變分推斷和學習。對於每個 `q(h_i|v)` 的最優形式，可以通過一個通用的不動點更新公式得到：
`~q(h_i|v) = exp(E_{h_{-i}~q(h_{-i}|v)}[log p(v,h)])` (公式 19.56)
然後對 `~q(h_i|v)` 進行歸一化得到 `q(h_i|v)`。在這個方程中，計算期望就能得到 `q(h_i|v)` 的表達式。重要的是，這個不動點方程不僅給出了迭代更新的方法，也揭示了最優解的泛函形式。例如，在一個簡單的雙變數連續模型中（`p(h) = N(h;0,I)`, `p(v|h) = N(v; w^T h; 1)`），通過應用此公式，可以發現最優的 `q(h_1|v)` 和 `q(h_2|v)` 都是高斯分佈，其參數可以通過優化得到 (公式 19.66, 19.67)。


**Q:** 對於連續潛變數，均值場近似下的最優單個因子 `q(h_i|v)` 的通用更新規則是什麼？

**A:** 如果採用均值場近似 `q(h|v) = Π_j q(h_j|v)`，並且固定所有 `q(h_j|v)` (對於 `j ≠ i`)，那麼最優的 `q(h_i|v)` 可以通過以下未歸一化的分佈形式得到：
`~q(h_i|v) = exp(E_{h_{-i}~Π_{j≠i}q(h_j|v)}[log p(v,h)])` (公式 19.56)
其中 `E_{h_{-i}~...}` 表示對除了 `h_i` 之外的所有潛變數求期望。計算出這個期望後，對 `~q(h_i|v)` 關於 `h_i` 積分（或求和）進行歸一化，即可得到最優的 `q(h_i|v)`。

**Q:** 在應用公式 19.56 推導連續潛變數的變分更新時，為什麼說它不僅給出了迭代更新方法，還揭示了最優解的泛函形式？

**A:** 因為公式 19.56 中的 `exp(...)` 項直接給出了（未歸一化的）最優 `q(h_i|v)` 的函數表達式。通過計算這個期望 `E_{h_{-i}}[log p(v,h)]`，我們可以分析結果的數學形式。例如，在教材中給出的簡單線性高斯模型例子中，通過計算這個期望，發現 `log ~q(h_1|v)` 是 `h_1` 的二次函數，這意味著最優的 `q(h_1|v)` 必然是高斯分佈。這是在沒有預先假設 `q` 是高斯分佈的情況下，通過變分原理推導出來的。

---

### 19.4.4 學習和推斷之間的相互作用

**重點摘要:**
在學習算法中使用近似推斷會影響學習過程，反過來學習過程也會影響推斷算法的準確性。訓練算法傾向于朝著使得近似推斷中的近似假設變得更加真實的方向來適應模型。當訓練參數時，變分學習會增加 `E_{h~q}[log p(v,h)]` (公式 19.68)。這意味著對於 `q(h|v)` 中機率很大的 `h`，模型會增加 `p(h|v)`；對於 `q(h|v)` 中機率很小的 `h`，模型會減小 `p(h|v)`。這種行為使得模型傾向於讓其真實後驗更接近於近似後驗的形式（例如，如果用單峰近似後驗來訓練模型，那麼所得模型的真實後驗會比使用精確推斷訓練的模型的後驗更接近單峰）。估計變分近似對模型的破壞程度是很困難的。如果 `L(v,θ,q) ≈ log p(v;θ)` 並且 `log p(v;θ)` 遠小於最優的 `log p(v;θ*)`，這可能是因為在 `θ*` 點處後驗分佈太過複雜，使得 `q` 分佈族無法準確描述，從而學習過程永遠無法到達 `θ*`。


**Q:** 近似推斷和模型學習之間存在怎樣的相互作用？

**A:**
1.  **近似推斷影響學習:** 如果在學習算法（如參數更新）中使用近似推斷得到的潛變數後驗，那麼模型參數的學習方向會受到該近似的影響。
2.  **學習影響近似推斷的準確性:** 模型參數的改變會導致真實的潛變數後驗 `p(h|v)` 發生變化。學習算法（特別是基於變分學習的）傾向于調整模型參數，使得近似推斷中做出的假設（例如均值場假設）對於新的模型參數來說更為準確。

**Q:** 為什麼說變分學習傾向于使得模型的真實後驗分佈更接近於所使用的近似後驗分佈的形式？

**A:** 變分學習的目標是最大化證據下界 `L(v,θ,q) = E_{h~q}[log p(v,h)] + H(q)`。在 M 步（或更一般的參數更新步驟），`q` 是固定的，模型參數 `θ` 被調整以增加 `E_{h~q}[log p(v,h)]`。這意味著模型會試圖在其認為潛變數 `h` 可能出現的區域（即 `q(h|v)` 機率高的區域）賦予更高的聯合機率 `p(v,h)`（從而也提高了條件機率 `p(h|v)`）。如果 `q` 具有某种結構（例如是單峰的），那麼模型學習到的真實後驗 `p(h|v)` 也會傾向於呈現類似的結構，因為模型試圖在 `q` 能夠很好覆蓋的區域內優化。

**Q:** 評估變分近似對模型學習的「危害」為什麼很困難？

**A:** 因為我們通常很難知道理論上最優的模型參數 `θ*`（即通過精確推斷和最大化真實對數概似得到的參數）以及對應的最優對數概似 `log p(v;θ*)` 是多少。我們可能觀察到訓練後模型的 `L(v,θ,q)` 與 `log p(v;θ)` 很接近（表明變分近似對於當前 `θ` 是準確的），但同時 `log p(v;θ)` 可能遠小於 `log p(v;θ*)`（表明模型本身沒有學好）。這種情況可能發生在：最優參數 `θ*` 對應的真實後驗 `p(h|v;θ*)` 非常複雜，以至於我們選擇的近似後驗族 `q` 無法很好地描述它，因此學習過程永遠無法達到 `θ*`。如果沒有一個能夠找到 `θ*` 的「超級學習算法」作為對照，就很難判斷是近似推斷本身限制了模型的性能，還是模型本身的能力問題。

---

## 19.5 學成近似推斷

**重點摘要:**
推斷過程本身（即從 `v` 得到最優 `q* = argmax_q L(v,q)`）可以被視為一個函數 `f`。如果這個優化過程（如不動點迭代）代價很高，可以學習一個神經網路 `~f(v;θ)` 來近似這個函數 `f`，從而實現快速的近似推斷。這被稱為學成近似推斷 (amortized inference)。

---

### 19.5.1 醒眠算法

**重點摘要:**
醒眠 (wake-sleep) 算法 (Hinton et al., 1995b; Frey et al., 1996) 解決了在訓練有向生成模型 `p(h,v)` 時，由於沒有監督訓練集 `(v,h)` 而難以訓練推斷網路 `q(h|v)` 的問題。
*   **Wake 階段 (生成模型學習):** 從數據中抽取 `v`，然後使用（可能是近似的）推斷網路 `q(h|v)` 產生 `h`。然後更新生成模型 `p(h,v)` 的參數，以增加 `p(v,h)` 的機率（例如，增加 `E_q[log p(h,v)]`）。
*   **Sleep 階段 (推斷模型學習):** 從生成模型 `p(h,v)` 中進行原始採樣，得到 `(v,h)` 對。然後將這些 `(v,h)` 對作為監督數據來訓練推斷網路 `q(h|v)`，使其能夠從 `v` 預測 `h`。
主要缺點是在學習早期，模型分佈與數據分佈偏差較大，導致 Sleep 階段訓練推斷網路的樣本質量不高。生物學上，做夢可能提供類似 Sleep 階段的樣本。


**Q:** 什麼是學成近似推斷 (amortized inference)？它的主要目的是什麼？

**A:** 學成近似推斷是指訓練一個單獨的參數化函數（通常是神經網路），稱為推斷網路或識別模型，使其能夠直接從輸入 `v` 預測潛變數的近似後驗分佈 `q(h|v)` 的參數。其主要目的是避免在每次需要推斷時都執行昂貴的迭代優化過程（如均值場不動點迭代）來找到最優的 `q`。一旦推斷網路訓練完成，對於新的 `v`，可以直接通過一次前向傳播得到 `q`。

**Q:** 醒眠算法包含哪兩個主要階段？它們各自的目標是什麼？

**A:** 醒眠算法包含兩個主要階段：
1.  **Wake 階段 (清醒階段):**
    *   **目標:** 學習生成模型 `p(h,v)` 的參數。
    *   **過程:** 從真實數據中抽取一個樣本 `v`。使用當前的（可能是近似的）推斷網路 `q(h|v)` 來推斷潛變數 `h`。然後，基於這個 `(v,h)` 對，更新生成模型 `p(h,v)` 的參數以增加 `p(v,h)` 的機率。
2.  **Sleep 階段 (睡眠階段):**
    *   **目標:** 學習推斷網路 `q(h|v)` 的參數。
    *   **過程:** 從當前的生成模型 `p(h,v)` 中進行原始採樣（即從先驗 `p(h)` 採樣 `h`，然後從 `p(v|h)` 採樣 `v`），得到一對 `(v,h)`。然後，將這個 `(v,h)` 對作為監督數據來訓練推斷網路 `q(h|v)`，使其能夠從 `v` 準確地預測 `h`。

**Q:** 醒眠算法的主要缺點是什麼？

**A:** 主要缺點是，在學習的早期階段，生成模型 `p(h,v)` 可能與真實的數據分佈 `p_data(v)` 相差很遠。因此，在 Sleep 階段從 `p(h,v)` 中採樣得到的 `(v,h)` 對可能與真實數據（以及其對應的真實潛在表示）很不一樣，導致推斷網路在這些「不真實」的樣本上進行訓練，學習效果可能不佳。

---

### 19.5.2 學成推斷的其他形式

**重點摘要:**
學成近似推斷策略已被應用於其他模型。例如，在 DBM 中，可以使用學成的推斷網路進行單遍傳遞來替代迭代的均值場不動點方程，以獲得更快的推斷。其訓練過程基於運行推斷網路，然後運行一步均值場來改進其估計，並訓練推斷網路輸出這個更精細的估計。預測性稀疏分解 (PSD) 模型可以看作是自編碼器和稀疏編碼的混合，其中編碼器被視為執行學成近似 MAP 推斷。變分自編碼器 (VAE) 是學成近似推斷成為核心方法的最新例子，推斷網路直接用於定義 ELBO `L`，然後調整推斷網路的參數以增大 `L`。


**Q:** 除了醒眠算法，學成近似推斷還應用於哪些其他模型或場景？

**A:**
1.  **深度玻爾茲曼機 (DBM):** Salakhutdinov and Larochelle (2010) 證明了在 DBM 中，使用學成推斷網路進行單遍前向傳播可以比迭代的均值場不動點方程更快地獲得推斷結果。訓練這種推斷網路的方法是：先運行推斷網路得到一個初始估計，然後運行一步均值場迭代來改進這個估計，最後訓練推斷網路直接輸出這個改進後的更精細的估計。
2.  **預測性稀疏分解 (PSD) / ISTA:** 這些模型可以看作是自編碼器和稀疏編碼的結合。其中，編碼器部分可以被視為一個執行學成近似 MAP 推斷的網路，它直接預測輸入的稀疏編碼。
3.  **變分自編碼器 (VAE):** VAE 是學成近似推斷的一個突出例子。其中的編碼器（推斷網路）直接參數化了近似後驗分佈 `q(z|x)`，這個 `q` 被用來定義證據下界 (ELBO) `L`。然後，推斷網路和生成網路的參數一起被優化以最大化 `L`。

---

希望這次的內容是完整的，並且對您有所幫助！