---
layout: default
title: Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101
---

<p align="center">
  <strong>Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101</strong>  
</p>
<p align="center">
  AI是一條孤獨且充滿惶恐及未知的旅程，花俏絢麗的收費課程或活動絕非通往成功的捷徑。<br>
  衷心感謝當時來自不同單位的AI同好參與者實名分享的寶貴經驗；如欲移除資訊還請告知。<br>
  由 <a href="https://www.twman.org/" target="_blank">TonTon Huang Ph.D.</a> 發起，及其當時任職公司(台灣雪豹科技)無償贊助場地及茶水點心。<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="180"></a>
    <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 100px !important;width: 180px !important;" ></a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101" target="_blank">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/" target="_blank">Facebook</a> |
  <a href="https://deep-learning-101.github.io/"> 回 GitHub Pages</a> |
  <a href="http://DeepLearning101.TWMAN.ORG" target="_blank">網站</a> |
  <a href="https://huggingface.co/DeepLearning101" target="_blank">Hugging Face Space</a>
</p>

# 第二十章 深度生成模型

<a href="https://www.youtube.com/watch?v=oiDYD1qibBQ" target="_blank" rel="noopener noreferrer"><i class="fab fa-youtube mr-1"></i>2018/04/13, Deep Generative Models @ Deep Learning Book Chapter 20</a><br>


**重點摘要:**
本章旨在介紹多種具體的深度生成模型，這些模型利用前面章節（十六至十九章）討論的技術進行構建和訓練。所有這些模型的核心目標是表示多個變量的機率分佈。部分模型允許顯式計算機率分佈函數，而另一些模型則不支持直接評估，但允許從分佈中採樣等隱式操作。某些模型使用圖模型語言進行結構化描述，而其他模型雖然也代表機率分佈，但無法簡單地從因子角度描述。


**Q:** 本章介紹的深度生成模型的主要共同目標是什麼？
 這些模型的主要共同目標是在某種程度上代表多個變量的機率分佈。

**Q:** 根據本章引言，不同生成模型在評估機率分佈函數方面有何差異？

**A:** 有些模型允許顯式地計算機率分佈函數；其他模型則不允許直接評估，但支持隱式獲取分佈知識的操作，如從分佈中採樣。

---

## 20.1 玻爾茲曼機

**重點摘要:**
玻爾茲曼機（Boltzmann Machine, BM）最初作為一種廣義的「聯結主義」模型被引入，用於學習二值向量上的任意機率分佈。它是基於能量的模型，其聯合機率分佈由能量函數 `E(x)` 定義：`P(x) = exp(-E(x))/Z` (公式 20.1)，其中 `Z` 是配分函數。對於二值玻爾茲曼機，能量函數通常為 `E(x) = -x^T U x - b^T x` (公式 20.2)。當引入隱藏單元（將單元 `x` 分為可見單元 `v` 和隱藏單元 `h`）時，能量函數變為 `E(v,h) = -v^T Rv - v^T Wh - h^T Sh - b^T v - c^T h` (公式 20.3)，這使得模型能夠模擬可見單元之間的高階交互作用，並成為離散變數上機率質量函數的萬能近似器。玻爾茲曼機的學習算法通常基於最大概似，但由於配分函數難以處理，其梯度必須使用近似方法（如第十八章的技術）。一個有趣的特性是，權重更新可以是「局部」的，僅取決於相連單元在不同分佈下的統計信息，這具有一定的生物學合理性（類似Hebbian學習）。


**Q:** 什麼是玻爾茲曼機的核心思想？其能量函數和機率分佈如何定義？

**A:** 玻爾茲曼機是一種基於能量的模型，旨在學習二值向量上的任意機率分佈。其能量函數為 `E(x)`，機率分佈為 `P(x) = exp(-E(x))/Z` (公式 20.1)，其中 `Z` 是配分函數。對於僅包含可見單元的二值玻爾茲曼機，能量函數為 `E(x) = -x^T U x - b^T x` (公式 20.2)。

**Q:** 訓練玻爾茲曼機時，為何通常需要近似方法？

**A:** 因為所有玻爾茲曼機都具有難以處理的配分函數 `Z`，這使得最大概似梯度的精確計算變得不可行。因此，必須使用如第十八章中討論的技術來近似該梯度。

**Q:** 在玻爾茲曼機中引入隱藏單元有何重要性？

**A:** 當並非所有變量都能被觀察到時（即存在隱藏單元），玻爾茲曼機變得更強大。隱藏單元類似於多層感知機中的隱藏單元，可以模擬可見單元之間的高階交互作用。這使得具有隱藏單元的玻爾茲曼機不再局限於建模變量間的線性關係，而是成為了離散變量上機率質量函數的萬能近似器。

**Q:** 玻爾茲曼機的學習規則為何被認為具有一定的「局部性」和生物學合理性？

**A:** 當基於最大概似的學習規則訓練時，連接兩個單元的特定權重的更新僅取決於這兩個單元在不同分佈下（`P_model(v)` 和 `P_data(v)P_model(h|v)`）收集的統計信息。這意味著學習規則是「局部」的，似乎在某種程度上是生物學合理的，類似於Hebbian學習規則（"fire together, wire together"）。

---

## 20.2 受限玻爾茲曼機

**重點摘要:**
受限玻爾茲曼機（Restricted Boltzmann Machine, RBM）是深度機率模型中最常見的組件之一。它是一個包含一層可觀察變量（`v`）和單層潛變量（`h`）的無向機率圖模型。RBM 的關鍵特性是其二分圖結構：觀察層或潛層中的任何單元之間不允許存在連接，連接僅存在於觀察層和潛層之間。與普通玻爾茲曼機一樣，RBM 也是基於能量的模型，其聯合機率分佈由能量函數 `E(v,h) = -b^T v - c^T h - v^T Wh` (公式 20.5) 指定，`P(v,h) = (1/Z)exp(-E(v,h))` (公式 20.4)。儘管配分函數 `Z` 仍然難解，但 RBM 的二分圖結構使得條件機率 `P(h|v)` 和 `P(v|h)` 易於計算且呈因子形式。


**Q:** 受限玻爾茲曼機（RBM）與普通玻爾茲曼機在結構上有何關鍵區別？

**A:** RBM 的關鍵結構區別在於它是一個二分圖。這意味著在可觀察層（`v`）內部或潛層（`h`）內部，單元之間沒有連接。所有連接都只存在於可觀察層和潛層之間。而普通玻爾茲曼機則允許層內連接。

**Q:** RBM 的能量函數是什麼形式？

**A:** RBM 的能量函數由下式給出：`E(v, h) = -b^T v - c^T h - v^T Wh` (公式 20.5)，其中 `b` 和 `c` 分別是可觀察層和潛層的偏置向量，`W` 是連接兩層的權重矩陣。

**Q:** 儘管RBM的配分函數 `Z` 難解，為何它在計算上仍具有某些優勢？

**A:** 儘管 `P(v)` 和配分函數 `Z` 難解，RBM 的二分圖結構使其條件分佈 `P(h|v)` 和 `P(v|h)` 具有因子形式，並且計算和採樣相對簡單。這對於訓練算法（如對比散度）至關重要。

---

### 20.2.1 條件分佈

**重點摘要:**
由於 RBM 的二分圖結構，其條件機率分佈 `P(h|v)` 和 `P(v|h)` 可以分解為各個單元條件機率的乘積，使得計算和採樣非常高效。
對於給定的可見單元 `v`，隱藏單元 `h_j` 的激活機率為 `P(h_j=1|v) = σ(c_j + v^T W_{:,j})` (公式 20.14)，其中 `σ` 是 sigmoid 函數。因此，`P(h|v) = Π_j P(h_j|v)`。
類似地，對於給定的隱藏單元 `h`，可見單元 `v_i` 的激活機率（對於二值可見單元）為 `P(v_i=1|h) = σ(b_i + W_{i,:} h)`。因此，`P(v|h) = Π_i P(v_i|h)` (公式 20.16)。
這些因子化的條件分佈是 RBM 能夠有效進行塊吉布斯採樣的基礎。


**Q:** 在 RBM 中，給定可見層 `v`，如何計算隱藏層 `h` 的條件機率 `P(h|v)`？它有什麼特性？

**A:** 給定可見層 `v`，隱藏層中每個單元 `h_j` 的激活是條件獨立的。其條件機率 `P(h_j=1|v) = σ(c_j + v^T W_{:,j})` (公式 20.14)，其中 `σ` 是 sigmoid 函數，`c_j` 是 `h_j` 的偏置，`W_{:,j}` 是連接到 `h_j` 的權重。因此，整個隱藏層的條件機率是各個隱藏單元條件機率的乘積：`P(h|v) = Π_j P(h_j|v)` (公式 20.15)。

**Q:** RBM 中的條件機率 `P(v|h)` 是否也具有類似的因子化特性？

**A:** 是的，類似於 `P(h|v)`，條件機率 `P(v|h)` 也是因子形式的。給定隱藏層 `h`，可見層中每個單元 `v_i` 的激活也是條件獨立的，其條件機率（對於二值可見單元）可以表示為 `P(v_i=1|h) = σ(b_i + W_{i,:} h)`。因此，`P(v|h) = Π_i P(v_i|h)` (公式 20.16)。

**Q:** 這些因子化的條件分佈對於 RBM 的訓練和推斷有何重要性？

**A:** 這些因子化的條件分佈使得可以高效地從 `P(h|v)` 和 `P(v|h)` 中進行採樣（塊吉布斯採樣）。這對於基於 MCMC 的訓練算法（如對比散度）至關重要，因為它們依賴於從模型分佈中生成樣本。

---

### 20.2.2 訓練受限玻爾茲曼機

**重點摘要:**
RBM 的訓練通常使用基於 MCMC 採樣的近似最大概似方法，例如對比散度（Contrastive Divergence, CD）、隨機最大概似（Stochastic Maximum Likelihood, SML，也稱為 Persistent CD, PCD）或比率匹配等。由於 RBM 可以高效計算 `P(h|v)` 的閉解形式，相較於其他一些更深層的無向模型（如深度玻爾茲曼機），其訓練相對直接。


**Q:** 訓練 RBM 的常用技術有哪些？

**A:** 訓練 RBM 的常用技術包括對比散度（CD）、隨機最大概似（SML 或 PCD）以及比率匹配等。這些方法用於近似計算難以處理的配分函數的梯度。

**Q:** 為什麼 RBM 相較於其他深度無向模型（如深度玻爾茲曼機）在訓練上相對直接？

**A:** 因為 RBM 允許以閉解形式計算 `P(h|v)`（即隱藏層在給定可見層時的條件機率），這簡化了許多訓練步驟，特別是基於 MCMC 的梯度估計。而其他一些深度模型可能同時面臨難處理的配分函數和難以推斷的後驗機率問題。

---

## 20.3 深度信念網路

**重點摘要:**
深度信念網路（Deep Belief Network, DBN）是最早成功應用深度架構訓練的非卷積模型之一，其引入開啟了當前深度學習的復興。DBN 是一個具有若干潛變量層的生成模型，潛變量通常是二值的，可見單元可以是二值或實數。DBN 的結構特點是：頂部兩層之間的連接是無向的（構成一個 RBM），而所有其他層之間的連接是有向的，箭頭指向最接近數據的層。其機率分佈由一系列條件機率和頂部 RBM 的聯合機率定義（公式 20.17-20.19）。DBN 的推斷是難解的，因為有向層內的「解釋消除」效應以及頂部兩層無向連接的相互作用。訓練 DBN 的標準方法是貪婪逐層訓練：首先訓練一個 RBM 來模擬輸入數據，然後將其隱藏層的激活作為下一個 RBM 的輸入進行訓練，以此類推。訓練好的 DBN 可以用作生成模型，或其權重可用於初始化一個多層感知機（MLP）進行分類任務（判別性微調）。


**Q:** 深度信念網路（DBN）的結構有何特點？

**A:** DBN 是一個混合圖模型。其頂部兩層構成一個無向的 RBM，而所有其他層之間的連接是有向的，並且箭頭通常指向最接近數據（可見層）的方向。層內沒有連接。

**Q:** DBN 的標準訓練過程是怎樣的？

**A:** DBN 通常採用貪婪逐層預訓練的方法。首先，訓練一個 RBM 來最大化 `P(v)`（其中 `v` 是可見數據）。然後，第一個 RBM 的隱藏層激活被用作訓練第二個 RBM 的輸入，這個過程可以重複以添加更多層。每個新的 RBM 都定義了 DBN 的另一層。

**Q:** DBN 中的推斷為何是難解的？

**A:** DBN 中的推斷是難解的，主要原因有兩個：1) 每個有向層內的相消解釋效應（explaining away effect）；2) 頂部兩個隱藏層之間無向連接的相互作用。

**Q:** 訓練好的 DBN 除了作為生成模型外，還有什麼常見用途？

**A:** 訓練好的 DBN 的權重和偏置經常被用來初始化一個多層感知機（MLP）的參數，然後再對該 MLP 進行判別性微調以執行分類等任務。

---

## 20.4 深度玻爾茲曼機

**重點摘要:**
深度玻爾茲曼機（Deep Boltzmann Machine, DBM）是另一種深度生成模型，與 DBN 不同，它是一個完全無向的模型。與 RBM 類似，DBM 的每一層內的每個變量都與相鄰層中的變量條件獨立（即層內無連接），但 DBM 擁有多個潛變量層。DBM 基於能量函數 `E` 參數化聯合機率分佈 `P(v, h^(1), ..., h^(L)) = (1/Z(θ))exp(-E(v, h^(1), ..., h^(L); θ))` (公式 20.24)。例如，一個三層隱藏層的 DBM 能量函數（省略偏置）為 `E = -v^T W^(1)h^(1) - h^(1)T W^(2)h^(2) - h^(2)T W^(3)h^(3)` (公式 20.25)。DBM 的二分圖結構（將奇數層和偶數層視為二分圖的兩部分）使得吉布斯採樣可以高效進行。


**Q:** 深度玻爾茲曼機（DBM）與深度信念網路（DBN）在模型結構上的主要區別是什麼？

**A:** 主要區別在於 DBM 是一個完全無向的模型，所有層之間的連接都是對稱的。而 DBN 是一個混合模型，頂部兩層是無向的（RBM），其餘層之間是有向的。

**Q:** DBM 的能量函數是如何定義的（以多層隱藏層為例）？

**A:** DBM 的能量函數表示為相鄰層之間交互作用的總和。例如，對於一個可見層 `v` 和三個隱藏層 `h^(1), h^(2), h^(3)` 的 DBM（省略偏置參數），能量函數可以定義為 `E(v, h^(1), h^(2), h^(3); θ) = -v^T W^(1)h^(1) - h^(1)T W^(2)h^(2) - h^(2)T W^(3)h^(3)` (公式 20.25)。

**Q:** DBM 的層結構如何使其能夠進行高效的吉布斯採樣？

**A:** DBM 的層可以組織成一個二分圖，其中奇數層在一側，偶數層在另一側（可見層通常視為第0層，即偶數層）。由於連接僅存在於相鄰層之間，當給定所有偶數層的狀態時，所有奇數層的單元都條件獨立，可以並行採樣。反之亦然。這使得塊吉布斯採樣非常高效。

---

### 20.4.1 有趣的性質

**重點摘要:**
DBM 相較於 DBN 的一個優點是其後驗分佈 `P(h|v)` 更簡單（儘管仍難解），這允許使用更豐富的後驗近似方法，特別是平均場（mean-field）近似。平均場推斷允許 DBM 捕獲自頂向下的反饋交互影響，這在神經科學上更具合理性。然而，DBM 的一個缺點是從中採樣相對困難，需要在所有層中使用 MCMC。


**Q:** DBM 在後驗分佈 `P(h|v)` 的近似方面相較於 DBN 有什麼優勢？

**A:** DBM 的後驗分佈 `P(h|v)`（儘管複雜）更容易用變分近似（特別是平均場近似）來近似。在 DBM 中，給定其他層的情況下，層內的所有隱藏單元都是條件獨立的。這種層內相互作用的缺失使得通過不動點方程優化變分下界並找到真正最佳的平均場期望值（在一些數值容差內）變得可能。

**Q:** 平均場推斷允許 DBM 捕獲哪種重要的交互影響，這在神經科學上有何意義？

**A:** 使用適當的平均場推斷允許 DBM 的近似推斷過程捕獲自頂向下的反饋交互作用的影響。這從神經科學的角度看是有趣的，因為已知人腦使用許多自上而下的反饋連接。

**Q:** DBM 在從模型生成樣本方面有什麼不足？

**A:** DBM 的一個不理想的特性是從中採樣相對困難。DBN 只需要在其頂部的一對層中使用 MCMC 採樣，其他層僅在採樣過程末尾涉及。而要從 DBM 生成樣本，必須在所有層中使用 MCMC，並且模型的每一層都參與每個馬爾可夫鏈轉移。

---

### 20.4.2 DBM均勻場推斷

**重點摘要:**
DBM 中，給定相鄰層，一層上的條件分佈是因子的。然而，所有隱藏層上的聯合後驗分佈 `P(h|v)` 通常不是因子的。為了進行推斷，通常使用平均場近似，將複雜的後驗分佈近似為一個完全因子化的分佈 `Q(h|v) = Π_j Q(h^(1)_j|v) Π_k Q(h^(2)_k|v) ...`。通過最小化 `KL(Q||P)`，可以導出迭代更新規則（不動點方程，如公式 20.33, 20.34），用於計算每個隱藏單元在 `Q` 分佈下的激活機率（例如 `ĥ^(1)_j` 和 `ĥ^(2)_k`）。這些不動點方程捕獲了層之間的雙向交互作用。


**Q:** 什麼是 DBM 中的平均場（均勻場）推斷？它試圖近似什麼？

**A:** DBM 中的平均場推斷是一種變分推斷方法，用於近似難以處理的後驗分佈 `P(h|v)`（給定可見單元 `v` 時所有隱藏單元 `h` 的分佈）。它將這個複雜的後驗分佈近似為一個更簡單的、完全因子化的分佈 `Q(h|v)`，其中所有隱藏單元在給定 `v` 時都是條件獨立的。

**Q:** 如何找到最佳的平均場近似分佈 `Q`？

**A:** 通過最小化 `Q` 和真實後驗 `P` 之間的 KL 散度 `KL(Q||P)`，可以導出一組不動點更新方程。這些方程迭代地更新 `Q` 分佈中每個隱藏單元的激活機率（如公式 20.33, 20.34 中的 `ĥ^(1)` 和 `ĥ^(2)`），直到收斂。

**Q:** 平均場推斷方程如何捕獲層之間的交互作用？

**A:** 平均場推斷的不動點方程（例如，`ĥ^(1)` 的更新依賴於 `v` 和 `ĥ^(2)`，而 `ĥ^(2)` 的更新依賴於 `ĥ^(1)`）隱式地捕獲了模型中各層之間的雙向交互作用。

---

### 20.4.3 DBM 的參數學習

**重點摘要:**
DBM 的學習面臨雙重挑戰：難解的配分函數和難解的後驗分佈。通過最大化對數概似的變分下界 `L(v, Q, θ)` (公式 20.35) 進行學習。這個下界仍然包含對數配分函數 `log Z(θ)`，因此其梯度需要近似。DBM 通常使用隨機最大概似（Stochastic Maximum Likelihood, SML）進行訓練。與 RBM 不同，DBM 的對比散度（CD）算法較慢，因為不能在給定可見單元時對隱藏單元進行高效採樣，需要運行完整的馬爾可夫鏈。算法 20.1 給出了一個訓練兩層隱藏層 DBM 的變分隨機最大概似算法。


**Q:** DBM 參數學習面臨的主要挑戰是什麼？

**A:** DBM 參數學習必須面對兩個主要挑戰：1) 難以處理的配分函數（如第十八章中的技術）；2) 難以處理的後驗分佈（如第十九章中的技術）。

**Q:** DBM 通常如何進行參數學習？目標函數是什麼？

**A:** DBM 通常通過最大化對數概似的變分下界 `L(v, Q, θ)` (公式 20.35) 來學習參數。這個下界由變分推斷（使用近似後驗 `Q(h|v)`）導出。

**Q:** 為什麼對比散度（CD）算法對於 DBM 來說通常比 RBM 慢？

**A:** 因為 DBM 不能在給定可見單元時對所有隱藏單元進行高效的條件獨立採樣（由於層間的依賴性，不同於 RBM 中隱藏單元在給定可見單元時的條件獨立性）。因此，每當需要新的負相樣本時，對比散度將需要運行（磨合）一條完整的馬爾可夫鏈來近似從模型聯合分佈中採樣，這比 RBM 的 CD-k 步驟更耗時。

---

### 20.4.4 逐層預訓練

**重點摘要:**
直接從隨機初始化開始訓練 DBM 通常效果不佳。一種常見且有效的方法是貪婪逐層預訓練：將 DBM 的每一層單獨視為一個 RBM 進行訓練。第一個 RBM 訓練為對輸入數據建模，後續每個 RBM 訓練為對前一個 RBM 後驗分佈的樣本進行建模。預訓練完成後，RBM 的參數需要進行調整（例如，除了頂部和底部 RBM，中間 RBM 的權重除以2）才能組合成 DBM，然後再進行聯合 PCD 訓練。這種預訓練有助於將模型參數初始化到一個較好的區域。


**Q:** 為什麼 DBM 通常需要逐層預訓練？

**A:** 因為從隨機初始化後直接使用隨機最大概似訓練 DBM 通常導致失敗，模型可能無法學習到如何充分表示分佈，或者性能不如僅使用 RBM。逐層預訓練有助於將模型參數初始化到一個更有利於後續聯合訓練的區域。

**Q:** DBM 的逐層預訓練與 DBN 的有何不同？

**A:** DBN 的逐層預訓練中，每個 RBM 的參數可以直接複製到相應的 DBN 層。而在 DBM 的情況下，RBM 的參數在組合成 DBM 之前必須進行修改。例如，Salakhutdinov and Hinton (2009a) 提倡在將其插入 DBM 之前，將所有 RBM（頂部和底部 RBM 除外）的權重除以2。此外，底部和頂部 RBM 的訓練可能需要使用其可見單元的兩個「副本」並約束權重。

**Q:** 在 DBM 的聯合 PCD 訓練步驟的負相期間，建議使用什麼技巧來獲得更好結果？

**A:** 為了使用深度玻爾茲曼機獲得最好結果，建議修改標準的 SML 算法，即在聯合 PCD 訓練步驟的負相期間使用少量的平均場推斷（Salakhutdinov and Hinton, 2009a）。具體來說，應當相對於其中所有單元彼此獨立的平均場分佈來計算能量梯度的期望。

---

### 20.4.5 聯合訓練深度玻爾茲曼機

**重點摘要:**
除了預訓練，還有其他聯合訓練 DBM 的方法。一種是中心化深度玻爾茲曼機（centered DBM），它通過重參數化模型（從所有狀態中減去一個向量 `μ`，如公式 20.37）來改善代價函數 Hessian 矩陣的條件數，從而可能無需預訓練即可訓練。另一種是多預測深度玻爾茲曼機（MP-DBM），其訓練準則允許使用反向傳播算法，避免了 MCMC 估計梯度的問題，但可能導致較差的概似或樣本質量，不過分類性能和缺失輸入推斷能力較好。MP-DBM 將平均場方程視為定義了一個循環網路，並訓練該網路對缺失值進行準確預測。


**Q:** 除了逐層預訓練後進行聯合訓練，還有哪些其他方法可以聯合訓練 DBM？請簡述其中一種。

**A:**
1.  **中心化深度玻爾茲曼機 (Centered DBM):** 通過對模型的狀態變量進行中心化（即 `x - μ`，如公式 20.37），改變了隨機梯度下降的動態，通常可以改善代價函數 Hessian 矩陣的條件數，使得模型可能在沒有預訓練的情況下就能成功訓練。
2.  **多預測深度玻爾茲曼機 (MP-DBM):** 該模型的訓練準則允許使用反向傳播算法來避免 MCMC 估計梯度的問題。它將平均場推斷方程視為一個循環網路，並訓練這個網路在給定部分輸入時準確預測剩餘（缺失）的單元值。雖然這可能不會產生好的概似或樣本，但它在分類和缺失輸入推斷方面表現更好。

**Q:** MP-DBM 的訓練方式與傳統 DBM 的最大概似訓練有何不同？它有哪些優缺點？

**A:** MP-DBM 不直接優化數據的對數概似。相反，它訓練一個推斷網路（基於平均場方程的循環網路）來準確預測輸入數據的任何子集，給定其餘部分。
*   **優點:** 允許使用反向傳播計算精確梯度，避免了 SML 訓練中梯度估計的偏差和方差問題，從而可以進行聯合訓練；在分類和缺失輸入填充任務上表現良好。
*   **缺點:** 訓練目標不是概似的下界，可能導致較差的生成樣本質量或概似值；訓練過程更像是啟發式的。

---

## 20.5 實值數據上的玻爾茲曼機

**重點摘要:**
標準玻爾茲曼機最初是為二值數據設計的，但許多應用（如圖像、音頻）需要處理實值數據。本節介紹了幾種將 RBM 擴展到實值可見單元的方法。

---

### 20.5.1 Gaussian-Bernoulli RBM

**重點摘要:**
Gaussian-Bernoulli RBM (GB-RBM) 是一種常見的 RBM 變體，其中可見單元是實值的（通常假設為高斯分佈），而隱藏單元是二值的。其條件分佈 `p(v|h)` 被建模為高斯分佈 `N(v; Wh, β⁻¹)` (公式 20.38)，其中 `β` 是精度矩陣（或其逆是協方差矩陣）。能量函數相應修改為 (公式 20.42)：`E(v,h) = (1/2)v^T(β⊙v) - (v⊙β)^TWh - b^T h`。精度 `β` 可以是固定的常數、可學習的標量、對角矩陣，但通常不允許非對角精度以避免矩陣求逆的複雜性。


**Q:** Gaussian-Bernoulli RBM (GB-RBM) 中可見單元和隱藏單元的類型是什麼？

**A:** 在 GB-RBM 中，可見單元 (`v`) 是實值的，其條件分佈通常假設為高斯分佈；隱藏單元 (`h`) 是二值的（伯努利分佈）。

**Q:** GB-RBM 的能量函數與標準二值 RBM 有何不同？

**A:** GB-RBM 的能量函數需要包含一個對應於高斯分佈可見單元的項。一種常見形式是 `E(v,h) = (1/2)v^T(β⊙v) - (v⊙β)^TWh - b^T h` (公式 20.42)，其中 `β` 是精度（或與精度相關的參數）。第一項是可見單元自身的能量，第二項是可見層與隱藏層的交互項，第三項是隱藏層的偏置項。

**Q:** 在 GB-RBM 中，精度矩陣 `β` 通常如何選擇或約束？為什麼？

**A:** 精度矩陣 `β` 可以被固定為常數（可能基於數據的邊緣精度估計），或者學習出來。它可以是標量乘以單位矩陣，或者是一個對角矩陣。通常不允許非對角的精度矩陣，因為高斯分佈的某些操作需要對精度矩陣求逆，而對角矩陣的求逆非常容易，非對角矩陣則複雜得多。

---

### 20.5.2 條件協方差的無向模型

**重點摘要:**
高斯 RBM 主要對給定隱藏單元的輸入條件均值建模，但不能捕獲條件協方差信息。為了更好地處理某些實值數據（尤其是自然圖像，其信息常嵌入像素間的協方差中），提出了幾種模型：
*   **均值和協方差 RBM (mcRBM):** 使用兩組隱藏單元，一組 (`h^(m)`) 建模條件均值（類似高斯RBM），另一組 (`h^(c)`) 建模條件協方差。能量函數是兩部分能量之和 `E_mc = E_m + E_c` (公式 20.43, 20.44, 20.45)。這導致了非對角的條件協方差矩陣 `C_mc_xh` (公式 20.47)。
*   **學生t分佈均值乘積 (mPoT) 模型:** 擴展了 PoT 模型，也產生多元高斯條件分佈，但隱藏變量的條件分佈是 Gamma 分佈。
*   **尖峰和平板 RBM (ssRBM):** 使用二值「尖峰」單元 (`h`) 和實值「平板」單元 (`s`)。可見單元的均值由 `(h⊙s)W` 給出。`h_i=1` 表示分量激活，`s_i` 表示強度。這允許對輸入的協方差建模，通過尖峰單元調節精度矩陣中的項（公式 20.53）。ssRBM 的優點是不需要矩陣求逆，可以使用吉布斯採樣的對比散度。


**Q:** 為什麼標準的高斯 RBM 可能不適合某些類型的實值數據，例如自然圖像？

**A:** 因為標準高斯 RBM 主要對給定隱藏單元的輸入條件均值進行建模，而不能很好地捕獲像素之間的條件協方差信息。在自然圖像等數據中，許多有用的信息內容嵌入於像素之間的協方差（關係）中，而不是原始像素值本身。

**Q:** 均值和協方差 RBM (mcRBM) 如何同時建模條件均值和條件協方差？

**A:** mcRBM 的隱藏層分為兩組單元：均值單元 (`h^(m)`) 和協方差單元 (`h^(c)`)。均值單元的作用類似於標準高斯 RBM 中的隱藏單元，用於建模條件均值。協方差單元則用於對條件協方差的結構進行建模。其能量函數是這兩部分能量的組合（公式 20.43-20.45），使得給定 `h^(m)` 和 `h^(c)` 時，`x` 的條件分佈是一個具有非對角協方差矩陣的多元高斯分佈（公式 20.47）。

**Q:** 尖峰和平板 RBM (ssRBM) 是如何利用「尖峰」和「平板」變量來建模實值數據的？它相較於 mcRBM 的一個主要優勢是什麼？

**A:** ssRBM 使用兩類隱藏單元：二值尖峰 (spike) 單元 `h` 和實值平板 (slab) 單元 `s`。當尖峰單元 `h_i=1` 時，表示相應的特徵（由權重 `W_{:,i}` 定義）是活躍的，而相應的平板變量 `s_i` 決定了該特徵的強度或方差貢獻。這允許模型對輸入的協方差進行建模，尖峰單元 `h_i` 會調節可見單元 `x` 的精度矩陣中的項（公式 20.53）。
    相較於 mcRBM，ssRBM 的一個主要優勢是其訓練（如使用吉布斯採樣的對比散度）和推斷不需要對任何矩陣進行求逆，這在計算上更為方便。

---

## 20.6 卷積玻爾茲曼機

**重點摘要:**
為了處理具有空間平移不變性或時間結構的超高維輸入（如圖像），可以將卷積結構（小核、權重共享）引入玻爾茲曼機（特別是RBM）。一個挑戰是如何將池化操作推廣到基於能量的模型。Lee et al. (2009) 提出了概率最大池化（probabilistic max pooling），它約束一個池化區域內的檢測器單元中最多只有一個可以處於活動狀態，從而使池化單元的狀態數從指數級降為線性級（`n+1`個狀態）。這使得構建卷積深度玻爾茲曼機成為可能，儘管在實際應用中仍面臨挑戰，如輸入尺寸變化和邊界效應。


**Q:** 為什麼要將卷積結構引入玻爾茲曼機？

**A:** 對於圖像等超高維輸入，它們通常具有空間平移不變性或時間結構。使用小核的離散卷積來替換傳統的全連接矩陣乘法（權重共享）是解決這類輸入問題的標準方式，可以顯著減少模型參數數量並利用數據的局部結構，從而提高統計效率和計算效率。

**Q:** 在基於能量的模型中推廣池化操作有何困難？概率最大池化是如何解決這個問題的？

**A:** 在前饋卷積網路中，池化函數（如最大值）是確定性的。但在基於能量的模型中，所有變量都是隨機的，直接計算包含池化約束的配分函數非常困難（例如，一個 `3x3` 的池化區域，每個檢測器單元是二值的，會有 `2^9=512` 個可能的能量狀態需要評估）。
    概率最大池化通過約束一個池化區域內的 `n` 個檢測器單元中，在任何時候最多只有一個可以處於活動狀態。這將池化區域的總狀態數減少到 `n+1`（`n` 個檢測器各有一個活動狀態，以及所有檢測器都關閉的狀態），從而使得計算可行。

**Q:** 卷積玻爾茲曼機在處理圖像邊界和可變輸入尺寸方面可能遇到哪些問題？

**A:**
*   **圖像邊界:** 由於玻爾茲曼機中連接的對稱性，如果不在邊界處進行隱式補零輸入，則會導致隱藏單元數量少於（或以不同方式連接到）可見單元，使得邊界處的可見單元建模不佳。隱式補零輸入則可能導致邊界處的隱藏單元由較少的輸入像素驅動，可能在需要時無法激活。
*   **可變輸入尺寸:** 配分函數會隨著輸入大小的改變而改變。許多卷積網路通過按比例縮放池化區域來實現尺寸不變性，但縮放玻爾茲曼機池化區域並不優雅，且大型池化區域的計算成本很高。概率最大池化雖然解決了計算問題，但仍不允許大小可變的池化區域。

---

## 20.7 用於結構化或序列輸出的玻爾茲曼機

**重點摘要:**
玻爾茲曼機可以擴展到建模條件機率分佈 `p(y|x)`，從而處理結構化輸出任務（`y` 的不同條目相關且需滿足約束）或序列建模任務（估計 `p(x^(1), ..., x^(τ))`，或 `p(x^(τ)|x^(1), ..., x^(τ-1))`）。
*   **條件 RBM (CRBM):** Taylor et al. (2007) 引入了 CRBM，用於建模 `p(x^(t)|x^(t-1), ..., x^(t-m))`。RBM 的偏置參數被設置為 `x` 前面 `m` 個值的線性函数。
*   **RNN-RBM:** Boulanger-Lewandowski et al. (2012) 提出 RNN-RBM，其中一個循環神經網路（RNN）在每個時間步生成 RBM 的所有參數（包括權重和偏置）。訓練時，損失函數應用於 RBM，其梯度需要通過 RNN 反向傳播。


**Q:** 玻爾茲曼機如何擴展以處理結構化輸出或序列數據？

**A:** 可以通過將玻爾茲曼機（特別是 RBM）修改為條件模型來實現。例如，在結構化輸出中，模型學習 `p(y|x)`，其中 `y` 的組件是相關的。在序列建模中，模型可以學習 `p(x^(t)|x^(<t))` 形式的因子。

**Q:** 條件 RBM (CRBM) 是如何工作的？一個例子是什麼？

**A:** CRBM 的思想是讓 RBM 的參數（通常是偏置）成為條件變量的函數。例如，在 Taylor et al. (2007) 的工作中，用於建模 `p(x^(t)|x^(t-1), ..., x^(t-m))` 的 RBM，其在 `x^(t)` 上的偏置參數是 `x` 的前 `m` 個歷史值的線性函數。這樣，RBM 的能量景觀會根據歷史上下文動態變化。

**Q:** RNN-RBM 與 CRBM 在參數生成上有何不同？

**A:** 在 CRBM 的某些變體中（如 Taylor et al. 的模型），只有 RBM 的偏置參數依賴於上下文。而在 RNN-RBM 中，一個 RNN 在每個時間步生成 RBM 的所有參數，包括權重和偏置。這使得 RNN-RBM 能夠學習更複雜的時變動態。

---

## 20.8 其他玻爾茲曼機

**重點摘要:**
玻爾茲曼機的框架非常豐富，允許許多變種和擴展。
*   **判別式 RBM:** 可以訓練 RBM 來最大化條件概似 `log p(y|v)`，而不僅僅是生成標準的 `log p(v)`。
*   **高階玻爾茲曼機:** 能量函數可以包含涉及多個變量乘積的項（而不僅僅是二階交互），例如用於建模視頻幀間空間變換的三向交互。
*   **選通 (Gating) 機制:** 可以使用 one-hot 類別變量的乘法來改變可見單元和隱藏單元之間的關係，或用於選通某些特徵以消除與分類問題不相關的輸入。
開發新形式的玻爾茲曼機需要細心和創造力，因為通常很難找到能保持模型所需的不同條件分佈可解性的能量函數。


**Q:** 除了標準的生成式 RBM，還有哪些其他類型的 RBM 或玻爾茲曼機變體？請舉例說明。

**A:**
1.  **判別式 RBM:** 旨在最大化條件概似 `log p(y|v)`，用於分類等任務，而不是生成式地建模 `p(v)`。
2.  **高階玻爾茲曼機:** 能量函數中包含超過兩個隨機變量乘積的項。例如，Memisevic and Hinton (2007, 2010) 使用隱藏單元和兩個不同圖像（例如視頻的連續幀）之間的三向交互來建模空間變換。
3.  **選通 RBM:** 如 Sohn et al. (2013) 介紹的帶有三階交互的玻爾茲曼機，其中與每個可見單元相關的二進制掩碼變量可以「選通」或消除該可見單元對隱藏單元的影響，從而移除與分類任務不相關的輸入。

**Q:** 為什麼開發新形式的玻爾茲曼機通常具有挑戰性？

**A:** 因為玻爾茲曼機的許多理想特性（如高效的條件採樣、某些近似推斷方法的可行性）依賴於能量函數的特定形式。設計新的能量函數來引入更複雜的交互或功能，同時保持模型所需的不同條件分佈的可解性（tractability），通常非常困難，需要細心和創造力。

---

## 20.9 通過隨機操作的反向傳播

**重點摘要:**
在生成模型中，常希望神經網路實現隨機變換。一種方法是引入額外的隨機輸入 `z`（從簡單分佈採樣），使得確定性函數 `f(x,z)` 對於觀察者來說是隨機的。
*   **重參數化技巧 (Reparameterization Trick):** 對於連續隨機變量 `y ~ N(μ, σ^2)`，可以將其重寫為 `y = μ + σz`，其中 `z ~ N(0,1)`。這樣，梯度可以通過 `μ` 和 `σ` 反向傳播。這也被稱為隨機反向傳播或擾動分析。
*   **REINFORCE 算法:** 當模型輸出離散變量 `y` 時，`f(z;ω)` 必須是階躍函數，其導數幾乎處處為零。REINFORCE 算法利用 `E_z[J(y)]` 的梯度是 `E_z[J(y)∇_ω log p(y;ω)]` (公式 20.61) 這一思想，允許通過蒙特卡洛估計梯度。該估計通常具有高方差，可以使用基線 (baseline) 進行方差縮減。


**Q:** 什麼是重參數化技巧？它為什麼在某些生成模型（如 VAE）的訓練中很重要？

**A:** 重參數化技巧是一種將隨機採樣過程改寫的方法，使得隨機性從模型的參數中分離出來，變成一個固定的外部隨機源。例如，從高斯分佈 `y ~ N(μ, σ^2)` 中採樣可以改寫為 `y = μ + σz`，其中 `z` 是從標準正態分佈 `N(0,1)` 中採樣的。
    這在 VAE 等模型中很重要，因為它允許梯度通過採樣步驟反向傳播到參數 `μ` 和 `σ`。如果直接從 `N(μ, σ^2)` 採樣，採樣操作本身是不可微的，梯度無法流動。

**Q:** 當模型輸出離散隨機變量時，為什麼不能直接使用重參數化技巧？此時可以使用什麼算法來估計梯度？

**A:** 當模型輸出離散變量時，生成函數 `f(z;ω)` 必須是階躍函數。階躍函數的導數在平台區域幾乎處處為零，在階躍邊界處未定義，因此基於梯度的優化無法直接獲得有關如何更新模型參數的有用信息。
    此時可以使用強化學習算法（如 REINFORCE 算法）的變體來估計梯度。REINFORCE 的核心思想是，即使 `J(f(z;ω))` 的導數無用，期望代價 `E_z[J(f(z;ω))]` 通常是光滑函數，其梯度可以通過 `E_z[J(y)∇_ω log p(y;ω)]` (公式 20.61) 來估計。

**Q:** REINFORCE 算法估計的梯度有什麼潛在問題？如何緩解？

**A:** REINFORCE 估計的梯度通常具有非常高的方差，這意味著需要採集大量樣本才能獲得對梯度的良好估計，或者在使用少量樣本（如單個樣本）時，SGD 收斂會非常緩慢且需要較小的學習率。
    可以使用方差縮減 (variance reduction) 技術來緩解這個問題，其中一種常見方法是引入一個不依賴於當前樣本 `y` 的基線 (baseline) `b(ω)`，並使用 `(J(y) - b(ω))∇_ω log p(y;ω)` 作為梯度估計量 (公式 20.69)。

---

## 20.10 有向生成網路

**重點摘要:**
有向圖模型（也稱信念網路）是另一類重要的圖模型。本節回顧了一些與深度學習社群相關的標準有向圖模型。

---

### 20.10.1 Sigmoid 信念網路

**重點摘要:**
Sigmoid 信念網路 (SBN) 是一種簡單形式的有向圖模型，具有二值狀態向量 `s`，其中每個元素 `s_i` 的條件機率由其祖先決定，通常通過 sigmoid 函數參數化：`p(s_i=1|pa(s_i)) = σ(b_i + Σ_{j<i} W_{j,i}s_j)` (公式 20.70)。常見結構是分層的，從獨立的頂層單元開始採樣，逐層向下生成，最終產生可見層。SBN 是可見單元上機率分佈的通用近似器。然而，給定可見單元，對隱藏單元的推斷是難解的。現代方法如重要性加權醒眠算法 (IWAS) 或雙向 Helmholtz 機有助於更有效地訓練 SBN。


**Q:** Sigmoid 信念網路 (SBN) 的基本結構和條件機率是如何定義的？

**A:** SBN 是一個有向無環圖模型，通常具有二值狀態單元。每個單元 `s_i` 的條件機率 `p(s_i|pa(s_i))`（給定其父節點 `pa(s_i)`）由一個 sigmoid 函數參數化，其輸入是父節點的線性組合加上一個偏置。例如，`p(s_i=1|pa(s_i)) = σ(b_i + Σ_{s_j ∈ pa(s_i)} W_{j,i}s_j)` (類似公式 20.70，其中 `j<i` 暗示了一種拓撲排序)。最常見的結構是分層的。

**Q:** 訓練 Sigmoid 信念網路（以及一般的有向離散網路）的主要困難是什麼？

**A:** 主要困難在於推斷。給定可見層的狀態，推斷隱藏層的後驗機率是難解的，因為存在「解釋消除 (explaining away)」效應。這使得精確計算變分下界或梯度變得困難。

**Q:** 有哪些現代方法可以更有效地訓練 Sigmoid 信念網路？

**A:** 最近基於重要性採樣、重加權的醒眠算法 (Bornschein and Bengio, 2015) 或雙向 Helmholtz 機 (Bornschein et al., 2015) 的方法使得可以快速訓練 SBN，並在基準任務上達到最佳表現。

---

### 20.10.2 可微生成器網路

**重點摘要:**
許多生成模型基於可微生成器網路 `g(z; θ^(g))` 的思想，該網路將潛變量 `z` 的樣本（通常從簡單分佈如高斯或均勻分佈中採樣）變換為樣本 `x` 或 `x` 上的分佈。`g` 通常由神經網路表示。
*   如果 `g` 直接輸出樣本 `x = g(z)` (公式 20.71)，且 `g` 可逆，則 `x` 的機率密度為 `p_x(x) = p_z(g⁻¹(x)) / |det(∂g/∂z)|` (公式 20.73)。
*   如果 `g` 輸出 `x` 的條件分佈參數（例如 Bernoulli 分佈的均值 `p(x_i=1|z) = g(z)_i` (公式 20.74)），則 `p(x) = E_z[p(x|z)]` (公式 20.75)。
這兩種方法都定義了一個分佈 `p_g(x)`，並允許使用重參數化技巧（如果 `z` 和 `x` 連續）或 REINFORCE（如果 `x` 離散）來訓練。困難在於如何設計和優化這些難以處理的準則，因為數據沒有指定 `z` 和 `x` 的對應關係。


**Q:** 什麼是可微生成器網路的基本思想？

**A:** 可微生成器網路 `g(z; θ^(g))` 是一個可微函數（通常是神經網路），它將從一個簡單的潛在分佈（如高斯或均勻分佈）中採樣得到的潛變量 `z` 轉換為數據空間中的樣本 `x`，或者轉換為 `x` 上的分佈的參數。

**Q:** 可微生成器網路可以通過哪兩種主要方式定義輸出 `x` 的分佈？

**A:**
1.  **直接生成樣本 (Directly emitting samples):** 生成器 `g` 直接輸出樣本 `x = g(z)`。如果 `g` 是可逆且可微的，則 `x` 的機率密度可以通過變量變換公式計算 (公式 20.73)。
2.  **生成條件分佈的參數 (Emitting parameters of a conditional distribution):** 生成器 `g` 輸出 `x` 在給定 `z` 時的條件分佈 `p(x|z)` 的參數。例如，如果 `x` 是二值的，`g(z)_i` 可以是 `p(x_i=1|z)`。則 `x` 的邊緣分佈是 `p(x) = E_z[p(x|z)]` (公式 20.75)。

**Q:** 訓練可微生成器網路的主要困難是什麼？

**A:** 主要困難在於學習準則通常是難以處理的，因為訓練數據僅提供 `x` 的樣本，而不指定其對應的潛變量 `z`。這與監督學習不同，監督學習中輸入和期望輸出都已給定。在生成建模中，學習過程需要確定如何有用地組織 `z` 空間以及如何從 `z` 映射到 `x`。

---

### 20.10.3 變分自編碼器

**重點摘要:**
變分自編碼器 (Variational Auto-Encoder, VAE) 是一種使用學好的近似推斷的有向模型，可以純粹使用基於梯度的方法進行訓練。VAE 包含一個編碼器（推斷網路或識別模型）`q(z|x)` 和一個解碼器（生成網路）`p_model(x|z)`。它通過最大化與數據點 `x` 相關的變分下界 `L(q)` (Evidence Lower Bound, ELBO) 來訓練 (公式 20.77)：
`L(q) = E_{z~q(z|x)}[log p_model(x|z)] - D_KL(q(z|x) || p_model(z))`
第一項是重構對數概似，第二項是近似後驗與模型先驗之間的 KL 散度，起正則化作用。如果 `z` 是連續的，可以使用重參數化技巧通過 `q(z|x)`（通常是高斯分佈，其均值和方差由編碼器網路輸出）反向傳播。VAE 的樣本有時會比較模糊。重要性加權自編碼器 (IWAE) 提供了更緊的下界。


**Q:** 變分自編碼器 (VAE) 的兩個主要組件是什麼？它們各自的作用是什麼？

**A:** VAE 的兩個主要組件是：
1.  **編碼器 (Encoder) / 推斷網路 / 識別模型 `q(z|x)`:** 給定輸入數據 `x`，它輸出潛變量 `z` 的近似後驗分佈的參數（通常是高斯分佈的均值和方差）。
2.  **解碼器 (Decoder) / 生成網路 `p_model(x|z)`:** 給定潛變量 `z`，它輸出數據 `x` 的條件機率分佈的參數，用於重構或生成數據。

**Q:** VAE 的訓練目標是什麼？請解釋其主要組成部分。

**A:** VAE 的訓練目標是最大化證據下界 (ELBO)，即 `L(q) = E_{z~q(z|x)}[log p_model(x|z)] - D_KL(q(z|x) || p_model(z))` (公式 20.77)。
*   `E_{z~q(z|x)}[log p_model(x|z)]`: 期望重構對數概似。它鼓勵解碼器能夠根據從編碼器得到的潛在表示 `z` 來準確地重構輸入 `x`。
*   `- D_KL(q(z|x) || p_model(z))`: 負的 KL 散度項。它是一個正則化項，懲罰編碼器產生的近似後驗 `q(z|x)` 與預設的潛變量先驗 `p_model(z)`（通常是標準正態分佈）之間的差異，促使潛在空間結構化。

**Q:** VAE 生成的樣本有時會出現什麼問題？是否有改進方法？

**A:** 從圖像上訓練的 VAE 中採樣的樣本往往有些模糊。這可能是由於最大概似目標的固有特性，或者是因為 VAE 通常在解碼器 `p_model(x|g(z))` 中使用高斯分佈，這與訓練具有均方誤差的傳統自編碼器類似。
    重要性加權自編碼器 (IWAE) 提出了一個更緊的 ELBO，通過從 `q(z|x)` 中採樣多個 `z` 並對其進行加權平均來估計 `log p_model(x)`。

---

### 20.10.4 生成對抗網路 (續)

**Q&A (續):**
**Q:** GAN 的典型價值函數是什麼？生成器和判別器的目標分別是什麼？

**A:** 典型的價值函數為 `v(G, D) = E_{x~p_data}[log D(x)] + E_{z~p_z}[log(1-D(G(z)))]` (公式 20.81)。
*   **判別器的目標:** 最大化 `v(G,D)`，即準確地將真實樣本識別為真（`D(x)` 接近 1），並將生成器生成的偽造樣本識別為假（`D(G(z))` 接近 0，從而 `log(1-D(G(z)))` 接近 `log(1)=0`）。
*   **生成器的目標:** 最小化 `v(G,D)`。實際上，為了獲得更好的梯度特性，生成器通常最大化 `E_{z~p_z}[log D(G(z))]`，即讓判別器將其生成的樣本誤認為是真實的（`D(G(z))` 接近 1）。

**Q:** 訓練 GAN 可能會遇到哪些主要挑戰？

**A:** 訓練 GAN 的主要挑戰包括：
1.  **不穩定性 (Instability):** 由於是 minimax 博弈，訓練過程可能不穩定，生成器和判別器可能無法達到一個良好的平衡點。
2.  **模式崩潰 (Mode Collapse):** 生成器可能只學會生成數據分佈中的少數幾種模式（即樣本多樣性不足），即使這些模式能夠很好地欺騙判別器。
3.  **梯度消失/爆炸 (Vanishing/Exploding Gradients):** 尤其是在早期版本的 GAN 中，如果判別器過於強大或過於弱小，生成器的梯度可能會消失或過大。
4.  **難以評估 (Difficulty in Evaluation):** 缺乏像對數概似這樣明確的評估指標，評估 GAN 的性能和收斂性比較困難。

---

### 20.10.5 生成矩匹配網路

**重點摘要:**
生成矩匹配網路 (Generative Moment Matching Network, GMMN) 是另一種基於可微生成器網路的生成模型。與 VAE 和 GAN 不同，它不需要將生成器網路與其他網路（如推斷網路或判別器網路）配對。GMMN 使用稱為矩匹配 (moment matching) 的技術進行訓練，其目標是使模型生成樣本的許多統計量（矩）盡可能與訓練集中樣本的統計量相似。它通過最小化一個稱為最大平均偏差 (Maximum Mean Discrepancy, MMD) 的代價函數來實現這一點。MMD 通過向核函數定義的特徵空間隱式映射，在無限維空間中測量第一矩的誤差，使得對無限維向量的計算變得可行。當且僅當比較的兩個分佈相等時，MMD 代價為零。


**Q:** 生成矩匹配網路 (GMMN) 的訓練目標是什麼？它與 GAN 和 VAE 有何不同？

**A:** GMMN 的訓練目標是使生成器生成的樣本的矩（統計量）與真實數據樣本的矩盡可能匹配。它通過最小化最大平均偏差 (MMD) 代價函數來實現。
    與 GAN 不同，GMMN 不需要一個判別器網路。與 VAE 不同，GMMN 不需要一個編碼器/推斷網路。它直接比較生成樣本和真實樣本在一個核誘導的特徵空間中的矩。

**Q:** 什麼是最大平均偏差 (MMD)？它在 GMMN 中扮演什麼角色？

**A:** 最大平均偏差 (MMD) 是一種用於度量兩個機率分佈之間差異的統計量。它通過將樣本映射到一個高維（甚至無限維）的再生核希爾伯特空間 (RKHS) 中，並比較它們在該空間中的均值（第一矩）的差異。當且僅當兩個分佈完全相同時，MMD 為零。
    在 GMMN 中，MMD 被用作代價函數，訓練生成器以最小化其生成的樣本分佈與真實數據分佈之間的 MMD。

**Q:** 訓練 GMMN 時，批量大小 (batch size) 有何重要性？

**A:** 由於矩的計算是基於樣本的經驗平均值，批量大小對於 MMD 的準確估計非常重要。
*   如果批量太小，MMD 可能會低估採樣分佈的真實變化量，導致訓練不穩定或效果不佳。
*   如果批量太大，雖然估計更準確，但訓練過程會變慢，因為計算單個小梯度步長需要處理許多樣本。

---

### 20.10.6 卷積生成網路

**重點摘要:**
在生成圖像時，將卷積結構引入生成器網路通常是有用的。這通常通過使用卷積算子的「轉置」（也常被稱為反卷積或解卷積，如第 9.5 節所述）來實現。這種方法能產生更逼真的圖像，並且比使用全連接層需要更少的參數。在識別任務的卷積網路中，信息流通常是從圖像到網路頂部的概括層（如類標籤），過程中信息被丟棄（例如通過池化層）。而在生成器網路中，情況相反，需要從抽象的潛在表示開始，逐步添加豐富的詳細信息，最終產生精細的圖像。由於大多數池化函數不可逆，不能直接將其反轉後放入生成器網路。一種常見的簡化操作是僅僅增加表示的空間大小，例如 Dosovitskiy et al. (2015) 引入的「去池化 (unpooling)」操作，它對應於某些簡化條件下最大池化的逆操作。


**Q:** 在生成圖像的生成器網路中，如何實現類似卷積的上採樣操作？

**A:** 通常使用卷積算子的「轉置」（transposed convolution），有時也被稱為反卷積 (deconvolution) 或分數步長卷積 (fractionally strided convolution)。這種操作可以將低解析度的特徵圖上採樣到高解析度的特徵圖，同時學習合適的濾波器。

**Q:** 生成器網路中的信息流與識別任務的卷積網路有何不同？這對網路設計有何啟示？

**A:**
*   **識別網路:** 信息從具體的圖像輸入流向抽象的、概括的表示（如類標籤），過程中通過池化等操作丟棄細節信息，保持對無關變換的不變性。
*   **生成網路:** 信息從抽象的潛在表示流向具體的、細節豐富的圖像輸出，過程中需要逐步添加和細化信息。
    這意味著生成網路需要類似於「反池化」或上採樣的機制來增加表示的空間維度和細節。

**Q:** 什麼是「去池化 (unpooling)」操作？它如何工作？

**A:** 「去池化」是一種簡化的上採樣操作，旨在近似最大池化的逆操作。例如，Dosovitskiy et al. (2015) 提出的版本中，假設最大池化的步幅等於池化區域的寬度，並且每個池化區域內的最大輸入被假定為左上角的輸入，其餘非最大輸入為零。去池化操作會將輸入張量中的每個值複製到輸出張量中一個 `k x k`（`k` 是池化區域大小）的塊的左上角，其餘位置填充零。雖然這個假設不現實，但後續的卷積層可以學習補償這種不尋常的輸出。

---

### 20.10.7 自回歸網路

**重點摘要:**
自回歸網路是沒有潛在隨機變量的有向機率模型。它們通過機率的鏈式法則分解觀察變量上的聯合機率，從而獲得形如 `P(x_d | x_{d-1}, ..., x_1)` 的條件機率的乘積。這些條件機率由神經網路表示。這種模型被稱為完全可見貝葉斯網路 (FVBN)。早期的 FVBN 使用邏輯回歸對每個條件分佈建模，後來擴展到帶有隱藏單元的神經網路。參數共享是自回歸網路中的一個重要主題，可以帶來統計和計算優勢。


**Q:** 什麼是自回歸網路的基本原理？

**A:** 自回歸網路基於機率的鏈式法則將高維數據的聯合機率分佈 `P(x_1, x_2, ..., x_d)` 分解為一系列條件機率的乘積：`P(x) = Π_{i=1}^d P(x_i | x_1, ..., x_{i-1})`。然後，每個條件機率 `P(x_i | x_{<i})` 都由一個神經網路（或更簡單的模型）來建模。

**Q:** 完全可見貝葉斯網路 (FVBN) 與自回歸網路是什麼關係？

**A:** 自回歸網路是完全可見貝葉斯網路 (FVBN) 的一種實現方式。FVBN 的圖結構是一個完全圖（或者說，變量之間存在一個拓撲排序，使得每個變量的父節點是其所有先序節點），並且沒有潛在變量。自回歸網路使用神經網路來參數化這些 FVBN 中的條件機率。

**Q:** 參數共享在自回歸網路中有何重要性？

**A:** 參數共享可以帶來統計優勢（需要學習的唯一參數較少，有助於泛化）和計算優勢（可能減少計算量）。例如，在 NADE 模型中，不同條件分佈 `P(x_i | x_{<i})` 的神經網路之間共享了大部分權重。

---

### 20.10.8 線性自回歸網路

**重點摘要:**
自回歸網路的最簡單形式是沒有隱藏單元、沒有參數或特徵共享的形式。每個條件機率 `P(x_i | x_{i-1}, ..., x_1)` 被參數化為線性模型（例如，對於實值數據是線性回歸，對於二值數據是邏輯回歸，對於離散數據是 softmax 回歸）。如果變量是連續的，線性自回歸網路只是表示多元高斯分佈的另一種方式，只能捕獲觀察變量之間線性的成對相互作用。它們與線性分類器具有相似的優缺點：可以用凸損失函數訓練，有時允許閉解形式，但模型容量有限。


**Q:** 線性自回歸網路如何對條件機率 `P(x_i | x_{<i})` 建模？

**A:** 線性自回歸網路使用線性模型來參數化每個條件機率。例如：
*   對於實值數據 `x_i`，`P(x_i | x_{<i})` 可以是線性回歸模型，即 `x_i` 的期望是 `x_{<i}` 的線性函數。
*   對於二值數據 `x_i`，`P(x_i=1 | x_{<i})` 可以是邏輯回歸模型。
*   對於離散數據 `x_i`，`P(x_i | x_{<i})` 可以是 softmax 回歸模型。

**Q:** 線性自回歸網路在表達能力上有什麼局限性？

**A:** 如果變量是連續的，線性自回歸網路只能表示多元高斯分佈，因此它只能捕獲觀察變量之間線性的成對相互作用。它的模型容量有限，無法表達複雜的非線性依賴關係。

**Q:** 線性自回歸網路與線性分類器在優缺點上有何相似之處？

**A:**
*   **優點:** 可以使用凸損失函數進行訓練，有時（如在高斯情況下）允許閉解形式的解，訓練相對簡單高效。
*   **缺點:** 模型本身不提供增加其容量的方法（即表達能力有限），必須使用其他技術（如輸入的基擴展或核技巧）來提高容量。

---

### 20.10.9 神經自回歸網路

**重點摘要:**
神經自回歸網路（如 Bengio and Bengio, 2000a,b 提出的）在圖模型結構上與邏輯自回歸網路相同，但使用帶有隱藏單元的神經網路來參數化每個條件分佈 `P(x_i | x_{<i})`。這大大增強了模型的表達能力，允許近似任意聯合分佈。此外，可以引入參數共享和特徵共享，例如，用於預測 `x_i` 所計算的隱藏層特徵可以重新用於預測後續的 `x_{i+k}` (k > 0)。這避免了傳統表格圖模型引起的維數災難。


**Q:** 神經自回歸網路與線性自回歸網路在參數化條件分佈方面有何不同？

**A:** 神經自回歸網路使用帶有隱藏單元的神經網路來參數化每個條件分佈 `P(x_i | x_{<i})`，而不是像線性自回歸網路那樣使用簡單的線性模型。這使得模型能夠學習更複雜的、非線性的條件依賴關係。

**Q:** 神經自回歸網路相比於傳統的表格離散機率模型有哪些優點？

**A:**
1.  **避免維數災難:** 通過使用神經網路參數化，即使輸入維度很高（`i-1` 很大），也不需要指數級數量的參數來估計條件機率，而是可以捕獲高階依賴性。
2.  **參數/特徵共享:** 允許在預測不同 `x_i` 時共享神經網路的參數或隱藏層的特徵，從而提高統計效率和泛化能力。例如，用於預測 `x_i` 的隱藏表示可以被重用於預測 `x_{i+1}, x_{i+2}, ...`。

**Q:** 圖 20.9 中展示的神經自回歸網路的特徵重用是如何實現的？

**A:** 圖 20.9 顯示，用於預測第 `i` 個變量 `x_i`（基於輸入 `x_1, ..., x_{i-1}`）所計算得到的隱藏層表示（圖中的 `h_1, h_2, h_3` 等）可以被後續預測 `x_{i+1}, x_{i+2}, ...` 的神經網路作為額外的輸入或特徵。這樣，早期的計算結果被重用，體現了特徵共享的思想。

---

### 20.10.10 NADE

**重點摘要:**
神經自回歸密度估計器 (Neural Autoregressive Density Estimator, NADE) (Larochelle and Murray, 2011) 是一種非常成功的神經自回歸網路形式。它與 Bengio and Bengio (2000b) 的原始神經自回歸網路具有相同的從左到右的連接結構，但 NADE 引入了特定的參數共享方案（如圖 20.10 和公式 20.83 所示）：從第 `k` 個輸入 `x_k` 到第 `j` 組隱藏單元（`j ≥ k`）中第 `l` 個元素的權重 `W_{j,l,k}` 在所有這樣的 `j` 組中是共享的，即 `W_{j,l,k} = W_{l,k}`。這種共享方案使得 NADE 的正向傳播與在 RBM 中執行均勻場推斷以填充缺失輸入的計算大致相似。NADE 架構可以擴展為 NADE-k（模擬均勻場循環推斷的 k 個時間步），也可以擴展到處理連續數據（如使用高斯混合模型作為輸出層，稱為 RNADE）。還有一種擴展是擺脫了對觀察變量任意順序的選擇需求，允許通過隨機採樣順序來處理任何順序，並形成多個排序模型的集成。


**Q:** NADE (神經自回歸密度估計器) 的參數共享方案有何特點？

**A:** NADE 的一個關鍵特點是其特定的權重共享模式。對於從輸入單元 `x_k` 到隱藏單元 `h_l^{(j)}`（表示用於預測 `x_j` 的第 `l` 個隱藏單元，且 `j > k`）的權重，這個權重 `W_{l,k}` 是共享的，即它不依賴於正在被預測的輸出單元 `x_j` 的索引 `j`，只要 `j > k`。換句話說，輸入 `x_k` 對所有後續預測中第 `l` 個隱藏單元的貢獻是相同的（如圖 20.10 中相同的線型所示）。

**Q:** NADE 的設計與 RBM 中的均勻場推斷有何聯繫？

**A:** Larochelle and Murray (2011) 選擇這種共享方案，是因為 NADE 模型中的正向傳播計算與在 RBM 中執行均勻場推斷（用於填充缺失輸入）的計算大致相似。特別是，均勻場推斷的第一步與 NADE 中的計算相同。

**Q:** NADE 如何擴展以處理實值數據？

**A:** NADE 可以擴展到處理實值數據，例如使用高斯混合模型 (Mixture of Gaussians) 作為其輸出層的參數化。每個混合組分的均值、方差和混合係數都由神經網路的輸出決定。這種模型的一個例子是 RNADE (Real-valued NADE)。

**Q:** 為什麼允許隨機採樣觀察變量的順序並集成多個排序模型是有益的？

**A:**
1.  **高效推斷:** 允許使用訓練好的自回歸網路對任何變量子集進行高效的條件預測或採樣，而不需要為每個可能的條件化場景重新訓練模型。
2.  **更好的泛化:** 集成（ensemble）由不同變量排序定義的多個模型，通常能比單個排序定義的單個模型更好地泛化，並為測試集分配更高的機率。這是因為不同的排序捕獲了數據中不同的依賴結構。

---

## 20.11 從自編碼器採樣

**重點摘要:**
許多自編碼器（如得分匹配、去噪自編碼器、收縮自編碼器）隱式地學習數據分佈。從這些模型採樣通常需要 MCMC。
*   **與任意去噪自編碼器相關的馬爾可夫鏈:** Bengio et al. (2013d) 展示了如何構建用於廣義去噪自編碼器的馬爾可夫鏈。鏈的每個步驟包括：1) 從先前狀態 `x` 開始，注入損壞噪聲得到 `~x`；2) 將 `~x` 編碼為 `h=f(~x)`；3) 解碼 `h` 以獲得重構分佈 `p(x|ω=g(h))` 的參數；4) 從該重構分佈中採樣下一個狀態 `x'`。
*   **夾合與條件採樣:** 類似於玻爾茲曼機，去噪自編碼器及其推廣（如 GSN）可用於從條件分佈 `p(x_f | x_c)` 中採樣，只需夾合觀察單元 `x_c`，並在給定 `x_c` 和採好的潛變量下僅重採樣自由單元 `x_f`。
*   **回退訓練過程:** 一種加速去噪自編碼器生成訓練收斂的方法，它不是執行單步編碼-解碼重構，而是包含多個隨機編碼-解碼步驟，並懲罰最後的（或沿途所有）機率重構。


**Q:** 對於那些不直接表示機率分佈的自編碼器（如去噪自編碼器），如何從它們學習到的分佈中進行採樣？

**A:** 通常需要使用馬爾可夫鏈蒙特卡洛 (MCMC) 方法。一種常見的策略是將重複的編碼和（帶噪聲的）解碼過程視為馬爾可夫鏈的轉移操作。從一個初始狀態開始，反覆進行「加噪-編碼-解碼-（可能再加噪）採樣」的步驟，鏈最終會收斂到模型隱含的平穩分佈。

**Q:** 描述一下與廣義去噪自編碼器相關的馬爾可夫鏈的一個採樣步驟。

**A:** 一個採樣步驟通常包括：
1.  從先前狀態 `x` 開始，通過一個損壞過程 `C(~x|x)` 注入噪聲，得到損壞的輸入 `~x`。
2.  將損壞的輸入 `~x` 通過編碼器函數 `f` 得到隱藏表示 `h = f(~x)`。
3.  將隱藏表示 `h` 通過解碼器函數 `g` 得到重構分佈 `p(x|ω=g(h))` 的參數 `ω`。
4.  從這個重構分佈 `p(x|ω)` 中採樣得到馬爾可夫鏈的下一個狀態 `x'`。

**Q:** 什麼是「回退訓練過程 (walkback training)」？它對去噪自編碼器的生成訓練有何好處？

**A:** 回退訓練過程是一種訓練去噪自編碼器的方法，它不只是像標準去噪自編碼器那樣訓練模型從單步損壞中重構，而是模擬一個更長的 MCMC 採樣鏈（即多個編碼-解碼步驟）。訓練目標是懲罰這個鏈最終（或沿途所有）的重構與乾淨數據之間的差異。這樣做可以更有效地去除數據中的偽模式，並幫助模型學習到更接近真實數據分佈的平穩分佈，從而加速生成訓練的收斂。

---

## 20.12 生成隨機網路

**重點摘要:**
生成隨機網路 (Generative Stochastic Network, GSN) 是去噪自編碼器的推廣，它在生成馬爾可夫鏈中除了可見變量 `x` 外，還明確包含潛變量 `h`。GSN 由兩個條件機率分佈參數化，指定馬爾可夫鏈的一步：
1.  `p(x^(k)|h^(k))`: 重建分佈，指示在給定當前潛在狀態下如何產生下一個可見變量。
2.  `p(h^(k)|h^(k-1), x^(k-1))`: 潛在狀態轉移分佈，指示在給定先前潛在狀態和可見變量下如何更新潛在狀態變量。
GSN 通過參數化生成過程而不是聯合分佈的數學形式來定義模型。其平穩分佈（如果存在）是隱式定義的。


**Q:** 生成隨機網路 (GSN) 與標準去噪自編碼器在馬爾可夫鏈的狀態上有何主要區別？

**A:** 標準去噪自編碼器的馬爾可夫鏈狀態通常只包含可見變量 `x`。而 GSN 的馬爾可夫鏈狀態明確包含可見變量 `x` 和潛變量 `h`。

**Q:** GSN 的馬爾可夫鏈轉移由哪兩個主要的條件機率分佈定義？

**A:** GSN 的一步轉移由以下兩個條件機率分佈參數化：
1.  `p(x^(k)|h^(k))`: 給定當前的潛在狀態 `h^(k)`，生成下一個可見狀態 `x^(k)` 的分佈（重建分佈）。
2.  `p(h^(k)|h^(k-1), x^(k-1))`: 給定前一個潛在狀態 `h^(k-1)` 和前一個可見狀態 `x^(k-1)`，生成當前潛在狀態 `h^(k)` 的分佈（潛在狀態轉移分佈）。

**Q:** GSN 是如何定義其所代表的機率分佈的？

**A:** GSN 不是通過一個明確的數學公式來定義可見變量（或可見與潛在變量）的聯合機率分佈 `p(x)` 或 `p(x,h)`。相反，它通過參數化一個生成過程（馬爾可夫鏈的轉移算子）來隱式地定義一個機率分佈。這個機率分佈是該馬爾可夫鏈的平穩分佈（如果存在且唯一）。

---

### 20.12.1 判別性 GSN

**重點摘要:**
GSN 的原始公式用於無監督學習和隱式建模 `p(x)`，但可以修改框架以優化 `p(y|x)`，用於監督學習任務。例如，Zhou and Troyanskaya (2014) 推廣 GSN，只反向傳播輸出變量上的重建對數機率，並保持輸入變量固定，成功應用於建模序列（蛋白質二級結構）。混合模型也可以通過組合監督和無監督目標來構建。


**Q:** GSN 如何被修改用於判別性任務（即建模 `p(y|x)`)？

**A:** 可以修改 GSN 的訓練準則，使其專注於預測目標輸出 `y`，同時將輸入 `x` 視為條件。例如，在訓練過程中，可以只反向傳播與輸出變量 `y` 的重建相關的損失（例如 `log p(y^(k)}=y | h^(k))`，其中 `y` 是真實標籤），同時保持輸入 `x` 固定或僅用於條件化馬爾可夫鏈的轉移。

**Q:** 在判別性 GSN 中，輸入序列和輸出序列在馬爾可夫鏈的每一步中是如何處理的？

**A:** 馬爾可夫鏈不僅僅作用於輸出變量，輸入序列也用於條件化該鏈。反向傳播使得模型能夠學習輸入序列如何條件化由馬爾可夫鏈隱含表示的輸出分佈。在每一步，潛在狀態的更新可能依賴於前一步的潛在狀態、前一步的輸出變量以及當前的（或歷史的）輸入變量。然後，新的輸出變量根據更新後的潛在狀態生成。

---

## 20.13 其他生成方案

**重點摘要:**
除了基於 MCMC 採樣、原始採樣或兩者混合的方法外，還存在其他生成樣本的方案。
*   **擴散反演 (Diffusion Inversion):** Sohl-Dickstein et al. (2015) 開發了一種基於非平衡熱力學學習生成模型的訓練方案。它假設希望採樣的機率分佈具有結構，該結構被一個逐漸增加熵的擴散過程破壞。為了形成生成模型，可以反向運行該過程，通過訓練模型逐漸將結構恢復到非結構化分佈。
*   **近似貝葉斯計算 (ABC):** Rubin et al. (1984) 提出的框架。樣本被拒絕或修改，以使樣本的選定函數的矩匹配期望分佈的那些矩。它修改樣本本身，而不是訓練模型自動發出具有正確矩的樣本。


**Q:** 擴散反演 (Diffusion Inversion) 生成模型的基本思想是什麼？它與去噪自編碼器有何相似和不同之處？

**A:**
*   **基本思想:** 擴散反演模型基於這樣一個想法：從一個結構化的數據分佈開始，通過一個已知的「前向」擴散過程逐漸向其中添加噪聲，直到數據變成完全無結構的噪聲（如高斯噪聲）。然後，訓練一個「反向」過程（生成模型），該過程學習如何從無結構的噪聲開始，逐步去除噪聲，將結構恢復回來，最終生成類似原始數據的樣本。
*   **與去噪自編碼器的相似之處:** 都涉及到從帶噪聲的數據中恢復結構。
*   **與去噪自編碼器的不同之處:** 標準的去噪自編碼器通常旨在一步從任意程度的噪聲中恢復乾淨數據。而擴散反演模型將這個去噪過程分解為許多小的、迭代的步驟，每個步驟只去除少量噪聲。這解決了去噪自編碼器在小噪聲情況下學習信號有限、大噪聲情況下去噪任務過於困難的兩難問題。

**Q:** 近似貝葉斯計算 (ABC) 生成樣本的方法與基於模型訓練的方法（如矩匹配網路）有何不同？

**A:**
*   **基於模型訓練 (如 GMMN):** 訓練一個參數化的生成模型（如神經網路），使其生成的樣本的統計矩（或其他特性）與真實數據的統計矩相匹配。模型本身被優化以產生具有期望特性的樣本。
*   **近似貝葉斯計算 (ABC):** 不是直接訓練一個模型。而是從一個（可能是簡單的）提議分佈中生成大量候選樣本。然後，根據這些候選樣本的某些匯總統計量與觀測數據的匯總統計量的接近程度，對這些樣本進行接受/拒絕或加權。它通過修改或選擇樣本本身來匹配期望的特性，而不是優化一個生成器。

---

## 20.14 評估生成模型

**重點摘要:**
評估生成模型是一個困難且微妙的任務。
*   **對數概似:** 在測試數據上的對數概似是一個理想的指標，但對於許多模型（如 EBM）來說是難解的，需要近似（如 AIS），而這些近似本身可能有偏差或高方差。
*   **數據預處理:** 預處理的微小變化（如圖像二值化與否）會從根本上改變任務，使得不同預處理下的模型比較不公平。對於二值模型，對數概似最多為零；對於實值模型，它可以是任意高的（因為是密度）。
*   **樣本視覺檢查:** 常用但主觀，可能無法檢測到模式崩潰或過擬合（僅複製訓練樣本）。顯示生成樣本的最近鄰訓練樣本有助於檢測過擬合。
*   **Theis et al. (2015) 的觀點:** 強調生成模型有多種用途，指標的選擇必須與模型的預期用途相匹配。例如，一些模型更擅長為真實點分配高機率，而另一些模型則擅長不為不真實的點分配高機率（這可能與優化 `D_KL(P_data||P_model)` 還是 `D_KL(P_model||P_data)` 有關）。目前所有指標都存在嚴重缺陷。


**Q:** 為什麼在評估生成模型時，比較不同模型在 MNIST 數據集上的對數概似需要特別小心？

**A:** 因為 MNIST 數據的處理方式會極大地影響對數概似的值和含義：
1.  **實值 vs. 二值:** 如果將 MNIST 像素視為實值（例如 [0,1] 區間內的灰度值），模型學習的是機率密度，對數概似可以是任意高的。如果將像素二值化（例如以 0.5 為閾值變為 0 或 1），模型學習的是機率質量，對數概似最多為零。這兩種情況下的對數概似值無法直接比較。
2.  **二值化方法:** 即使都使用二值化，不同的二值化方法（例如固定閾值 vs. 將灰度值視為伯努利機率進行採樣）也會導致不同的數據分佈，從而影響對數概似。
    因此，必須確保比較的模型是在完全相同的數據表示和預處理方案下訓練和評估的。

**Q:** 僅通過視覺檢查生成樣本的質量來評估生成模型有哪些局限性？

**A:**
1.  **主觀性:** 視覺質量的判斷是主觀的，不同觀察者可能有不同看法。
2.  **模式崩潰 (Mode Collapse) 的隱蔽性:** 模型可能只生成數據分佈中少數幾個看起來不錯的模式，而忽略了大部分模式。如果生成的樣本多樣性不足，僅看少量樣本可能無法發現這個問題。
3.  **過擬合的隱蔽性:** 模型可能只是記住並輕微修改了訓練樣本。如果只看生成的樣本本身，很難判斷它們是否僅僅是訓練數據的複製。
4.  **無法檢測低機率但重要的錯誤:** 模型可能在大部分情況下生成好樣本，但在某些罕見但關鍵的輸入或情況下失敗，這通過少量視覺檢查難以發現。

**Q:** Theis et al. (2015) 認為在選擇評估生成模型的指標時，應該考慮什麼核心原則？

**A:** 核心原則是指標的選擇必須與模型的預期用途相匹配。生成模型有多種不同的用途（例如，密度估計、樣本生成、異常檢測、數據壓縮、特徵學習等），不同的指標更適合評估模型在特定用途上的表現。沒有一個單一的指標能夠全面評估生成模型的所有方面。

---

## 20.15 結論

**重點摘要:**
訓練具有隱藏單元的生成模型是一種有力的方法，可以讓模型理解給定訓練數據中的複雜世界。通過學習模型 `p_model(x)` 和表示 `p_model(h|x)`，生成模型可以解答關於 `x` 輸入變量之間關係的許多推斷問題，並且可以在層次的不同層對 `h` 求期望來執行各種任務。本章探討了許多不同的深度生成模型架構，每種都有其優勢和挑戰。


**Q:** 根據本章結論，訓練具有隱藏單元的生成模型的主要價值是什麼？

**A:** 主要價值在於讓模型能夠理解和表示在給定訓練數據中觀察到的複雜世界的結構和規律。通過學習數據的潛在表示 `h` 以及數據本身的機率分佈 `p_model(x)`，生成模型不僅可以生成新的數據樣本，還可以執行各種推斷任務，例如回答關於輸入變量之間關係的問題，以及在層次的不同層對潛在表示 `h` 求期望來執行下游任務。

**Q:** 本章討論的各種深度生成模型架構是否有一種普適的最佳選擇？

**A:** 沒有。本章介紹了多種不同的深度生成模型架構（如玻爾茲曼機、DBN、DBM、VAE、GAN、自回歸模型等），每種模型都有其自身的優勢、劣勢、適用場景和訓練挑戰。選擇哪種模型取決於具體的應用需求、數據特性以及可用的計算資源。

---

希望這些詳細的摘要和Q&A對您有所幫助！