## 第八章 深度模型中的優化 (開篇引言)

2017-03-10

Optimization for Training Deep Models

https://www.youtube.com/watch?v=DeXH5IMHfcs

**重點摘要:**
深度學習算法在許多情況下都涉及到優化。模型中的進行推斷（如PCA）涉及到求解優化問題。我們經常使用解析優化去證明或設計算法。在深度學習涉及到的諸多優化問題中，最難的是神經網路訓練。即使是用幾百台機器投入幾天到幾個月來解決單個神經網路訓練問題，也是很常見的。因為這其中的優化問題很重要，代價也很高，因此研究者們開發了一組專門為此設計的優化技術。本章會介紹神經網路訓練中的這些優化技術。
本章主要關注這一類特定的優化問題：尋找神經網路上的一組參數 `θ`，它能顯著地降低代價函數 `J(θ)`，該代價函數通常包括整個訓練集上的性能指標和額外的正則化項。首先會介紹機器學習中優化與純優化有哪些不同。接下來，介紹導致神經網路優化困難的幾個具體挑戰。然後，介紹幾個實用算法，包括優化算法本身和初始化參數的策略。更高級的算法能夠在訓練中自適應調整學習率，或者使用代價函數二階導數包含的信息。最後，介紹將幾個簡單優化算法結合成高級過程的優化策略。


**Q:** 深度學習中最重要的優化問題是什麼？為什麼它如此重要且代價高昂？

**A:** 深度學習中最重要的優化問題是**神經網路的訓練**。它重要是因為模型的性能直接取決於訓練（優化）的好壞。它代價高昂是因為現代神經網路通常具有非常多的參數（從百萬到數十億），並且訓練數據集也非常龐大，導致訓練過程需要大量的計算資源和時間。

**Q:** 本章將主要關注解決哪一類特定的優化問題？這個問題的目標是什麼？

**A:** 本章主要關注尋找神經網路上的一組參數 `θ`，使得一個代價函數 `J(θ)` 顯著降低。這個代價函數通常包含兩部分：一部分是衡量模型在整個訓練集上的性能的指標（如經驗風險），另一部分是額外的正則化項（用於防止過擬合）。

**Q:** 本章將涵蓋哪些關於神經網路優化的主題？

**A:** 本章將涵蓋以下主題：
1.  機器學習中的優化與純優化的區別。
2.  導致神經網路優化困難的具體挑戰。
3.  實用的優化算法（包括基本算法和更高級的算法）。
4.  初始化參數的策略。
5.  將簡單優化算法組合成高級過程的優化策略。

---

## 8.1 學習和純優化有什麼不同

**重點摘要:**
用於深度模型訓練的優化算法與傳統的優化算法在幾個方面有所不同。機器學習通常是間接作用的。在大多數機器學習問題中，我們關註某些性能度量 `P`，其定義於測試集上並且可能是不可解的。因此，我們只是間接地優化 `P`。我們希望通過降低代價函數 `J(θ)` 來提高 `P`。這一點與純優化不同，純優化最小化目標 `J` 本身。訓練深度模型的優化算法通常也會包括一些針對機器學習目標函數的特定結構的特化。
通常，代價函數可以寫為訓練集上的平均，如 `J(θ) = E_{(x,y)∼p̂_data} L(f(x;θ), y)` (公式 8.1)，其中 `L` 是每個樣本的損失函數，`f(x;θ)` 是輸入 `x` 時預測的輸出，`y` 是目標輸出。我們更希望最小化從數據生成分布 `p_data` 的期望，而不是僅僅是有限訓練集上的對應目標函數 (公式 8.2)。

---

### 8.1.1 經驗風險最小化

**重點摘要:**
機器學習算法的目標是降低預期的泛化誤差 (公式 8.2)。這個數量被稱為**風險 (risk)**。如果我們知道了真實的數據生成分布 `p_data`，那麼風險最小化是一個可以通過優化算法解決的問題。然而，我們通常不知道 `p_data(x,y)`，只知道訓練集中的樣本。
將機器學習問題轉化回一個可以用數值優化算法解決的優化問題的最簡單方法是最小化訓練集上的期望損失。這意味著用訓練集上的經驗分布 `p̂(x,y)` 替代真實分布 `p(x,y)`。我們將最小化**經驗風險 (empirical risk)** (公式 8.3)：
`E_{(x,y)∼p̂_data} [L(f(x;θ), y)] = (1/m) Σ_{i=1}^m L(f(x^(i);θ), y^(i))`
基於最小化這種平均訓練誤差的訓練過程被稱為**經驗風險最小化 (empirical risk minimization)**。在這種情況下，機器學習仍然和傳統的直接優化很相似。我們並不真正關心最優化風險，而是希望在最低經驗風險的同時，也使得真實風險很低。


**Q:** 什麼是「風險 (risk)」在機器學習中的含義？為什麼我們通常不能直接最小化風險？

**A:** 在機器學習中，「風險」通常指的是模型在真實的、未知的數據生成分布 `p_data` 上的期望損失或泛化誤差，即 `J*(θ) = E_{(x,y)∼p_data} L(f(x;θ), y)`。這是我們真正希望最小化的目標。
    我們通常不能直接最小化風險，因為我們**不知道真實的數據生成分布 `p_data`**。我們只有從這個分布中採樣得到的一個有限的訓練數據集。

**Q:** 什麼是「經驗風險 (empirical risk)」？經驗風險最小化 (ERM) 是什麼？

**A:** 「經驗風險」是指模型在**訓練數據集**上的平均損失，即 `(1/m) Σ_{i=1}^m L(f(x^(i);θ), y^(i))`。它是對真實風險的一個經驗估計，用訓練集上的經驗分布 `p̂_data` 替代了真實分布 `p_data`。
    **經驗風險最小化 (Empirical Risk Minimization, ERM)** 是一種學習策略，其目標是找到一組模型參數 `θ`，使得經驗風險（即訓練誤差）最小化。

**Q:** 經驗風險最小化與純優化問題有何相似之處？它在機器學習中的最終目標是什麼？

**A:** 經驗風險最小化與純優化問題的相似之處在於，兩者都是在最小化一個明確定義的目標函數（對於 ERM 來說是經驗風險）。
    然而，在機器學習中，ERM 的最終目標並不僅僅是最小化訓練誤差。我們更希望的是，通過最小化經驗風險，模型也能夠在未見過的數據上表現良好，即具有較低的**真實風險（泛化誤差）**。因此，防止過擬合（即經驗風險很低但真實風險很高）是 ERM 框架下的一個重要考慮。

---

### 8.1.2 代理損失函數和提前終止

**重點摘要:**
有時，我們真正關心的損失函數（比如分類誤差 0-1 損失）並不能被高效地優化。例如，0-1 損失在幾乎所有地方導數都為零，精確地最小化 0-1 損失通常是不可解的。在這種情況下，我們通常會優化一個**代理損失函數 (surrogate loss function)**。代理損失函數作為原目標的代理，具備一些優點，例如它是連續可微的，或者其梯度能提供更好的訓練指引。例如，負對數概似通常用作 0-1 損失的代理。
**提前終止 (early stopping)** 也是一種偏離純優化目標的策略。模型（特別是容量較大的模型）在訓練集上通常會達到非常低的損失（甚至零損失），但此時在驗證集上的泛化誤差可能已經開始上升（過擬合）。提前終止策略是在驗證集上的誤差開始上升時就停止訓練，即使訓練集上的損失仍在下降。這意味著我們返回的不是訓練損失最小的參數，而是泛化性能（在驗證集上衡量的）較好的參數。


**Q:** 什麼是代理損失函數 (surrogate loss function)？為什麼在機器學習中有時需要使用它？

**A:** 代理損失函數是指一個替代我們真正關心的、但難以直接優化的原始損失函數的、更容易處理的損失函數。
    在機器學習中有時需要使用它，因為：
    1.  **原始損失函數難以優化:** 某些損失函數（如分類任務中的 0-1 損失，即錯誤率）是離散的、非凸的，或者在其大部分定義域內梯度為零，這使得基於梯度的優化算法難以直接應用。
    2.  **代理損失函數具有更好的數學性質:** 代理損失函數通常被設計為連續可微的、凸的（或至少局部是），或者能夠提供更有用的梯度信息，從而更容易被標準的優化算法處理。
    例如，在分類問題中，交叉熵損失（或負對數概似）通常被用作 0-1 損失的一個代理。

**Q:** 什麼是提前終止 (early stopping)？它為什麼是一種偏離純粹最小化訓練誤差的策略？

**A:** 提前終止是一種在訓練神經網路（或其他迭代學習模型）時常用的正則化技術和停止準則。它的做法是：在訓練過程中，除了監控模型在訓練集上的性能外，還同時監控其在一個獨立的驗證集上的性能。如果在某個點，模型在訓練集上的性能仍在提升（例如，訓練損失仍在下降），但在驗證集上的性能開始惡化（例如，驗證損失開始上升，表明模型開始過擬合），就提前停止訓練過程，並通常選擇在驗證集上性能最好的那個時刻的模型參數作為最終模型。
    它之所以是一種偏離純粹最小化訓練誤差的策略，是因為它的目標不是找到使訓練誤差達到絕對最小的參數，而是找到一個在訓練誤差和泛化能力（通過驗證集性能來近似）之間取得較好平衡的參數。它犧牲了在訓練集上的完美擬合，以換取在未見過數據上更好的表現。

---

### 8.1.3 批量算法和小批量算法

**重點摘要:**
機器學習算法和一般優化算法不同的一點是，機器學習算法的目標函數通常可以分解為訓練樣本上的求和。優化算法在計算參數的每一次更新時，通常僅使用整個代價函數中的一部分來估計代價函數的期望值。
*   **批量 (Batch) / 確定性 (Deterministic) 梯度算法:** 使用整個訓練集來計算每一步的梯度。計算精確，但當訓練集很大時，計算成本非常高。
*   **隨機 (Stochastic) / 在線 (Online) 梯度算法:** 每次更新只使用單個訓練樣本來計算梯度。計算速度快，但梯度估計噪聲大。
*   **小批量 (Minibatch) 梯度算法:** 介於兩者之間，每次更新使用一小部分（小批量）訓練樣本來計算梯度。這是深度學習中最常用的方法，它在梯度估計的準確性和計算效率之間取得了較好的平衡。小批量的大小通常會影響：
    *   梯度估計的方差（批量越大，方差越小）。
    *   計算效率（太小的批量可能無法充分利用並行計算能力）。
    *   內存消耗。
    *   某些硬件（如 GPU）對特定大小的批量有性能偏好。
小批量隨機梯度下降 (Minibatch SGD) 的一個重要方面是，如果數據點是獨立同分布的，那麼從小批量計算得到的梯度期望等於從整個訓練集計算得到的梯度期望。然而，由於小批量樣本的隨機性，梯度的方差較大，這既可能是有益的（幫助跳出局部最優），也可能是有害的（導致收斂不穩定）。
獨立同分布假設在實踐中可能不成立，例如當數據點是按順序處理的流數據 (stream) 時，每個新的小批量可能代表了與之前不同的數據分布。


**Q:** 在優化機器學習模型的目標函數時，批量梯度下降、隨機梯度下降和小批量隨機梯度下降有何區別？

**A:** 它們的主要區別在於每次參數更新時用來計算梯度的訓練樣本數量：
1.  **批量梯度下降 (Batch Gradient Descent):** 在每次參數更新時，使用**整個訓練數據集**來計算損失函數關於參數的梯度。
2.  **隨機梯度下降 (Stochastic Gradient Descent, SGD):** 在每次參數更新時，只隨機選取**一個訓練樣本**來計算梯度。有時也指每次使用一個非常小的固定數量的樣本（例如，1個）。
3.  **小批量隨機梯度下降 (Minibatch Stochastic Gradient Descent):** 在每次參數更新時，使用訓練數據集的一個**小子集（小批量, minibatch）**來計算梯度。小批量的大小通常是一個超參數（例如，32, 64, 128, 256）。

**Q:** 小批量隨機梯度下降相比於批量梯度下降和隨機梯度下降有哪些優缺點？為什麼它在深度學習中被廣泛使用？

**A:**
*   **相比於批量梯度下降:**
    *   **優點:**
        *   **計算效率高:** 每次更新不需要處理整個數據集，對於大規模數據集來說，訓練速度更快。
        *   **內存效率高:** 不需要將整個數據集一次性載入內存。
        *   **可能跳出局部最優:** 由於梯度的隨機性，有機會跳出較差的局部最優點。
    *   **缺點:**
        *   **梯度估計有噪聲:** 每次更新的梯度只是對真實梯度的一個估計，其方差較大，可能導致損失函數在優化過程中出現震盪。
        *   **收斂可能不穩定:** 需要仔細調整學習率等超參數。
*   **相比於（單樣本）隨機梯度下降:**
    *   **優點:**
        *   **梯度估計方差更小:** 使用小批量而不是單個樣本可以顯著降低梯度估計的方差，使得優化過程更穩定。
        *   **更好地利用並行計算能力:** 現代計算硬件（如 GPU）可以高效地並行處理小批量數據中的多個樣本，從而提高計算效率。單樣本 SGD 難以充分利用這種並行性。
    *   **缺點:**
        *   **計算量比單樣本 SGD 略大:** 但通常由於並行計算的優勢，實際運行時間可能更短。
**在深度學習中被廣泛使用的原因:**
小批量 SGD 在梯度估計的準確性（方差）、計算效率（尤其是在 GPU 上）以及優化過程的穩定性之間提供了一個很好的平衡。它既避免了批量梯度下降在大數據集上的高計算成本，又比單樣本 SGD 具有更穩定的梯度和更好的硬件利用率。

---

## 8.2 神經網路優化中的挑戰

**重點摘要:**
優化神經網路是一個極其困難的任務。傳統的機器學習會小心設計目標函數和約束，以確保優化問題是凸的，從而避免一般非凸優化帶來的複雜性。在神經網路優化中，我們遇到的一般是非凸情況。

---

### 8.2.1 病態

**重點摘要:**
在優化凸函數時，會遇到一些挑戰。這其中最突出的是 Hessian 矩陣 `H` 的**病態 (ill-conditioning)**。病態是指 Hessian 矩陣的條件數（最大特徵值與最小特徵值之比）非常大。病態問題在神經網路訓練過程中普遍存在。
當 Hessian 矩陣病態時，梯度下降的表現會很差。梯度下降的方向是 `-g`，而理想的更新方向（如牛頓法）是 `-H⁻¹g`。如果條件數很大，意味著損失函數在某些方向上曲率很高（陡峭），而在另一些方向上曲率很低（平坦）。梯度下降在曲率高的方向上可能會來回震盪，而在曲率低的方向上進展緩慢。這導致整體收斂速度變慢。圖 8.1 展示了這種情況。


**Q:** 在優化問題中，「病態 (ill-conditioning)」通常指的是什麼？它與 Hessian 矩陣有何關聯？

**A:** 在優化問題中，「病態」通常指的是目標函數的 Hessian 矩陣的**條件數 (condition number)** 非常大。Hessian 矩陣是目標函數關於參數的二階偏導數矩陣，它描述了目標函數在某一點的局部曲率。條件數是 Hessian 矩陣的最大特徵值與最小特徵值之比。
    如果條件數很大，則稱 Hessian 矩陣是病態的。

**Q:** Hessian 矩陣病態對基於梯度的優化算法（如梯度下降）有什麼負面影響？

**A:** Hessian 矩陣病態會導致基於梯度的優化算法（特別是簡單的梯度下降）表現不佳，主要體現在：
1.  **收斂速度緩慢:** 梯度下降的更新方向是負梯度方向 `-g`。如果 Hessian 矩陣病態，意味著損失函數的等值線（對於二次函數是橢球）在某些方向上非常「扁平」（曲率小，對應小的特徵值），而在另一些方向上非常「陡峭」（曲率大，對應大的特徵值）。梯度方向通常不會指向最優點。
    *   在陡峭的方向上，學習率如果稍大，就容易導致震盪，需要很小的學習率才能穩定下降。
    *   在扁平的方向上，梯度本身就很小，即使學習率合適，進展也會非常緩慢。
    這導致整體收斂到最優解的過程非常緩慢。
2.  **對學習率敏感:** 為了在陡峭方向上不發散，學習率需要設得很小，但這會進一步減慢在扁平方向上的學習速度。
3.  **早停於次優解:** 由於收斂緩慢，算法可能在達到真正最優解之前就因為達到最大迭代次數或耐心耗盡而提前停止。

---

### 8.2.2 局部極小值

**重點摘要:**
對於凸優化問題，任何局部極小值都是全局極小值。但對於非凸函數（如神經網路的損失函數），可能存在許多局部極小值。如果優化算法陷入一個代價較高的局部極小值，而無法到達代價更低的全局極小值，就會導致問題。
然而，對於深度神經網路，一個普遍的觀點是，大多數局部極小值的代價與全局極小值的代價非常接近。更嚴重的問題可能是存在大量的**鞍點 (saddle points)**。
**模型可辨識性 (Model Identifiability)** 問題與局部極小值有關。如果一個模型有多組不同的參數值能夠產生完全相同的輸出（即模型不可辨識），那麼損失函數必然會有許多等價的局部極小值。例如，交換神經網路同一層中兩個隱藏單元的所有權重，模型的輸出不變，這導致了參數空間的**權重空間對稱性 (weight space symmetry)**。這些由不可辨識性導致的局部極小值通常代價相同，不是主要問題。
真正的挑戰在於那些代價顯著高於全局最小值的局部極小值。


**Q:** 在非凸優化問題中，局部極小值 (local minima) 帶來的主要挑戰是什麼？

**A:** 主要挑戰是，基於梯度的優化算法可能會收斂到一個局部極小值，而這個局部極小值的代價（損失函數值）可能遠高於全局極小值的代價。一旦陷入這樣的局部極小值，由於其周圍所有方向的梯度都指向該點（或梯度為零），標準的梯度下降算法很難從中逃逸出來去尋找更好的解。

**Q:** 什麼是「模型可辨識性 (model identifiability)」？它與神經網路損失函數中的局部極小值有何關聯？

**A:** 如果一個模型存在多組不同的參數值，它們都能產生完全相同的模型輸出（即實現相同的從輸入到輸出的映射），那麼這個模型就被稱為是**不可辨識的 (non-identifiable)**。
    模型不可辨識性會導致損失函數中出現許多具有相同代價的局部極小值（甚至全局極小值）。例如，在一個多層感知機中，如果交換同一隱藏層中任意兩個隱藏單元的所有輸入和輸出權重，整個網路的輸入輸出函數保持不變。這種**權重空間對稱性 (weight space symmetry)** 意味著對於一個最優解，必然存在許多其他等價的最優解（通過對稱變換得到），它們都是（全局）極小值。這些由不可辨識性導致的等價極小值通常不是優化中的主要障礙，因為它們的代價都是最優的。

**Q:** 對於深度神經網路的優化，局部極小值和鞍點哪個被認為是更主要的問題？

**A:** 近年來的研究趨勢表明，對於非常高維的深度神經網路損失函數，**鞍點 (saddle points)** 可能比代價顯著高於全局最小值的局部極小值更為普遍和更具挑戰性。
    *   **局部極小值:** 在這些點，所有方向的曲率都是正的（Hessian 矩陣正定）。
    *   **鞍點:** 在這些點，梯度為零，但 Hessian 矩陣既有正特徵值也有負特徵值，即在某些方向上是局部極小，在另一些方向上是局部極大。
    在高維空間中，隨機函數的局部極小值更有可能具有與全局最小值相近的代價，而鞍點則可能非常普遍。優化算法可能會在鞍點附近的平坦區域進展緩慢。

---

### 8.2.3 高原、鞍點和其他平坦區域

**重點摘要:**
對於很多高維非凸函數而言，局部極小值（以及極大值）實際上都遠少於另一類梯度為零的點：鞍點。鞍點附近的 Hessian 矩陣同時具有正負特徵值。在鞍點處，某些方向的點比鞍點高，另一些方向的點比鞍點低。
多類隨機函數表現出以下性質：低維空間中，局部極小值很常見；高維空間中，局部極小值很罕見，而鞍點則很常見。對於一個隨機函數 `f: R^n → R`，在臨界點（梯度為零的點）處 Hessian 矩陣為負定的（即局部極大值）或正定的（即局部極小值）的機率隨維度 `n` 的增加呈指數級下降。
這對於神經網路優化意味著，我們主要擔心的不是陷入代價很高的局部極小值，而是優化算法可能會在鞍點附近的平坦區域長時間停滯。
**高原 (Plateaus)** 是指梯度接近於零的、廣闊的平坦區域。它們可以是局部極小值、局部極大值或鞍點的組合。
牛頓法等二階方法可以設計成能夠有效處理鞍點（例如，通過檢測 Hessian 矩陣的負特徵值並沿相應方向移動），但對於高維神經網路，計算和存儲完整的 Hessian 矩陣通常不可行。


**Q:** 什麼是鞍點 (saddle point)？它與局部極小值和局部極大值有何不同？

**A:** 鞍點是目標函數上梯度為零的點，但它既不是局部極小值也不是局部極大值。
*   **局部極小值:** 梯度為零，且在該點附近所有方向上，函數值都比該點高（Hessian 矩陣正定）。
*   **局部極大值:** 梯度為零，且在該點附近所有方向上，函數值都比該點低（Hessian 矩陣負定）。
*   **鞍點:** 梯度為零，但在某些方向上，函數值比該點高（像山谷的底部），而在另一些方向上，函數值比該點低（像山脊的頂部）。其 Hessian 矩陣是不定的，即既有正特徵值也有負特徵值。

**Q:** 為什麼在高維空間中，鞍點被認為比代價高的局部極小值更常見，對優化構成更大的挑戰？

**A:** 理論和經驗研究表明，對於高維的隨機（或類似隨機的）非凸函數（如神經網路的損失函數）：
1.  **局部極小值罕見:** 使得 Hessian 矩陣所有特徵值都為正（形成局部極小值）或都為負（形成局部極大值）的機率隨著維度的增加呈指數級下降。換句話說，在高維空間中，很難「碰巧」讓所有方向的曲率都指向同一個方向。
2.  **鞍點普遍:** 使得 Hessian 矩陣既有正特徵值又有負特徵值（形成鞍點）的機率則相對較高。
    對優化構成更大挑戰的原因是：
    *   **梯度下降在鞍點附近可能停滯:** 儘管鞍點不是局部最優，但其梯度接近於零，標準的梯度下降算法在鞍點附近可能會因為梯度過小而進展非常緩慢，看起來像是收斂了。
    *   **逃逸鞍點需要特定方向:** 要從鞍點逃逸，需要沿著 Hessian 矩陣負特徵值對應的方向移動，而簡單的梯度下降可能無法有效地找到這個方向。

**Q:** 除了鞍點，還有哪些類型的「平坦區域」會對神經網路優化造成困難？

**A:**
*   **高原 (Plateaus):** 指的是損失函數表面上一個廣闊的、梯度幾乎為零的平坦區域。這些高原可以是局部極小值、局部極大值或鞍點的組合，或者僅僅是曲率非常低的區域。在高原上，梯度非常小，導致優化算法進展極其緩慢。
*   **寬闊的山谷:** 如果損失函數形成一個非常寬闊、底部平坦的山谷，即使梯度不完全為零，但如果學習率設置不當或優化算法探索能力不足，也可能導致算法在山谷中緩慢移動或來回震盪。

---

**由於後續的章節涉及到具體的優化算法，我將為每個主要小節提供摘要和 Q&A。**

---

## 8.3 基本算法

**重點摘要:**
本節回顧梯度下降法，這是訓練整個訓練集的梯度的下降方向。可以使用隨機梯度下降來極大地加速，沿著隨機挑選的小批量的數據的梯度下降方向。

---

### 8.3.1 隨機梯度下降

**重點摘要:**
隨機梯度下降 (Stochastic Gradient Descent, SGD) 及其變種很可能是一般機器學習中應用最多的優化算法，特別是在深度學習中。如第 8.1.3 節所討論的，按照數據生成分布抽取 `m` 個小批量（獨立同分布的）樣本，通過計算它們梯度的無偏估計。
算法 8.1 展示了沿著這個梯度的估計下降。SGD 算法中的一個關鍵參數是學習率 `ϵ_k`。SGD 收斂的一個充分條件是：
`Σ_{k=1}^∞ ϵ_k = ∞` 且 `Σ_{k=1}^∞ ϵ_k^2 < ∞` (公式 8.12, 8.13)
實踐中，一般會線性衰減學習率直到第 `τ` 次迭代：`ϵ_k = (1-α)ϵ_0 + αϵ_τ`，其中 `α = k/τ` (公式 8.14)。在 `τ` 步迭代之後，一般使 `ϵ` 保持常數。選擇初始學習率 `ϵ_0` 和最終學習率 `ϵ_τ` (通常設為 `ϵ_0` 的 1%) 非常重要。


**Q:** 什麼是隨機梯度下降 (SGD)？它與批量梯度下降的主要區別是什麼？

**A:** 隨機梯度下降 (SGD) 是一種迭代優化算法，用於尋找函數的最小值（或最大值）。
    與批量梯度下降的主要區別在於計算梯度的方式：
    *   **批量梯度下降:** 在每次參數更新時，使用**整個訓練數據集**來計算損失函數關於參數的梯度。
    *   **隨機梯度下降:** 在每次參數更新時，使用訓練數據集的一個**小子集（小批量, minibatch）**（在極端情況下，只使用單個隨機樣本）來估計梯度。
    由於使用的是梯度的隨機估計，SGD 的每次迭代計算成本遠低於批量梯度下降，尤其是在大規模數據集上。

**Q:** SGD 算法中，學習率 `ϵ_k` 的選擇為何至關重要？理論上，學習率需要滿足什麼條件才能保證收斂？

**A:** 學習率 `ϵ_k` 控制了參數更新的步長。其選擇至關重要，因為：
*   **學習率太小:** 收斂速度會非常緩慢。
*   **學習率太大:** 可能導致損失函數在最優解附近震盪，甚至發散。
    理論上，為了保證 SGD 收斂到一個局部最小值（對於非凸函數）或全局最小值（對於凸函數），學習率序列 `ϵ_k` 需要滿足以下條件（Robbins-Monro 條件）：
    1.  `Σ_{k=1}^∞ ϵ_k = ∞` (學習率的總和必須發散，以確保算法能夠探索到足夠遠的參數空間)
    2.  `Σ_{k=1}^∞ ϵ_k^2 < ∞` (學習率的平方和必須收斂，以確保學習率最終會變得足夠小，使得算法能夠在最優解附近穩定下來，而不是持續震盪)

**Q:** 在實踐中，常用的學習率衰減策略是什麼樣的？

**A:** 在實踐中，常用的學習率衰減策略是在訓練初期使用較大的學習率以快速下降，然後隨著訓練的進行逐漸減小學習率以實現更精細的調整和穩定收斂。
    書中提到的一種常見策略是**線性衰減然後保持常數**：
    `ϵ_k = (1-α)ϵ_0 + αϵ_τ`  對於 `k ≤ τ`
    `ϵ_k = ϵ_τ`                       對於 `k > τ`
    其中 `ϵ_0` 是初始學習率，`ϵ_τ` 是在 `τ` 次迭代後達到的最終學習率（通常遠小於 `ϵ_0`，例如 `ϵ_0` 的 1%），`α = k/τ`。
    其他常用的衰減策略還包括步進衰減（每隔固定 epoch 數將學習率乘以一個因子）、指數衰減、餘弦退火等。

---

### 8.3.2 動量

**重點摘要:**
雖然 SGD 仍然是非常流行且受歡迎的優化方法，但其學習過程有時會很緩慢。**動量 (momentum)** 方法 (Polyak, 1964) 旨在加速學習，特別是處理高曲率、小但一致的梯度，或是帶有噪聲的梯度。動量算法積累了之前梯度指數級衰減的移動平均，並繼續沿該方向移動。
速度向量 `v` 被初始化為 0。然後，`v` 和 `θ` 的更新規則如下：
`v ← αv - ϵ∇_θ( (1/m) Σ_{i=1}^m L(f(x^(i);θ), y^(i)) )` (公式 8.15)
`θ ← θ + v` (公式 8.16)
其中 `α ∈ [0,1)` 是決定之前梯度的貢獻衰減得有多快的超參數（通常設為 0.5, 0.9 或 0.99）。`v` 類比於物理學中的速度，`αv` 模擬了阻力。
動量旨在主要解決兩個問題：Hessian 矩陣的病態問題和隨機梯度的方差問題。它有助於在損失函數表面上梯度方向一致的維度上加速，而在梯度方向頻繁改變的維度上抑制震盪。


**Q:** 什麼是動量 (momentum) 方法？它的核心思想是什麼？

**A:** 動量方法是一種用於加速隨機梯度下降 (SGD) 收斂並改善其優化路徑的技術。
    其核心思想是引入一個「速度 (velocity)」向量 `v`，該向量積累了過去梯度的指數加權移動平均。在每次參數更新時，不是直接使用當前的梯度來更新參數，而是使用這個速度向量。速度向量傾向於保持先前的移動方向，同時受到當前梯度的影響。
    直觀上，可以將優化過程想像成一個滾下山坡的小球。小球不僅受到當前位置的坡度（梯度）的影響，還受到其自身慣性（先前積累的速度）的影響。

**Q:** 動量方法的參數更新規則是什麼？其中超參數 `α`（動量係數）的作用是什麼？

**A:** 動量方法的參數更新規則如下：
1.  計算當前小批量的梯度：`g_t = ∇_θ L(θ_t)`
2.  更新速度向量：`v_{t+1} = αv_t - ϵg_t`  (公式 8.15 的一種形式，其中 `ϵ` 是學習率)
3.  更新參數：`θ_{t+1} = θ_t + v_{t+1}` (公式 8.16)
    超參數 `α`（通常稱為動量係數或衰減因子，取值範圍 `[0,1)`）控制了先前速度對當前速度的影響程度：
    *   如果 `α` 接近 0，則先前速度的影響很小，更新主要由當前梯度決定（接近標準 SGD）。
    *   如果 `α` 接近 1（例如 0.9, 0.99），則先前速度的影響很大，速度向量會積累並保持一個持續的移動方向。

**Q:** 動量方法主要有助於解決 SGD 中的哪些問題？

**A:** 動量方法主要有助於解決 SGD 中的以下問題：
1.  **Hessian 矩陣的病態問題:** 在損失函數的「峽谷」地帶（即某些方向曲率很小，梯度變化緩慢；而垂直於峽谷的方向曲率很大，梯度變化劇烈），標準 SGD 可能會在峽谷壁之間來回震盪，而在沿著峽谷底部的方向上進展緩慢。動量可以幫助抑制這種震盪（因為震盪方向的梯度會相互抵消），並在梯度方向比較一致的峽谷底部方向上加速前進。
2.  **高方差的隨機梯度:** 由於 SGD 使用小批量數據估計梯度，梯度本身帶有噪聲。動量通過對歷史梯度進行平均，有助於平滑梯度的更新方向，減少噪聲的影響。
3.  **逃離局部極小值（在某些情況下）:** 由於慣性的存在，動量更新有時可以幫助優化過程「衝過」一些較淺的局部極小值或鞍點。

---

### 8.3.3 Nesterov 動量

**重點摘要:**
Nesterov 動量 (Nesterov, 1983, 2004; Sutskever et al., 2013) 是標準動量方法的一個變種。其核心區別在於梯度計算的點。在標準動量中，梯度是在當前位置 `θ` 處計算的。在 Nesterov 動量中，梯度是在參數近似於「下一步將要到達的位置」（即 `θ + αv`）處計算的。
更新規則如下：
`v ← αv - ϵ∇_θ[ (1/m) Σ_{i=1}^m L(f(x^(i);θ + αv), y^(i)) ]` (公式 8.21)
`θ ← θ + v` (公式 8.22)
Nesterov 動量可以被看作是試圖在「跳躍」之前「向前看一步」，如果預計跳躍會導致損失增加，就進行修正。在凸批量梯度情況下，Nesterov 動量將誤差率從 `O(1/k)` 改善到 `O(1/k^2)`。對於 SGD，它沒有改善收斂率，但實踐中通常表現更好。


**Q:** Nesterov 動量與標準動量方法在梯度計算方面的主要區別是什麼？

**A:** 主要區別在於**計算梯度的點不同**：
*   **標準動量:** 梯度是在**當前參數位置 `θ_t`** 處計算的，即 `∇_θ L(θ_t)`。然後，這個梯度被用來更新速度向量，進而更新參數。
*   **Nesterov 動量:** 梯度是在一個「預測的」未來參數位置（或稱為「向前看」的位置）**`θ_t + αv_t`** 處計算的，即 `∇_θ L(θ_t + αv_t)`。其中 `αv_t` 是基於先前速度的一個「試探性」的跳躍。然後，使用在這個預測位置計算得到的梯度來修正和更新速度向量，進而更新參數。

**Q:** Nesterov 動量的直觀解釋是什麼？為什麼它可能比標準動量表現更好？

**A:** 直觀解釋是，Nesterov 動量在決定最終的更新方向之前，先「試探性地」沿著先前積累的動量方向（即 `αv_t`）「跳」一步，然後在那個預測的未來位置評估梯度。如果這個預測位置的梯度表明繼續沿著動量方向前進會導致損失增加（例如，如果快要越過一個山谷的底部），那麼計算得到的梯度就會有一個「修正」作用，使得最終的更新步長不會過大，或者方向會有所調整。
    可以將其想像成一個更聰明的小球：標準動量的小球是先加速再看坡度；而 Nesterov 動量的小球是先預估一下如果保持當前速度會滾到哪裡，然後在那個預估點看坡度，再決定如何調整加速度。
    它可能比標準動量表現更好，因為這種「向前看」的機制可以提供更及時的修正，減少在最優解附近的震盪，並可能更快地收斂。在凸優化問題中，Nesterov 動量具有更好的理論收斂率。在非凸的深度學習問題中，儘管理論保證較少，但實踐中 Nesterov 動量也常常表現出比標準動量略好的性能。

---

**後續章節將討論更高級的優化算法和初始化策略。**

---

## 8.4 參數初始化策略

**重點摘要:**
訓練深度模型的算法通常是迭代的，因此需要指定參數的初始值。這是一個非常重要的環節，因為：
*   **對稱性破壞:** 如果所有權重都初始化為相同的值（例如全零），那麼同一層中的所有隱藏單元將會學習到相同的特征，這顯然不是我們想要的。需要隨機初始化來打破這種對稱性。
*   **收斂速度和最終性能:** 好的初始化可以將參數置於一個更容易優化、更快收斂且能達到更好最終性能的區域。差的初始化可能導致優化困難、收斂緩慢或陷入差的局部最優/鞍點。
*   **梯度消失/爆炸:** 不恰當的初始化可能加劇梯度消失或梯度爆炸問題。
常用的策略是將權重從一個具有較小方差的隨機分布（如均勻分布或高斯分布）中抽取，偏置通常初始化為零或一個小的正值（對於 ReLU）。
**Xavier 初始化 / Glorot 初始化** (Glorot and Bengio, 2010) (公式 8.23) 和 **He 初始化** (He et al., 2015) 是兩種考慮到層的輸入和輸出單元數量來調整權重初始方差的流行方法，旨在保持激活值和梯度在網路中傳播時的方差大致穩定。
對於循環網路，初始化循環權重矩陣以使其接近恆等映射（或譜半徑略小於1）有助於學習長期依賴。


**Q:** 為什麼神經網路的參數初始化策略如此重要？

**A:** 參數初始化策略之所以重要，主要有以下原因：
1.  **打破對稱性 (Symmetry Breaking):** 如果同一層中的所有權重都初始化為相同的值（例如，全零或相同的常數），那麼在訓練過程中，這些單元將會計算得到相同的梯度並以相同的方式更新，導致它們學習到完全相同的特征。隨機初始化可以打破這種對稱性，使得不同的單元能夠學習到不同的特征。
2.  **影響優化過程:**
    *   **收斂速度:** 好的初始化可以將參數置於一個損失函數表面相對平緩且接近最優解的區域，從而加速收斂。
    *   **避免陷入差的局部最優/鞍點:** 差的初始化可能導致優化算法從一開始就陷入代價較高的局部最優點或在鞍點附近停滯。
3.  **控制激活值和梯度的方差:**
    *   **梯度消失/爆炸:** 不恰當的初始化（例如，權重過大或過小）可能導致激活值在前向傳播過程中變得過大或過小，進而導致梯度在反向傳播過程中爆炸或消失。好的初始化策略試圖在網路的不同層之間保持激活值和梯度的方差大致穩定。
4.  **最終模型性能:** 初始化會影響優化算法最終能夠找到的解的質量，從而影響模型的泛化性能。

**Q:** 如果神經網路的權重都初始化為零，會發生什麼問題？

**A:** 如果權重都初始化為零：
1.  **對稱性無法打破:** 同一層中的所有隱藏單元將會計算得到完全相同的輸出（因為它們接收相同的輸入並使用相同的零權重）。
2.  **梯度相同:** 在反向傳播時，這些對稱的單元也會計算得到完全相同的梯度。
3.  **更新相同:** 因此，在參數更新後，這些單元的權重仍然會保持相同。
    這意味著同一層中的所有單元將永遠學習到相同的特征，網路的表達能力會大大受限，無法學習複雜的模式。這就是為什麼需要隨機初始化來打破這種對稱性。
    （注意：偏置通常可以初始化為零，因為即使權重隨機初始化，偏置的梯度也會不同。）

**Q:** 什麼是 Xavier/Glorot 初始化和 He 初始化？它們的核心思想是什麼？

**A:** Xavier/Glorot 初始化和 He 初始化是兩種流行的、旨在保持網路中激活值和梯度方差穩定的權重初始化方法。
*   **核心思想:** 為了使信息能夠有效地在深度網路中向前和向後傳播，理想情況下，每一層的輸出激活值的方差應該與其輸入激活值的方差大致相同，並且反向傳播的梯度的方差也應該在各層之間保持相對穩定。如果方差逐層增大，可能導致梯度爆炸；如果逐層減小，則可能導致梯度消失。
    這兩種方法都通過分析線性變換和常見激活函數對方差的影響，來推導出一個合適的權重初始化的方差。
*   **Xavier/Glorot 初始化 (Glorot and Bengio, 2010):**
    *   適用於對稱的激活函數，如 tanh 或線性激活。
    *   通常將權重從一個均值為 0、方差為 `Var(W) = 2 / (fan_in + fan_out)` 的均勻分布或高斯分布中採樣。其中 `fan_in` 是該層的輸入單元數量，`fan_out` 是該層的輸出單元數量。
    *   其目標是在前向傳播和反向傳播時都保持方差大致不變。
*   **He 初始化 (He et al., 2015):**
    *   專門為 ReLU 及其變體（如 Leaky ReLU）設計的。由於 ReLU 會將負值置零，它改變了激活值的分布和方差傳播的特性。
    *   通常將權重從一個均值為 0、方差為 `Var(W) = 2 / fan_in` 的高斯分布中採樣（或者對於均勻分布，範圍是 `[-sqrt(6/fan_in), sqrt(6/fan_in)]`）。
    *   其目標是主要確保在前向傳播時激活值的方差保持穩定。
    這些初始化方法顯著改善了深度神經網路的訓練穩定性和性能。

---

**後續章節將討論更高級的優化算法。**

---

## 8.5 自適應學習率算法

**重點摘要:**
自適應學習率算法為每個參數獨立地調整學習率，而不是對所有參數使用單一的全局學習率。
*   **AdaGrad (Adaptive Gradient)** (Duchi et al., 2011): 獨立地適應所有模型參數的學習率，縮放每個參數反比於其所有梯度歷史平方值總和的平方根。對於稀疏特征，它會給予較大的更新；對於頻繁出現的特征，它會給予較小的更新。缺點是分母中的梯度平方和會持續累積，導致學習率最終變得非常小，可能過早停止學習。
*   **RMSProp (Root Mean Square Propagation)** (Hinton, 2012, 未發表): 修改 AdaGrad 以解決其學習率急劇下降的問題。它不是累積所有歷史梯度平方，而是使用梯度平方的指數加權移動平均。
*   **Adam (Adaptive Moment Estimation)** (Kingma and Ba, 2014): 結合了 RMSProp 和動量的思想。它不僅存儲梯度平方的指數加權移動平均 `v`（類似 RMSProp），還存儲梯度的指數加權移動平均 `m`（類似動量）。然後使用 `m̂ / (sqrt(v̂) + δ)`（其中 `m̂` 和 `v̂` 是對 `m` 和 `v` 的偏差修正）來更新參數。Adam 通常被認為是一個非常魯棒且表現良好的默認優化器。

---

### 8.5.1 AdaGrad


**Q:** AdaGrad 算法是如何自適應地調整每個參數的學習率的？

**A:** AdaGrad 為模型中的每個參數 `θ_i` 維護一個該參數歷史梯度的平方和 `r_i`。在每次更新參數 `θ_i` 時，全局學習率 `ϵ` 會被除以 `sqrt(δ + r_i)`（其中 `δ` 是一個小的平滑項，防止除以零）。
    具體來說，如果 `g_{t,i}` 是在時間 `t` 參數 `θ_i` 的梯度：
    1.  累計梯度平方：`r_{t,i} = r_{t-1,i} + g_{t,i}^2`
    2.  參數更新：`θ_{t+1,i} = θ_{t,i} - (ϵ / (δ + sqrt(r_{t,i}))) * g_{t,i}`
    這意味著：
    *   對於那些歷史梯度一直較大的參數，其有效學習率會減小得更快。
    *   對於那些歷史梯度一直較小的參數（例如，稀疏特征對應的參數），其有效學習率會相對較大。

**Q:** AdaGrad 算法的一個主要缺點是什麼？

**A:** 主要缺點是其學習率會持續單調下降。由於分母中的梯度平方和 `r_i` 是不斷累加的，它只會增加而不會減少。這導致隨著訓練的進行，有效學習率會變得越來越小，最終可能變得過小，使得模型在達到最優解之前就停止了有效的學習（即過早收斂）。

---

### 8.5.2 RMSProp


**Q:** RMSProp 算法是如何改進 AdaGrad 以解決其學習率過早衰減問題的？

**A:** RMSProp (Root Mean Square Propagation) 通過修改 AdaGrad 中累計梯度平方和的方式來解決學習率過早衰減的問題。它不是簡單地將所有歷史梯度的平方加起來，而是使用**梯度平方的指數加權移動平均**。
    具體來說，它維護一個梯度平方的移動平均 `r_t`：
    `r_t = ρ * r_{t-1} + (1 - ρ) * g_t^2`
    其中 `g_t` 是當前的梯度，`ρ` 是一個衰減率（類似於動量中的 `α`，例如設為 0.9）。然後，參數更新時，學習率被除以 `sqrt(δ + r_t)`。
    通過使用指數加權移動平均，RMSProp 使得更近期的梯度平方對 `r_t` 的影響更大，而較早的梯度平方的影響會隨時間衰減。這防止了 `r_t` 無限增長，從而避免了學習率過早變得太小。

---

### 8.5.3 Adam


**Q:** Adam (Adaptive Moment Estimation) 算法結合了哪些先前優化算法的思想？

**A:** Adam 算法結合了以下兩種主要思想：
1.  **動量 (Momentum):** 它像動量方法一樣，維護了梯度的一階矩（均值）的指數加權移動平均 `m_t`。這個 `m_t` 近似於梯度的均值，幫助加速在梯度方向一致的維度上的學習，並抑制震盪。
2.  **RMSProp / AdaGrad 的自適應學習率:** 它像 RMSProp 一樣，維護了梯度平方的二階矩（未中心化的方差）的指數加權移動平均 `v_t`。這個 `v_t` 用於對每個參數的學習率進行縮放，類似於 RMSProp 和 AdaGrad。

**Q:** Adam 算法的參數更新涉及到哪些步驟？為什麼它通常被認為是一個很好的默認優化器？

**A:** Adam 算法的參數更新大致包含以下步驟（簡化版）：
1.  計算當前小批量的梯度 `g_t`。
2.  更新梯度的一階矩估計（動量項）：`m_t = β_1 * m_{t-1} + (1 - β_1) * g_t`
3.  更新梯度的二階矩估計（RMSProp 項）：`v_t = β_2 * v_{t-1} + (1 - β_2) * g_t^2`
4.  對 `m_t` 和 `v_t` 進行偏差修正（因為它們在早期迭代中會偏向於零）：
    `m̂_t = m_t / (1 - β_1^t)`
    `v̂_t = v_t / (1 - β_2^t)`
5.  參數更新：`θ_{t+1} = θ_t - ϵ * m̂_t / (sqrt(v̂_t) + δ)`
    其中 `β_1`, `β_2` 是衰減率（類似動量和 RMSProp 中的），`ϵ` 是全局學習率，`δ` 是小的平滑項。
    Adam 通常被認為是一個很好的默認優化器，因為：
    *   **結合了動量和自適應學習率的優點:** 既能加速收斂，又能為不同參數自動調整學習率。
    *   **相對魯棒:** 對於超參數的選擇（如 `β_1`, `β_2`, `ϵ`）通常不如 SGD 那麼敏感，其默認值在許多情況下都能工作得不錯。
    *   **計算高效:** 額外的計算開銷相對較小。
    *   **適用於大規模數據和參數問題:** 在許多深度學習任務中表現良好。

---

### 8.5.4 選擇正確的優化算法

**重點摘要:**
目前還沒有一個公認的「最佳」優化算法。選擇哪種算法取決於具體問題和用戶對超參數的熟悉程度。Schaul et al. (2014) 對多種優化算法在大量學習任務上的比較表明，沒有單一算法在所有情況下都顯著優於其他算法。自適應學習率算法（如 RMSProp, AdaDelta, Adam）通常表現出相似的魯棒性，並且通常比需要手動調整學習率的 SGD 或帶動量的 SGD 更容易使用（即對初始學習率的選擇不那麼敏感）。然而，精心調整的 SGD（帶動量和 Nesterov 動量）有時也能達到與自適應方法相當甚至更好的性能，但可能需要更多的超參數搜索。


**Q:** 在實踐中，如何選擇合適的優化算法？是否有一個普遍適用的「最佳」算法？

**A:** 目前還沒有一個被公認為在所有情況下都「最佳」的優化算法。選擇合適的優化算法通常取決於：
1.  **具體問題的特性:** 例如，數據集的規模、模型的架構、損失函數的形狀等。
2.  **用戶對算法的熟悉程度和調整超參數的經驗:** 某些算法（如 SGD 帶動量）可能需要更仔細的超參數調整才能達到最佳性能，而自適應學習率算法（如 Adam）通常對超參數的選擇更魯棒一些，其默認值在許多情況下就能工作得不錯。
3.  **計算資源:** 某些算法可能計算成本略高。
4.  **最新的研究進展和社區的最佳實踐:** 深度學習領域發展迅速，新的優化算法和改進不斷出現。
    **建議:**
    *   對於大多數情況，**Adam** 通常是一個很好的**默認起點**，因為它結合了動量和自適應學習率的優點，並且相對容易使用。
    *   **SGD 帶動量（或 Nesterov 動量）** 仍然是一個非常強大的選擇，如果能夠仔細調整學習率和動量參數，它有時可以達到比自適應方法更好或相當的性能，並且可能具有更好的泛化能力（儘管這一點尚有爭議）。
    *   **RMSProp** 和 **AdaDelta** 也是不錯的自適應學習率算法，可以作為備選。
    *   如果非常關心達到理論上可能的最佳性能，並且有足夠的計算資源進行超參數搜索，那麼可能需要嘗試多種算法並仔細調整它們的超參數。
    最終，最好的方法通常是通過在驗證集上進行實驗來比較不同算法和超參數設置的性能。

---

**後續章節將討論二階近似方法和更高級的優化策略。**

---

## 8.6 二階近似方法

**重點摘要:**
二階近似方法利用目標函數的二階導數（Hessian 矩陣）來指導優化。
*   **牛頓法 (Newton's Method):** (公式 8.27) 使用 Hessian 矩陣的逆來對梯度進行變換，從而直接跳向（對於二次函數）或接近（對於一般函數）極小值。`θ* = θ_0 - H⁻¹∇_θJ(θ_0)`。優點是收斂快（如果函數接近二次型且 Hessian 正定），缺點是計算、存儲和求逆 Hessian 矩陣的成本非常高（對於 `k` 個參數是 `O(k^3)` 或 `O(k^2)`），使其不適用於大型神經網路。
*   **共軛梯度法 (Conjugate Gradients):** (公式 8.29) 是一種迭代方法，用於求解形如 `Hx=b` 的線性方程組（這與最小化二次函數等價）。它避免了直接計算或存儲 Hessian 及其逆，而是通過一系列共軛方向進行線搜索。對於二次函數，它能在至多 `n` 步內找到最優解。對於非二次函數，可以迭代使用。
*   **BFGS (Broyden-Fletcher-Goldfarb-Shanno) 算法:** 一種擬牛頓法 (quasi-Newton method)。它迭代地近似 Hessian 矩陣的逆，而無需顯式計算 Hessian 或其逆。它使用梯度信息來逐步構建和更新這個近似。
*   **L-BFGS (Limited-memory BFGS):** BFGS 的一個內存受限版本，它不存儲完整的 Hessian 逆的近似，而是只存儲最近幾次迭代的信息來隱式地表示這個近似。這使得 L-BFGS 適用於參數數量非常大的問題，但其收斂速度可能不如 BFGS。

---

### 8.6.1 牛頓法


**Q:** 什麼是牛頓法？它的參數更新規則是什麼？

**A:** 牛頓法是一種二階優化算法，它利用目標函數的一階導數（梯度）和二階導數（Hessian 矩陣）來尋找函數的極小值（或極大值，或鞍點，如果 Hessian 不定）。
    如果我們在當前點 `θ_0` 對目標函數 `J(θ)` 進行二階泰勒展開近似：
    `J(θ) ≈ J(θ_0) + (θ - θ_0)^T ∇_θJ(θ_0) + (1/2) (θ - θ_0)^T H (θ - θ_0)`
    其中 `H` 是 `J(θ)` 在 `θ_0` 處的 Hessian 矩陣。
    令這個二次近似的梯度為零，可以解出下一步的更新點 `θ*`：
    `θ* = θ_0 - H⁻¹ ∇_θJ(θ_0)` (公式 8.27)
    這就是牛頓法的參數更新規則。它直接跳向二次近似的極小點。

**Q:** 牛頓法相較於梯度下降有哪些潛在的優點和主要的缺點？

**A:**
*   **潛在優點:**
    *   **快速收斂:** 如果目標函數是二次的且 Hessian 矩陣 `H` 是正定的，牛頓法可以在一步內直接跳到最小值。對於一般的、性質良好的凸函數，牛頓法通常具有二次收斂速度，遠快於梯度下降的線性收斂速度。
    *   **處理病態 Hessian:** 牛頓法通過乘以 Hessian 的逆 `H⁻¹` 來對梯度方向進行校正，可以有效地處理 Hessian 矩陣病態（條件數很大）導致的問題，使得更新方向更接近指向最優點。
*   **主要缺點:**
    1.  **計算成本高:**
        *   **計算 Hessian 矩陣:** 對於有 `k` 個參數的模型，Hessian 矩陣是一個 `k x k` 的矩陣，計算它需要 `O(k^2)` 個二階偏導數。
        *   **存儲 Hessian 矩陣:** 需要 `O(k^2)` 的存儲空間。
        *   **求 Hessian 矩陣的逆:** 計算 `H⁻¹` 的成本通常是 `O(k^3)`。
        對於現代深度神經網路（參數 `k` 通常是百萬級甚至更高），這些計算和存儲成本是不可接受的。
    2.  **Hessian 矩陣可能非正定:** 如果 Hessian 矩陣不是正定的（例如，在鞍點附近，或者如果目標函數非凸），那麼 `-H⁻¹g` 的方向可能指向極大值或鞍點，而不是極小值。需要對牛頓法進行修改（例如，通過對 Hessian 矩陣進行修正使其正定，如 Levenberg-Marquardt 算法）來處理這種情況。

---

### 8.6.2 共軛梯度法


**Q:** 什麼是共軛梯度法 (Conjugate Gradient method)？它主要用於解決什麼類型的問題？

**A:** 共軛梯度法是一種迭代算法，主要用於求解大型稀疏對稱正定線性方程組 `Ax = b`。它也可以被用來最小化二次函數 `(1/2)x^T Ax - b^T x`，因為這個二次函數的梯度為零的條件恰好是 `Ax = b`。
    在更一般的非線性優化問題中，共軛梯度法被用作一種迭代的線搜索方法，它避免了直接計算、存儲或求逆 Hessian 矩陣，但仍然試圖利用一些關於 Hessian 的（隱含的）二階信息。

**Q:** 共軛梯度法是如何避免直接計算 Hessian 矩陣及其逆的？它與最速下降法（梯度下降）有何不同？

**A:** 共軛梯度法通過構造一系列相對於 Hessian 矩陣 `H` 共軛的搜索方向 `d_t` 來進行優化。
    `d_t = -∇_θJ(θ_t) + β_t d_{t-1}` (公式 8.29 的一種形式)
    其中 `∇_θJ(θ_t)` 是當前的負梯度方向，`d_{t-1}` 是前一個搜索方向，`β_t` 是一個係數，用於確保 `d_t` 與 `d_{t-1}`（以及所有先前的搜索方向）相對於 `H` 是共軛的。
    *   **與最速下降法（梯度下降）的不同:**
        *   **搜索方向:** 梯度下降總是沿著當前的負梯度方向進行搜索。而共軛梯度法則會選擇一個考慮了先前搜索方向的、相對於 Hessian 共軛的新的搜索方向。
        *   **收斂性（對於二次函數）:** 對於 `n` 維的二次函數，共軛梯度法理論上可以在至多 `n` 次迭代內找到精確的最優解。而梯度下降則可能需要更多的迭代，並且其路徑可能是「之」字形的。
        *   **Hessian 信息利用:** 儘管共軛梯度法不顯式計算 Hessian，但其構造共軛方向的過程隱含地利用了 Hessian 的信息，從而實現了比簡單梯度下降更快的收斂。

**Q:** 共軛梯度法在非二次優化問題中是如何應用的？

**A:** 對於非二次的、一般的非線性優化問題，共軛梯度法仍然可以作為一種有效的迭代方法。在這種情況下，它不再保證在有限步內收斂到最優解，但通常比梯度下降收斂得更快。
    應用方式通常是：
    1.  在當前點計算梯度。
    2.  使用某種公式（如 Fletcher-Reeves (公式 8.30) 或 Polak-Ribière (公式 8.31)）計算 `β_t`，以構造新的共軛搜索方向 `d_t`。
    3.  沿著搜索方向 `d_t` 進行一次線搜索 (line search)，找到一個合適的步長 `α_t`，使得目標函數有足夠的下降。
    4.  更新參數：`θ_{t+1} = θ_t + α_t d_t`。
    5.  重複以上步驟。
    為了處理非二次性，有時需要定期地將搜索方向重置為負梯度方向（例如，每隔 `n` 次迭代，其中 `n` 是參數數量）。

---

### 8.6.3 BFGS


**Q:** 什麼是 BFGS (Broyden-Fletcher-Goldfarb-Shanno) 算法？它屬於哪一類優化方法？

**A:** BFGS 算法是一種**擬牛頓法 (quasi-Newton method)**。擬牛頓法的核心思想是避免直接計算、存儲和求逆 Hessian 矩陣（像牛頓法那樣），而是通過迭代地近似 Hessian 矩陣的逆 `B_t ≈ H_t⁻¹`。

**Q:** BFGS 算法是如何近似 Hessian 矩陣的逆的？它使用了哪些信息？

**A:** BFGS 算法從一個初始的 Hessian 逆的近似 `B_0`（通常是單位矩陣）開始。在每次迭代 `t` 中：
1.  計算搜索方向：`p_t = -B_t g_t` (其中 `g_t` 是當前梯度)。
2.  進行線搜索，找到步長 `α_t`，更新參數：`θ_{t+1} = θ_t + α_t p_t`。
3.  計算參數變化量 `s_t = θ_{t+1} - θ_t` 和梯度變化量 `y_t = g_{t+1} - g_t`。
4.  使用 `s_t` 和 `y_t` 以及當前的近似 `B_t`，通過一個特定的更新公式（BFGS 更新公式）來計算下一個 Hessian 逆的近似 `B_{t+1}`。這個更新公式被設計為滿足某些條件（如割線方程 `B_{t+1}y_t = s_t`）並保持近似矩陣的對稱性和正定性（如果可能）。
    BFGS 算法利用了最近的參數變化和梯度變化信息來逐步改進對 Hessian 逆的近似。

**Q:** 什麼是 L-BFGS (Limited-memory BFGS)？它為什麼適用於大規模優化問題？

**A:** L-BFGS (Limited-memory BFGS) 是 BFGS 算法的一個變種，專門為處理那些參數數量非常大（使得存儲完整的 `n x n` Hessian 逆的近似 `B_t` 不可行）的問題而設計。
    它通過不顯式地存儲和更新完整的 `B_t` 矩陣，而是只存儲最近 `m` 次迭代的參數變化向量 `s_i` 和梯度變化向量 `y_i`（`m` 通常是一個較小的值，例如 5 到 20）。然後，在計算搜索方向 `-B_t g_t` 時，它使用這些存儲的向量通過一個遞歸的兩階段迴路算法來隱式地計算這個乘積，而無需實際形成 `B_t` 矩陣。
    由於它只需要存儲少量的歷史向量（`O(mn)` 的空間），而不是完整的 `O(n^2)` 矩陣，因此 L-BFGS 的內存開銷要小得多，使其適用於大規模優化問題（如訓練大型神經網路）。然而，由於它只使用了有限的歷史信息，其收斂速度可能略遜於存儲完整信息的 BFGS。

---

**由於後續章節涉及到更具體的優化策略和與機器學習的聯繫，我將繼續為每個主要小節提供摘要和 Q&A。**

---

## 8.7 優化策略和元算法

**重點摘要:**
優化本身並非真正的算法，而是一般的模板，可以特定地產生算法。這些方法通常是迭代的，從一個初始猜測開始，然後重複應用更新規則。

---

### 8.7.1 批標準化

**重點摘要:**
批標準化 (Batch Normalization, BN) (Ioffe and Szegedy, 2015) 是一種在深度神經網路訓練中被廣泛使用且非常有效的技術。它不是一個優化算法，而是一種網絡層的設計，有助於改善優化過程和模型的泛化能力。
*   **核心思想:** 在網路的每一層的仿射變換（`Wx+b`）之後、非線性激活函數之前，對該層的輸入（或稱為預激活值）進行標準化處理，使其具有零均值和單位方差。然後，再通過兩個可學習的參數 `γ`（縮放因子）和 `β`（平移因子）對標準化後的值進行仿射變換，以恢復其表示能力。
    `Ĥ = (H - μ) / σ` (標準化，其中 `μ` 和 `σ` 是在當前小批量上計算的均值和標準差) (公式 8.35, 8.36, 8.37 的組合)
    `BN(H) = γĤ + β`
*   **作用:**
    1.  **緩解內部協變量偏移 (Internal Covariate Shift):** 深度網路中，由於前一層參數的更新，後續層的輸入分布會不斷變化，這使得後續層需要不斷適應這種變化，減慢了訓練速度。批標準化通過將每層的輸入標準化，使得其分布相對穩定，從而緩解了這個問題。
    2.  **加速收斂:** 允許使用更大的學習率，並減少了對參數初始化的敏感性。
    3.  **正則化效果:** 由於均值和標準差是在每個小批量上計算的，這為模型的激活值引入了一定的噪聲，具有輕微的正則化效果，有時可以減少對 Dropout 等其他正則化方法的需求。
    4.  **避免梯度飽和:** 通過將激活值保持在一個較好的範圍內（遠離激活函數的飽和區），有助於梯度更好地流動。
在測試時，使用整個訓練集（或其移動平均）的均值和方差來進行標準化，而不是測試批次的統計量。


**Q:** 什麼是批標準化 (Batch Normalization)？它在神經網路的哪一部分進行操作？

**A:** 批標準化 (BN) 是一種在深度神經網路中對每一層的輸入（通常是在仿射變換之後、非線性激活函數之前）進行標準化處理的技術。
    它對一個小批量 (minibatch) 數據在該層的激活值進行操作，使其具有零均值和單位方差。然後，再通過兩個可學習的參數（縮放因子 `γ` 和平移因子 `β`）對標準化後的值進行仿射變換。

**Q:** 批標準化是如何計算的？它涉及到哪些統計量和可學習參數？

**A:** 對於一個小批量數據在某一層的激活值 `H`（假設其維度是 `批量大小 x 特征數量`）：
1.  **計算小批量均值 `μ` 和方差 `σ^2`:** 對於每個特征維度，獨立地計算該小批量數據在這個特征上的均值和方差。
2.  **標準化:** 對每個激活值進行標準化：`Ĥ = (H - μ) / sqrt(σ^2 + ε)` (其中 `ε` 是一個小的常數，防止除以零)。
3.  **縮放和平移:** `BN(H) = γĤ + β`。
    其中：
    *   `μ` 和 `σ^2` 是在當前小批量上計算得到的統計量。
    *   `γ`（縮放因子）和 `β`（平移因子）是該批標準化層的可學習參數，它們允許網路學習恢復原始激活值的表示能力，甚至學習一個不同於零均值單位方差的分布。

**Q:** 批標準化為什麼能夠改善深度神經網路的訓練？它主要解決了什麼問題？

**A:** 批標準化能夠改善深度神經網路訓練的原因及其解決的主要問題包括：
1.  **緩解內部協變量偏移 (Internal Covariate Shift):** 這是批標準化最初提出的主要動機。在深度網路中，由於前一層參數的更新，後續層的輸入激活值的分布會不斷發生變化。這種變化使得後續層需要不斷地去適應新的輸入分布，從而減慢了整體訓練速度。批標準化通過將每層的輸入標準化為近似零均值和單位方差，使得輸入分布更加穩定，從而緩解了這個問題。
2.  **加速訓練收斂:** 由於輸入分布更穩定，可以使用更大的學習率，並且模型對參數初始化的敏感性降低，從而加速了訓練過程。
3.  **具有正則化效果:** 由於均值和標準差是在每個小批量上計算的，這為模型的激活值引入了一定的噪聲（因為不同小批量的統計量會有所不同）。這種噪聲可以起到輕微的正則化作用，有時可以減少對其他正則化方法（如 Dropout）的需求或允許使用較小的正則化強度。
4.  **避免梯度飽和:** 通過將激活值（在非線性激活函數之前）保持在一個較好的範圍內（遠離激活函數的飽和區域，例如 sigmoid 或 tanh 的兩端），有助於梯度更好地流動，緩解梯度消失問題。

---

### 8.7.2 坐標下降

**重點摘要:**
坐標下降 (Coordinate Descent) 是一種優化策略，它不是同時更新所有參數，而是一次只優化一個參數（或一小組參數，稱為塊坐標下降, block coordinate descent），同時保持其他參數固定。
*   **工作原理:** 迭代地遍歷所有參數（或參數塊）。對於當前選擇的參數（塊），找到使其目標函數最小的值（通常通過解析解或簡單的單變量優化），然後更新該參數（塊），再移動到下一個。
*   **適用情況:** 當對單個參數（或小子集）的優化非常高效，或者當參數之間存在某種程度的解耦時，坐標下降可能有效。
*   **優點:** 實現簡單，有時對於某些特定結構的問題（如稀疏編碼中的字典學習和編碼推斷交替優化）非常有效。
*   **缺點:** 對於參數之間高度相關的問題，收斂可能非常緩慢，因為一次只優化一個參數可能無法有效地沿著「對角線」方向移動。它不適用於所有類型的目標函數。


**Q:** 什麼是坐標下降 (coordinate descent) 算法？它是如何工作的？

**A:** 坐標下降是一種迭代優化算法，它通過輪流地對目標函數的單個坐標（即單個參數）或一小組坐標（塊坐標下降）進行優化，同時保持所有其他坐標固定不變。
*   **工作原理:**
    1.  初始化所有參數。
    2.  重複以下步驟直到收斂：
        a.  對於模型中的每個參數 `θ_i`（或者每個參數塊）：
            i.  固定所有其他參數 `θ_j` (j ≠ i) 的值。
            ii. 找到使目標函數 `J(θ_1, ..., θ_i, ..., θ_n)` 關於當前參數 `θ_i` 最小的那個值（通常通過對 `J` 關於 `θ_i` 求偏導並令其為零來求解，如果可以解析求解的話，否則可能需要進行一次簡單的單變量優化）。
            iii. 將 `θ_i` 更新為這個找到的最小值。

**Q:** 坐標下降算法適用於什麼類型的優化問題？它有哪些優點和局限性？

**A:**
*   **適用情況:**
    *   當對單個參數（或一小組參數）進行優化（即求解子問題）非常容易或有閉式解時。
    *   當參數之間存在一定程度的解耦或弱相關性時。
    *   對於某些具有特定結構的問題，例如稀疏編碼中的字典學習和編碼推斷交替優化，其中一個子問題可以用坐標下降高效求解。
    *   當目標函數關於每個坐標是凸的時候，收斂性通常有保證。
*   **優點:**
    *   實現相對簡單。
    *   對於某些特定問題，可能非常高效。
    *   不需要計算完整的梯度向量或 Hessian 矩陣，只需要對單個（或少量）參數的導數信息。
*   **局限性:**
    *   **收斂速度:** 如果參數之間存在強相關性，或者目標函數的等值線是「細長」的且其主軸不沿著坐標軸方向，那麼坐標下降的收斂速度可能會非常緩慢，因為它只能沿著坐標軸方向進行更新，無法有效地沿著「對角線」方向移動。
    *   **不適用於非光滑函數:** 如果目標函數關於某些坐標不是光滑的（例如，包含 L1 正則化項），標準的坐標下降可能無法直接應用，需要使用其變體（如近端梯度法中的坐標下降）。
    *   **陷入非駐點:** 對於某些非凸、非光滑函數，坐標下降甚至可能收斂到一個不是局部極小值（甚至不是駐點）的點。

---

### 8.7.3 Polyak 平均

**重點摘要:**
Polyak 平均 (Polyak and Juditsky, 1992) 是一種簡單的技術，用於改進隨機梯度下降 (SGD) 的收斂性（特別是在凸優化中）。它不是直接使用 SGD 在每次迭代結束時得到的參數 `θ^(t)` 作為最終解，而是計算 SGD 軌跡上參數的平均值：
`θ̂^(t) = (1/t) Σ_{i=1}^t θ^(i)` (公式是一個簡化形式)
或者更常用的指數移動平均：
`θ̂^(t) = αθ̂^(t-1) + (1-α)θ^(t)` (公式 8.39)
在凸優化中，Polyak 平均後的解通常比 SGD 的最後一個解具有更好的收斂率（例如，對於強凸函數，從 `O(1/k)` 改善到 `O(1/k)` 的常數因子更優，或者在某些情況下收斂率本身有改善）。在深度學習的非凸優化中，Polyak 平均（或其變體，如指數移動平均 EMA）也經常被用來獲得更穩定和平滑的參數估計，並可能在測試集上獲得略好的性能。


**Q:** 什麼是 Polyak 平均？它在隨機梯度下降 (SGD) 中是如何應用的？

**A:** Polyak 平均（也稱為 Polyak-Ruppert 平均）是一種通過對 SGD 算法在訓練過程中產生的參數迭代序列進行平均，來獲得一個可能更好的最終參數估計的技術。
    在 SGD 中，它通常這樣應用：
    1.  運行標準的 SGD 算法，得到一系列參數迭代值 `θ^(1), θ^(2), ..., θ^(T)`。
    2.  計算這些參數的（加權或非加權）平均值作為最終的模型參數。
        *   簡單平均：`θ̄ = (1/T) Σ_{t=1}^T θ^(t)`
        *   指數移動平均 (EMA)：`θ̄_t = α θ̄_{t-1} + (1-α) θ^(t)` (其中 `α` 是一個衰減因子，例如 0.99 或 0.999)

**Q:** Polyak 平均為什麼能夠改善 SGD 的性能，尤其是在凸優化問題中？

**A:**
1.  **減少噪聲的影響:** SGD 的參數更新由於使用小批量梯度而帶有噪聲，導致參數軌跡可能圍繞最優解震盪。對這個震盪的軌跡進行平均，可以有效地平滑掉這些噪聲，使得平均後的參數更接近最優解。
2.  **更好的收斂率（對於凸優化）:** 對於某些類型的凸優化問題（例如，強凸函數），理論上可以證明，Polyak 平均後的解的收斂率優於標準 SGD 的最後一個迭代點的收斂率。例如，它可能以更快的速度接近最優解，或者達到更低的期望誤差。
3.  **更穩定的解:** 平均後的參數通常比 SGD 軌跡上的單個點更穩定。
    在深度學習的非凸優化中，儘管理論保證較少，但實踐中 Polyak 平均（或 EMA）也常常被用來獲得更平滑的權重更新和可能略好的泛化性能，因為它有助於模型在損失函數的寬闊平坦區域找到更穩定的點。

---

### 8.7.4 監督預訓練

**重點摘要:**
有時，如果模型太複雜難以優化，或者如果某個任務非常困難，直接訓練整個模型來解決最終任務可能效果不佳。**監督預訓練 (Supervised Pretraining)** 是一種策略，它不是直接優化最終的代價函數，而是先訓練模型解決一個更簡單的相關任務，然後將學到的參數作為解決最終複雜任務的初始值或一部分。
*   **逐層預訓練 (Greedy Layer-Wise Supervised Pretraining):** (Bengio et al., 2007c) 每次只訓練模型的一層（或一小組層），將其輸出作為下一個要訓練的層的輸入。每一層的訓練都是一個監督任務（例如，預測原始目標，或者預測一個中間的、更容易的目標）。訓練完一層後，其參數被固定，然後再訓練下一層。這種方法在深度學習的早期被用於初始化深度網路。
*   **遷移學習中的預訓練:** 在一個大規模的、相關的數據集上預訓練一個模型（例如，在 ImageNet 上訓練一個圖像分類模型），然後將這個預訓練好的模型（或其一部分）的權重作為初始值，用於在一個目標任務（通常數據量較小）上進行微調。這是一種非常成功的監督預訓練形式。
*   **FitNets (Romero et al., 2015):** 一種知識蒸餾的變體，用於訓練非常深且窄的「學生」網路。它不僅讓學生網路模仿「教師」網路的最終輸出，還讓學生網路的中間層去擬合教師網路對應中間層的輸出（稱為「提示」, hints）。


**Q:** 什麼是監督預訓練？它與無監督預訓練的主要區別是什麼？

**A:** 監督預訓練是指在訓練目標模型解決最終的、複雜的監督學習任務之前，先使用標註數據訓練該模型（或其一部分）解決一個或多個相關的、通常更簡單的輔助監督學習任務。通過這些輔助任務學到的參數被用作最終任務訓練的初始值。
    與無監督預訓練的主要區別在於**是否使用標籤信息**：
    *   **無監督預訓練:** 在沒有標籤的數據上進行訓練，目標是學習數據的內在結構或表示（例如，自編碼器學習重構輸入，RBM 學習數據分布）。
    *   **監督預訓練:** 在帶有標籤的數據上進行訓練，目標是學習一個能夠完成某個（可能是輔助的）監督任務的模型。

**Q:** 逐層監督預訓練 (greedy layer-wise supervised pretraining) 是如何工作的？

**A:** 逐層監督預訓練是一種訓練深度神經網路的方法，其步驟如下：
1.  首先，只訓練網路的第一層（或前幾層），使其能夠完成一個與最終任務相關的（可能是簡化的）監督任務，例如，預測最終的目標標籤。
2.  訓練完成後，固定第一層（或前幾層）的參數。
3.  然後，在第一層（或前幾層）的輸出之上添加新的層，並只訓練這些新添加的層來完成同一個（或一個更複雜的）監督任務。
4.  重複這個過程，逐層地添加和訓練新的層，直到整個深度網路構建完成。
5.  最後，通常會對整個網路進行一次端到端的微調 (fine-tuning)，即解凍所有層的參數，並在最終任務的數據上用較小的學習率進行整體訓練。
    這種方法的「貪心」之處在於，每一層的訓練都是在先前層參數固定的情況下進行的，只優化當前層的參數。

**Q:** 遷移學習中的預訓練（例如，在 ImageNet 上預訓練模型然後用於其他視覺任務）可以被看作是一種監督預訓練嗎？為什麼？

**A:** 是的，遷移學習中的預訓練（例如，在 ImageNet 上預訓練一個圖像分類模型）可以被看作是一種非常成功的監督預訓練形式。
    原因如下：
    1.  **源任務是監督的:** 在 ImageNet 上的預訓練本身就是一個大規模的圖像分類任務，這是一個監督學習問題（輸入是圖像，輸出是類別標籤）。
    2.  **知識遷移:** 通過在這個大規模、多樣化的數據集上進行監督學習，模型（特別是其較低層的卷積層）學會了提取通用的、具有層次結構的視覺特征（如邊緣、紋理、部件、物體形狀等）。
    3.  **用於初始化下游任務:** 這些預訓練好的權重（或模型的一部分）隨後被用作初始化一個新的、針對特定下游任務（例如，醫學圖像分析、特定對象檢測）的模型。然後，這個新模型在下游任務的（通常較小的）標註數據上進行微調。
    這種策略利用了在大規模數據集上學到的通用知識來幫助在數據量較小或任務更具體的場景中進行學習。

---

### 8.7.5 設計有助於優化的模型

**重點摘要:**
改進優化的最好方法並非總是改進優化算法。相反，設計一個更易於優化的模型通常更有效。
*   **線性變換鏈的梯度問題:** 在非常深的、僅包含線性變換的模型中，梯度很容易消失或爆炸。
*   **跳躍連接 (Skip Connections) / 殘差連接 (Residual Connections):** (Srivastava et al., 2015; He et al., 2016 - ResNet) 引入從較淺層到較深層的直接連接，允許信息和梯度「跳過」中間層。這使得可以訓練非常深的網路，並顯著緩解了梯度消失問題。
*   **批標準化 (Batch Normalization):** 如前所述，也有助於優化。
*   **將優化問題分解:** 例如，在某些模型中，可以將困難的優化問題分解為多個更容易的子問題，或者引入輔助損失函數來指導中間層的學習。


**Q:** 為什麼說「設計一個更易於優化的模型」有時比「改進優化算法」更有效？

**A:** 因為優化算法的性能在很大程度上取決於它所要優化的目標函數（損失函數）的「景觀 (landscape)」。如果損失函數的景觀非常崎嶇、包含許多差的局部最優點、鞍點或廣闊的平坦區域，那麼即使是最先進的優化算法也可能難以找到好的解。
    通過改進模型的架構設計，我們可以：
    1.  **改善損失景觀:** 使得損失函數更平滑、更接近凸函數，或者減少差的局部最優點和鞍點的數量/影響。
    2.  **促進梯度流動:** 使得梯度能夠更容易地在網路中反向傳播，避免梯度消失或爆炸。
    3.  **簡化優化問題:** 例如，通過引入某些約束或結構，使得優化問題的某些部分可以解析求解或更容易處理。
    一個更易於優化的模型架構本身就可以讓相對簡單的優化算法也能取得好結果。

**Q:** 什麼是跳躍連接 (skip connection) 或殘差連接 (residual connection)？它們如何幫助訓練非常深的網路？

**A:**
*   **跳躍連接/殘差連接:** 是一種在深度神經網路中引入的連接方式，它允許信息（激活值或梯度）從網路的較淺層「跳過」一個或多個中間層，直接傳遞到較深的層。在殘差網路 (ResNet) 中，一個典型的殘差塊學習的是對輸入的一個殘差映射 `F(x)`，其輸出是 `H(x) = F(x) + x`，其中 `x` 是通過一個恆等映射（跳躍連接）直接從輸入傳遞過來的。
*   **如何幫助訓練非常深的網路:**
    1.  **緩解梯度消失問題:** 跳躍連接為梯度提供了一條「捷徑」，使得梯度可以更容易地從較深的層反向傳播到較淺的層，而不會在經過許多非線性變換後完全衰減。這使得可以有效地訓練比以前更深得多的網路。
    2.  **更容易學習恆等映射:** 如果某個網絡塊的理想功能是恆等映射（即輸出等於輸入），那麼對於標準的層疊結構，模型需要學習將權重調整到接近實現恆等。而對於殘差塊，模型只需要學習將殘差部分 `F(x)` 驅動為零，這通常更容易。這使得非常深的網路至少能夠達到與其較淺版本相當的性能，而不會因為深度增加而導致性能下降（退化問題）。
    3.  **促進信息流動:** 允許不同抽象級別的特征在網路中更直接地交互。

---

### 8.7.6 延拓法和課程學習

**重點摘要:**
延拓法 (Continuation Method) 是一種通用的優化策略，它從一個更容易解決的問題版本開始，然後逐漸將問題變形為原始的、更困難的問題，同時使用前一個較易問題的解作為下一個更難問題的初始值。
**課程學習 (Curriculum Learning)** (Bengio et al., 2009) 是延拓法在機器學習中的一個實例。它模仿人類學習的方式，先向模型展示更容易的樣本或更簡單的任務，然後逐漸增加難度。
*   **樣本排序:** 按照某種難度度量（例如，噪聲水平、樣本複雜度、標註質量）對訓練樣本進行排序，先用容易的樣本訓練，再逐步引入困難的樣本。
*   **任務簡化:** 先訓練模型解決一個簡化版本的任務，然後再遷移到完整任務。
課程學習的背後思想是，從一個好的起點（通過解決簡單問題獲得）開始優化困難問題，可能比直接從隨機初始點開始更容易找到好的解，並且可能避免陷入差的局部最優。


**Q:** 什麼是延拓法 (continuation method) 的基本思想？

**A:** 延拓法的基本思想是，當我們需要解決一個困難的優化問題時，不是直接嘗試解決這個原始問題，而是從一個與原始問題相關但更容易解決的簡化版本的問題開始。首先找到這個簡化問題的解。然後，逐漸地、平滑地將這個簡化問題「變形」或「延拓」回原始的困難問題，並在每個變形步驟中，都使用前一個（略微簡化的）問題的解作為當前問題的初始猜測。通過這種方式，我們希望能夠逐步引導優化過程到達原始困難問題的一個良好解。

**Q:** 什麼是課程學習 (curriculum learning)？它如何將延拓法的思想應用於機器學習？

**A:** 課程學習 (Bengio et al., 2009) 是延拓法在機器學習（特別是模型訓練）中的一種具體應用。它模仿人類或動物的學習過程，即先從簡單的概念和樣本開始學習，然後逐漸過渡到更複雜的概念和樣本。
    在機器學習中，這意味著：
    1.  **設計一個「課程」:** 根據某種標準（例如，樣本的噪聲水平、複雜度、與任務的相關性等）對訓練樣本或任務的難度進行排序。
    2.  **逐步增加難度:** 在訓練初期，只向模型展示那些「容易」的樣本或任務。隨著訓練的進行，逐漸引入越來越「困難」的樣本或任務，直到最終模型在完整的、具有代表性的訓練數據上進行訓練。
    課程學習的目標是通過提供一個更容易的初始學習環境，幫助優化算法更快地收斂到一個好的解，並可能避免陷入差的局部最優點。

**Q:** 課程學習可能帶來哪些好處？

**A:**
1.  **加速收斂:** 從簡單樣本開始學習，模型可以更快地掌握一些基本的模式和結構，為後續學習更複雜的模式打下基礎，從而可能加速整體訓練過程。
2.  **改善泛化和最終性能:** 通過一個循序漸進的學習過程，模型可能更容易找到一個泛化能力更好、代價更低的解，而不是從一開始就面對所有困難和噪聲。
3.  **避免差的局部最優:** 從一個更容易的損失景觀開始優化，然後逐漸過渡到更複雜的景觀，可能有助於優化算法避開那些在直接優化困難問題時容易陷入的差的局部最優點。
