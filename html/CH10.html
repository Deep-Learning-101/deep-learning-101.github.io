<!DOCTYPE html>
<html lang="zh-Hant">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>循環神經網路與遞歸網路 - Deep Learning 101</title>
    <script src="https://cdn.tailwindcss.com/3.4.3"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/framer-motion/10.18.0/framer-motion.umd.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
            background-color: #f5f5f7; /* Apple-like light grey */
            color: #1d1d1f; /* Apple-like dark grey for text */
        }
        .bento-box {
            background-color: #ffffff; /* White boxes */
            border-radius: 1.5rem; /* Generous rounding */
            padding: 2rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05), 0 10px 20px rgba(0,0,0,0.05);
            overflow: hidden; /* Ensures content respects border radius */
            display: flex;
            flex-direction: column;
        }
        .bento-title {
            font-size: 1.75rem; /* 28px */
            line-height: 2.25rem; /* 36px */
            font-weight: 700;
            margin-bottom: 1rem;
            color: #1d1d1f;
        }
        .bento-title-large {
            font-size: 2.5rem; /* 40px */
            line-height: 3rem; /* 48px */
            font-weight: 700;
            margin-bottom: 1.5rem;
        }
        .bento-subtitle {
            font-size: 1.125rem; /* 18px */
            font-weight: 600;
            color: #0071e3; /* Apple blue for subtitles/accents */
            margin-top: 0.5rem;
            margin-bottom: 0.5rem;
        }
        .bento-text {
            font-size: 1rem; /* 16px */
            line-height: 1.75; /* More spacing for readability */
            color: #333333; /* Slightly lighter than main text */
        }
        .bento-text strong, .bento-text b {
            font-weight: 600;
            color: #1d1d1f;
        }
        .bento-text em {
            font-style: italic;
            color: #0071e3;
        }
        .bento-text a {
            color: #0071e3;
            text-decoration: none;
        }
        .bento-text a:hover {
            text-decoration: underline;
        }
        .bento-list {
            list-style-position: inside;
            padding-left: 0.5rem;
        }
        .bento-list li {
            margin-bottom: 0.75rem;
            padding-left: 1rem;
            position: relative;
        }
        .bento-list li::before {
            content: "\f111"; /* Font Awesome circle icon */
            font-family: "Font Awesome 6 Free";
            font-weight: 900;
            color: #0071e3; /* Apple blue */
            font-size: 0.5rem;
            position: absolute;
            left: -0.25rem;
            top: 0.5em;
        }

        .highlight-tech {
            background: linear-gradient(90deg, rgba(0, 113, 227, 0.15) 0%, rgba(0, 113, 227, 0.05) 100%);
            padding: 0.1rem 0.5rem;
            border-radius: 0.5rem;
            display: inline-block;
        }
        .icon-large {
            font-size: 3rem;
            margin-bottom: 1rem;
            color: #0071e3; /* Apple blue for icons */
        }
        .content-wrapper {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }
        .grid-container {
            display: grid;
            gap: 1.5rem; /* Spacing between boxes */
            grid-template-columns: repeat(1, minmax(0, 1fr));
        }

        @media (min-width: 768px) { /* md */
            .grid-container {
                grid-template-columns: repeat(2, minmax(0, 1fr));
            }
        }

        @media (min-width: 1024px) { /* lg */
            .grid-container {
                grid-template-columns: repeat(3, minmax(0, 1fr));
            }
            .col-span-lg-1 { grid-column: span 1 / span 1; }
            .col-span-lg-2 { grid-column: span 2 / span 2; }
            .col-span-lg-3 { grid-column: span 3 / span 3; }

            .row-span-lg-1 { grid-row: span 1 / span 1; }
            .row-span-lg-2 { grid-row: span 2 / span 2; }
        }

        .bento-box > .motion-div-full-height {
            flex-grow: 1;
            display: flex;
            flex-direction: column;
        }

        .top-info-box {
            background-color: #e9e9ed;
            padding: 1.5rem 2rem;
            border-radius: 1.5rem;
            margin-bottom: 2rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.03);
            margin: 0 auto;
            text-align: center;
        }
        .top-info-title {
            font-size: 2rem; /* 32px */
            font-weight: 700;
            color: #1d1d1f;
            margin-bottom: 0.5rem;
        }
        .top-info-text {
            font-size: 1rem;
            line-height: 1.6;
            color: #333333;
        }
        .top-info-text p {
            margin-bottom: 0.5rem;
        }
        .top-info-text a {
            color: #0071e3;
            font-weight: 500;
            text-decoration: none;
        }
        .top-info-text a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div id="app" class="content-wrapper">

        <div class="top-info-box">
            <h1 class="top-info-title">Deep Learning 101</h1>
            <div class="top-info-text">
            <p align="center">
            <strong>Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101</strong>
            </p>
            <p align="center">
            AI是一條孤獨且充滿不確定的惶恐旅程，花俏絢麗的收費課程或活動絕非通往成功的捷徑。<br>
            衷心感謝當時來自不同單位的AI同好參與者實名分享的寶貴經驗；如欲移除資訊還請告知。<br>
            由 <a href="https://www.twman.org/" target="_blank">TonTon Huang Ph.D.</a> 發起，及其當時任職公司(台灣雪豹科技)無償贊助場地及茶水點心。<br>
            </p>
            <p align="center">
            <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
                <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="180" style="display:inline-block;"></a>
                 <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank" style="display:inline-block;">
                    <img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 100px !important;width: 180px !important; display:inline-block;">
            </p>
            <p align="center">
            <a href="https://www.youtube.com/@DeepLearning101" target="_blank">YouTube</a> |
            <a href="https://www.facebook.com/groups/525579498272187/" target="_blank">Facebook</a> |
            <a href="https://deep-learning-101.github.io/"> 回 GitHub Pages</a> |
            <a href="http://DeepLearning101.TWMAN.ORG" target="_blank">網站</a> |
            <a href="https://huggingface.co/DeepLearning101" target="_blank">Hugging Face Space</a>
            </p>
                <p class="mt-3">
                    <strong>2017/05/05, Recurrent and Recursive Nets @ Deep Learning Book Chapter 10</strong>
                <p>
                    <a href="https://www.youtube.com/watch?v=p6xzPqRd46w" target="_blank" rel="noopener noreferrer"><i class="fab fa-youtube mr-1"></i> Video: Recurrent and Recursive Nets Explained</a><br>
                    <span class="text-xs text-slate-500">(Covers RNNs, LSTMs, GRUs, Seq2Seq models, and their applications, based on the book chapter.)</span>
                </p>
            </div>
        </div>

        <header class="text-center my-12">
            <h1 class="text-5xl md:text-7xl font-bold mb-4 bg-clip-text text-transparent bg-gradient-to-r from-blue-600 to-sky-400">
                循環神經網路與遞歸網路
            </h1>
            <p class="text-xl text-slate-600">
                探討處理序列數據的RNN、LSTM、GRU及其挑戰與應用
            </p>
        </header>

        <div class="grid-container">
            <div class="bento-box col-span-lg-3">
                <i class="fas fa-project-diagram icon-large"></i>
                <h2 class="bento-title-large">什麼是RNN？為什麼需要RNN？ <span class="text-lg font-normal text-slate-500">What & Why RNN?</span></h2>
                <p class="bento-text">
                    循環神經網路 (Recurrent Neural Network, RNN) 是一種能夠處理<strong class="highlight-tech">序列數據</strong>的神經網路架構。與傳統神經網路 (如前饋網路 FFN 和卷積網路 CNN) 不同，RNN 具有<strong class="highlight-tech">循環結構</strong>，允許資訊在時間步之間傳遞。
                </p>
                <p class="bento-text mt-4">
                    RNN 的主要優勢在於能夠處理具有<strong class="highlight-tech">順序關係</strong>的數據。序列數據的含義往往取決於元素的順序，傳統網路難以捕捉這種順序的重要性及<strong class="highlight-tech">跨越較長距離的依賴關係</strong>。
                </p>
                <ul class="bento-list bento-text mt-2">
                    <li>FFN 在不同位置使用不同權重，參數量隨序列長度顯著增加，且難以捕捉遠距離依賴。</li>
                    <li>CNN 使用小型滑動窗口，主要捕捉<strong class="highlight-tech">局部</strong>關係 (如鄰近詞彙或像素)，難以捕捉跨越較大距離的依賴。</li>
                    <li>RNN 透過在時間步之間傳遞資訊，能夠處理<strong class="highlight-tech">可變長度的序列</strong>並<strong class="highlight-tech">在序列的不同位置共享知識</strong>。</li>
                </ul>
            </div>

            <div class="bento-box col-span-lg-2">
                <i class="fas fa-share-alt-square icon-large"></i>
                <h2 class="bento-title">RNN 的參數共享 <span class="text-base font-normal text-slate-500">Parameter Sharing</span></h2>
                <p class="bento-text">
                    參數共享是 RNN 的一個關鍵特性。在處理序列的不同時間步時，RNN 使用<strong class="highlight-tech">相同的權重集</strong>和相同的更新規則。例如，連接 $h^{(t-1)}$ 到 $h^{(t)}$ 的權重矩陣 $W$ 在所有時間步是共享的。
                </p>
                <h3 class="bento-subtitle mt-4">參數共享的好處 <span class="text-sm font-normal text-slate-500">Benefits of Sharing</span></h3>
                <ul class="bento-list bento-text">
                    <li><strong>處理可變長度的序列：</strong> 由於每個時間步的計算模塊相同，RNN 可以靈活處理任意長度的輸入序列。</li>
                    <li><strong>在時間上共享統計強度：</strong> 模型可以在序列的不同位置和不同長度的序列之間泛化學到的模式。</li>
                    <li><strong>減少模型參數：</strong> 避免了參數隨序列長度線性增長的問題，顯著減少了總參數量，使得模型更高效且不容易過擬合。</li>
                </ul>
            </div>

            <div class="bento-box col-span-lg-1">
                <i class="fas fa-cogs icon-large"></i>
                <h2 class="bento-title">RNN的訓練: BPTT <span class="text-base font-normal text-slate-500">Training via BPTT</span></h2>
                <p class="bento-text">
                    通過時間反向傳播 (Backpropagation Through Time, BPTT) 是將標準的反向傳播算法應用於<strong class="highlight-tech">展開後的循環神經網路計算圖</strong>，用於計算損失函數關於 RNN 參數的梯度。
                </p>
                <h3 class="bento-subtitle mt-4">工作原理 <span class="text-sm font-normal text-slate-500">How BPTT Works</span></h3>
                <ol class="bento-list bento-text">
                    <li><strong>前向傳播 (Forward Pass)：</strong> 沿著時間序列對 RNN 進行前向計算，得到每個時間步的隱藏狀態、輸出和總損失。</li>
                    <li><strong>反向傳播 (Backward Pass)：</strong> 從最終時間步的損失開始，沿著展開的計算圖<strong class="highlight-tech">從後向前反向傳播梯度</strong>。</li>
                    <li><strong>梯度累加 (Gradient Accumulation)：</strong> 共享參數的總梯度是其在每個時間步產生的局部梯度的<strong class="highlight-tech">總和</strong> ($\frac{\partial L}{\partial W} = \sum_t \frac{\partial L}{\partial W^{(t)}}$)。</li>
                </ol>
            </div>

            <div class="bento-box col-span-lg-3">
                <i class="fas fa-network-wired icon-large"></i>
                <h2 class="bento-title-large">RNN 的變種與挑戰 <span class="text-lg font-normal text-slate-500">Variants & Challenges</span></h2>
            </div>

            <div class="bento-box col-span-lg-1">
                <i class="fas fa-exchange-alt icon-large"></i>
                <h3 class="bento-subtitle">雙向 RNN (BiRNN) <span class="text-sm font-normal text-slate-500">Bidirectional RNN</span></h3>
                <p class="bento-text">
                    <strong>優點：</strong> 能夠在每個時間步利用輸入序列的<strong class="highlight-tech">完整過去和未來上下文資訊</strong>，對於需要理解全局依賴的任務 (如機器翻譯編碼器、NER) 很有幫助。
                </p>
                <p class="bento-text mt-2">
                    <strong>局限性：</strong> 需要<strong class="highlight-tech">一次性獲得整個輸入序列</strong>，不適用於<strong class="highlight-tech">實時預測</strong>任務。計算成本約為單向 RNN 的兩倍。BiRNN 是一個 RNN 模型，這與 Seq2Seq 模型中的 Encoder 和 Decoder 是兩個**分開的** RNN 不同。
                </p>
            </div>

            <div class="bento-box col-span-lg-1">
                <i class="fas fa-sitemap icon-large"></i>
                <h3 class="bento-subtitle">遞歸神經網路 (Recursive NN) <span class="text-sm font-normal text-slate-500">Recursive Neural Network</span></h3>
                <p class="bento-text">
                    遞歸網路通過在輸入的<strong class="highlight-tech">樹狀結構</strong>上遞歸應用一個相同的神經網路模塊來學習和傳播資訊。資訊通常從<strong class="highlight-tech">葉節點向根節點</strong>傳播 (自底向上)。
                </p>
                <p class="bento-text mt-2">
                    RNN 可以看作是遞歸網路的一個<strong class="highlight-tech">特殊情況</strong>，即處理的樹結構退化為一個<strong class="highlight-tech">線性鏈</strong>。遞歸網路更擅長處理具有<strong class="highlight-tech">明顯層次化結構</strong>的數據。
                </p>
            </div>

            <div class="bento-box col-span-lg-1">
                <i class="fas fa-memory icon-large"></i>
                <h3 class="bento-subtitle">外部記憶網路 <span class="text-sm font-normal text-slate-500">External Memory Network</span></h3>
                <p class="bento-text">
                    如神經圖靈機 (NTM) 或可微神經計算機 (DNC)，包含 <strong class="highlight-tech">控制器</strong> (通常是 RNN)、<strong class="highlight-tech">外部記憶</strong> (二維矩陣) 和 <strong class="highlight-tech">讀寫頭</strong>。
                </p>
                 <p class="bento-text mt-2">讀寫頭通過<strong class="highlight-tech">注意力機制</strong>與外部記憶交互，包括基於內容和基於位置的尋址。</p>
            </div>


            <div class="bento-box col-span-lg-3">
                <i class="fas fa-arrows-alt-h icon-large"></i>
                <h2 class="bento-title">序列到序列 (Seq2Seq) 模型 <span class="text-base font-normal text-slate-500">Sequence-to-Sequence Models</span></h2>
                <p class="bento-text">
                    Seq2Seq 模型常用於處理輸入和輸出都是序列的任務，如<strong class="highlight-tech">機器翻譯</strong>和<strong class="highlight-tech">對話生成</strong>。典型架構包含一個 <strong class="highlight-tech">Encoder</strong> 和一個 <strong class="highlight-tech">Decoder</strong>，通常使用 RNN (如 GRU 或 LSTM)。
                </p>
                <div class="grid md:grid-cols-2 gap-6 mt-4">
                    <div>
                        <h3 class="bento-subtitle">應用與技巧 <span class="text-sm font-normal text-slate-500">Applications & Techniques</span></h3>
                        <ul class="bento-list bento-text">
                            <li>Google 的機器翻譯模型曾使用 GRU，並嘗試將輸入句子<strong class="highlight-tech">反轉</strong>以改善效果。</li>
                            <li>對話生成：Google 用於通用聊天和客服系統，訓練數據源自電影腳本和客服記錄。</li>
                            <li>數據準備：需整理<strong class="highlight-tech">對話資料對</strong>，中文需<strong class="highlight-tech">斷詞</strong>，建立詞彙表，處理低頻詞 (`UNK`)。</li>
                             <li>訓練效率：使用 <strong class="highlight-tech">mini-batch</strong>，長度相似的序列同批，不足用 <strong class="highlight-tech">padding</strong> 補齊。</li>
                            <li>評估指標：<strong class="highlight-tech">困惑度 (Perplexity)</strong> 越低，模型預測能力越好。</li>
                        </ul>
                    </div>
                    <div>
                        <h3 class="bento-subtitle">Dual Encoder 模型 <span class="text-sm font-normal text-slate-500">Dual Encoder Model</span></h3>
                        <p class="bento-text">
                            一種用於提升<strong class="highlight-tech">檢索式</strong>對話系統排名的模型。將輸入 (context) 和潛在的回應 (response) 分別通過兩個 Encoder 編碼為向量，然後計算它們的相似度，希望好的對話對相似度高。
                        </p>
                    </div>
                </div>
            </div>

            <div class="bento-box col-span-lg-1 row-span-lg-2">
                <i class="fas fa-exclamation-triangle icon-large"></i>
                <h2 class="bento-title">梯度消失與爆炸 <span class="text-base font-normal text-slate-500">Vanishing/Exploding Gradients</span></h2>
                <p class="bento-text">
                    這是訓練 RNN 的主要挑戰，特別是在處理<strong class="highlight-tech">長序列</strong>時。在 BPTT 過程中，由於使用<strong class="highlight-tech">鏈式法則</strong>，梯度需要通過多個時間步<strong class="highlight-tech">連續相乘</strong>。
                </p>
                <h3 class="bento-subtitle mt-4">梯度消失 <span class="text-sm font-normal text-slate-500">Vanishing Gradient</span></h3>
                <p class="bento-text">
                    如果乘數 (權重矩陣和激活函數的梯度) 的值<strong class="highlight-tech">持續小於 1</strong>，梯度會指數級<strong class="highlight-tech">減小</strong>，導致遠距離時間步的梯度變得微不足道，模型難以學習長距離依賴。
                </p>
                <h3 class="bento-subtitle mt-4">梯度爆炸 <span class="text-sm font-normal text-slate-500">Exploding Gradient</span></h3>
                <p class="bento-text">
                    如果乘數值<strong class="highlight-tech">持續大於 1</strong>，梯度會指數級<strong class="highlight-tech">增大</strong>，導致訓練不穩定。
                </p>
            </div>

            <div class="bento-box col-span-lg-2 row-span-lg-2">
                <i class="fas fa-cogs icon-large"></i> <h2 class="bento-title">LSTM 與 GRU <span class="text-base font-normal text-slate-500">Long Short-Term Memory & Gated Recurrent Unit</span></h2>
                <p class="bento-text">
                    LSTM 和 GRU 是 RNN 的變體，旨在解決標準 RNN 的梯度消失問題。
                </p>
                <div class="mt-4 space-y-6">
                    <div>
                        <h3 class="bento-subtitle"><i class="fas fa-brain mr-2"></i>長短期記憶網路 (LSTM) <span class="text-sm font-normal text-slate-500">Long Short-Term Memory</span></h3>
                        <p class="bento-text">
                            引入了 <strong class="highlight-tech">記憶單元 (Cell State)</strong> 和三個<strong class="highlight-tech">門 (Gates)</strong>：遺忘門 (Forget Gate)、輸入門 (Input Gate) 和輸出門 (Output Gate)。這些門控制著資訊如何在記憶單元中流動和更新。記憶單元的更新包含<strong class="highlight-tech">加法</strong>操作，這是 LSTM 能夠緩解梯度消失的關鍵。
                        </p>
                    </div>
                    <div>
                        <h3 class="bento-subtitle"><i class="fas fa-microchip mr-2"></i>門控循環單元 (GRU) <span class="text-sm font-normal text-slate-500">Gated Recurrent Unit</span></h3>
                        <p class="bento-text">
                            LSTM 的簡化版本，參數較少。它將遺忘門和輸入門合二為一成為<strong class="highlight-tech">更新門 (Update Gate)</strong>，並引入<strong class="highlight-tech">重置門 (Reset Gate)</strong>。GRU 也沒有單獨的記憶單元，直接在隱藏狀態中進行資訊傳遞和控制。
                        </p>
                    </div>
                </div>
                 <p class="bento-text mt-4">GRU 和 LSTM 在許多任務上表現相似，且通常優於標準 RNN。</p>
            </div>

            <div class="bento-box col-span-lg-1">
                <i class="fas fa-layer-group icon-large"></i>
                <h3 class="bento-subtitle">增加 RNN 深度 <span class="text-sm font-normal text-slate-500">Increasing RNN Depth</span></h3>
                <p class="bento-text">
                    在 RNN 中堆疊多個 RNN 層可以增加模型的<strong class="highlight-tech">表達能力</strong>，學習更複雜的非線性函數和層次化的時間表示。較低層可能捕捉局部、短期的模式，而較高層學習更全局、長期的結構。
                </p>
            </div>

            <div class="bento-box col-span-lg-2">
                <i class="fas fa-rocket icon-large"></i>
                <h2 class="bento-title">RNN 的其他應用 <span class="text-base font-normal text-slate-500">Other Applications of RNNs</span></h2>
                <p class="bento-text">除了序列到序列任務，RNN 還廣泛應用於:</p>
                <ul class="bento-list bento-text grid grid-cols-2 gap-x-4">
                    <li>語音識別</li>
                    <li>文本生成</li>
                    <li>情感分析</li>
                    <li>命名實體識別 (NER)</li>
                    <li>時序數據預測</li>
                    <li>圖像描述生成</li>
                    <li>影片分析</li>
                    <li>異常檢測</li>
                    <li>推薦系統</li>
                    <li>圖像處理 (替代 CNN)</li>
                </ul>
            </div>
            <div class="bento-box col-span-lg-3">
                <i class="fas fa-users icon-large"></i>
                <h2 class="bento-title">提及的人物與組織 <span class="text-base font-normal text-slate-500">Mentioned Figures & Organizations</span></h2>
                <p class="bento-text">
                資料來源中提到了以下人物和組織，他們在 RNN 的發展、教學或應用中扮演了角色：
                </p>
                <ul class="bento-list bento-text grid grid-cols-2 md:grid-cols-3 gap-x-4 text-sm">
                    <li>Line 與未知講者</li>
                    <li>李宏毅</li>
                    <li>Andrew Ng</li>
                    <li>Stanford CS224N</li>
                    <li>Google</li>
                    <li>印度人老師 (LSTM 教學影片)</li>
                    <li>Karpathy (Andrej Karpathy)</li>
                    <li>教育部的國語辭典</li>
                    <li>中研院</li>
                    <li>百度</li>
                    <li>WMT (Workshop on Statistical Machine Translation)</li>
                    <li>PTT</li>
                    <li>jieba 斷詞器</li>
                </ul>
                 <p class="bento-text mt-2 text-sm">這些提及主要圍繞著 RNN 的理論、發展、挑戰及其在自然語言處理領域的應用。</p>
            </div>


        </div>
    </div>

    <script>
    document.addEventListener('DOMContentLoaded', () => {
        const { animate, inView } = motion;

        const headerH1 = document.querySelector('header h1');
        if (headerH1) {
            animate(headerH1, { opacity: [0, 1], y: [-50, 0] }, { duration: 0.8, ease: 'easeOut' });
        }
        const headerP = document.querySelector('header p');
        if (headerP) {
            animate(headerP, { opacity: [0, 1], y: [50, 0] }, { duration: 0.8, delay: 0.2, ease: 'easeOut' });
        }

        const topInfoBox = document.querySelector('.top-info-box');
        if (topInfoBox) {
            topInfoBox.style.opacity = 0;
            topInfoBox.style.transform = 'translateY(-30px)';
            animate(topInfoBox, { opacity: 1, y: 0 }, { duration: 0.7, ease: 'easeOut' });
        }

        const bentoBoxes = document.querySelectorAll('.bento-box');
        bentoBoxes.forEach((box, index) => {
            box.style.opacity = 0;
            box.style.transform = 'translateY(20px) scale(0.95)';
            inView(box, () => {
                animate(box, { opacity: 1, y: 0, scale: 1 }, { duration: 0.5, delay: (index % Math.min(bentoBoxes.length, 3)) * 0.08, ease: 'easeOut' });
            }, { amount: 0.1 });
        });
    });
    </script>
</body>
</html>