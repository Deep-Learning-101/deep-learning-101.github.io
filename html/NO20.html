<!DOCTYPE html>
<html lang="zh-Hant">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VAE 於動漫角色影像生成之研究 - Deep Learning 101</title>
    <script src="https://cdn.tailwindcss.com/3.4.3"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/framer-motion/10.18.0/framer-motion.umd.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
            background-color: #f5f5f7; /* Apple-like light grey */
            color: #1d1d1f; /* Apple-like dark grey for text */
        }
        .bento-box {
            background-color: #ffffff; /* White boxes */
            border-radius: 1.5rem; /* Generous rounding */
            padding: 2rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05), 0 10px 20px rgba(0,0,0,0.05);
            overflow: hidden; /* Ensures content respects border radius */
            display: flex;
            flex-direction: column;
        }
        .bento-title {
            font-size: 1.75rem; /* 28px */
            line-height: 2.25rem; /* 36px */
            font-weight: 700;
            margin-bottom: 1rem;
            color: #1d1d1f;
        }
        .bento-title-large {
            font-size: 2.5rem; /* 40px */
            line-height: 3rem; /* 48px */
            font-weight: 700;
            margin-bottom: 1.5rem;
        }
        .bento-subtitle {
            font-size: 1.125rem; /* 18px */
            font-weight: 600;
            color: #0071e3; /* Apple blue for subtitles/accents */
            margin-top: 0.5rem;
            margin-bottom: 0.5rem;
        }
        .bento-text {
            font-size: 1rem; /* 16px */
            line-height: 1.75; /* More spacing for readability */
            color: #333333; /* Slightly lighter than main text */
        }
        .bento-text strong, .bento-text b {
            font-weight: 600;
            color: #1d1d1f;
        }
        .bento-text em {
            font-style: italic;
            color: #0071e3;
        }
        .bento-text a {
            color: #0071e3;
            text-decoration: none;
        }
        .bento-text a:hover {
            text-decoration: underline;
        }
        .bento-list {
            list-style-position: inside;
            padding-left: 0.5rem;
        }
        .bento-list li {
            margin-bottom: 0.75rem;
            padding-left: 1rem;
            position: relative;
        }
        .bento-list li::before {
            content: "\f111"; /* Font Awesome circle icon */
            font-family: "Font Awesome 6 Free";
            font-weight: 900;
            color: #0071e3; /* Apple blue */
            font-size: 0.5rem;
            position: absolute;
            left: -0.25rem;
            top: 0.5em;
        }

        .highlight-tech {
            background: linear-gradient(90deg, rgba(0, 113, 227, 0.15) 0%, rgba(0, 113, 227, 0.05) 100%);
            padding: 0.1rem 0.5rem;
            border-radius: 0.5rem;
            display: inline-block;
        }
        .icon-large {
            font-size: 3rem;
            margin-bottom: 1rem;
            color: #0071e3; /* Apple blue for icons */
        }
        .content-wrapper {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }
        .grid-container {
            display: grid;
            gap: 1.5rem; /* Spacing between boxes */
            grid-template-columns: repeat(1, minmax(0, 1fr));
        }

        @media (min-width: 768px) { /* md */
            .grid-container {
                grid-template-columns: repeat(2, minmax(0, 1fr));
            }
        }

        @media (min-width: 1024px) { /* lg */
            .grid-container {
                grid-template-columns: repeat(3, minmax(0, 1fr));
            }
            .col-span-lg-1 { grid-column: span 1 / span 1; }
            .col-span-lg-2 { grid-column: span 2 / span 2; }
            .col-span-lg-3 { grid-column: span 3 / span 3; }

            .row-span-lg-1 { grid-row: span 1 / span 1; }
            .row-span-lg-2 { grid-row: span 2 / span 2; }
        }

        .bento-box > .motion-div-full-height {
            flex-grow: 1;
            display: flex;
            flex-direction: column;
        }

        .top-info-box {
            background-color: #e9e9ed;
            padding: 1.5rem 2rem;
            border-radius: 1.5rem;
            margin-bottom: 2rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.03);
            margin-left: auto;
            margin-right: auto;
            text-align: center;
        }
        .top-info-title { /* Applied to the main title inside top-info-box */
            font-size: 2rem; /* 32px */
            font-weight: 700;
            color: #1d1d1f;
            margin-bottom: 0.5rem;
        }
        .top-info-text { /* Applied to the paragraph group in top-info-box */
            font-size: 1rem;
            line-height: 1.6;
            color: #333333;
        }
        .top-info-text p {
            margin-bottom: 0.5rem;
        }
         .top-info-text p:last-child {
            margin-bottom: 0;
        }
        .top-info-text a {
            color: #0071e3;
            font-weight: 500;
            text-decoration: none;
        }
        .top-info-text a:hover {
            text-decoration: underline;
        }
        .centered-image {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 400px;
        }
        .markdown-links a {
             margin-left: 0.25rem;
             margin-right: 0.25rem;
        }
    </style>
</head>
<body>
    <div id="app" class="content-wrapper">

        <div class="top-info-box">
            <p><strong>The top private AI Meetup in Taiwan, launched on 2016/11/11 @ 83F, Taipei 101</strong></p>
            <p><strong>Deep Learning 101, 台灣曾經最高最早發起的深度學習社群 @ 83F, 台北101</strong></p>
            <p>AI是條寂寞且惶恐的道路，花俏的收費課程或活動絕不會是條捷徑<br>
            本頁內容為過往實名分享制的讀書會，感謝來自不同公司參與者的支持；如欲移除資訊還請告知。<br>
            Deep Learning 101 只由 TonTon Huang Ph.D. 及其當時任職公司無償贊助場地及茶水點心，無 Co-organizer</p>
            <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank" rel="noopener noreferrer">
                <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" class="centered-image my-4">
            </a>
            <p class="markdown-links">
              <a href="https://www.youtube.com/@DeepLearning101">YouTube</a> |
              <a href="https://www.facebook.com/groups/525579498272187/">台灣人工智慧社團 FB</a> |
              <a href="https://www.twman.org/">TonTon Huang Ph.D.</a> |
              <a href="http://DeepLearning101.TWMAN.ORG">台灣人工智慧社團 網站</a> |
              <a href="https://huggingface.co/DeepLearning101">Hugging Face</a>
            </p>
             <p class="mt-4">
                <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important; display: inline-block;" ></a>
            </p>
            <hr class="my-6 border-gray-300">
            <h1 class="top-info-title">VAE: A generative model for 2D anime character faces</h1>
            <div class="top-info-text">
                <p><em>2018/06/08 - Nat, Boris, Alice, Ian (蠻牛小隊)</em></p>
                <p>
                    <a href="https://www.youtube.com/watch?v=DF9GMPU8wPU" target="_blank" rel="noopener noreferrer"><i class="fab fa-youtube mr-1"></i> Video: VAE for Anime Character Generation</a>
                     | <a href="https://deep-learning-101.github.io/" rel="noopener noreferrer">回 GitHub Pages</a>
                </p>
            </div>
        </div>

        <header class="text-center my-12">
            <h1 class="text-5xl md:text-7xl font-bold mb-4 bg-clip-text text-transparent bg-gradient-to-r from-blue-600 to-sky-400">
                Variational Autoencoder (VAE) 於動漫角色影像生成
            </h1>
            <p class="text-xl text-slate-600">
                研究筆記與新手入門指南：從 AE 到 VAE 的生成模型探索
            </p>
        </header>

        <div class="grid-container">
            <div class="bento-box col-span-lg-3">
                <i class="fas fa-cogs icon-large"></i>
                <h2 class="bento-title-large">從 Autoencoder (AE) 談起 <span class="text-lg font-normal text-slate-500">Starting with Autoencoders</span></h2>
                <p class="bento-text">
                    傳統 <strong class="highlight-tech">Autoencoder (AE)</strong> 可想成數據壓縮與解壓縮過程。透過 <strong class="highlight-tech">Encoder</strong> 將高維輸入壓縮至低維 <strong class="highlight-tech">Latent Space (潛在空間)</strong> 的 <strong class="highlight-tech">Latent Vector (潛在向量)</strong>，再由 <strong class="highlight-tech">Decoder</strong> 還原。
                </p>
                <p class="bento-text mt-2">
                    AE 主要用於資料降維和特徵提取。然而，傳統 AE 將每個輸入數據點映射到潛在空間中的一個「點」，形成離散的 <strong class="highlight-tech">Clusters (群集)</strong>。這些群集間的「空白區域」若用於採樣解碼，結果往往不佳，限制了其生成新數據的能力。
                </p>
            </div>

            <div class="bento-box col-span-lg-3">
                <i class="fas fa-magic icon-large"></i>
                <h2 class="bento-title-large">Variational Autoencoder (VAE)：邁向生成模型 <span class="text-lg font-normal text-slate-500">VAE: Towards Generative Models</span></h2>
                <p class="bento-text">
                    VAE 不再將輸入編碼為潛在空間的單一「點」，而是編碼為一個「區域」或「分佈」，通常假定為<strong class="highlight-tech">高斯分佈</strong>。Encoder 輸出描述此分佈的<strong class="highlight-tech">均值向量 (μ)</strong> 和<strong class="highlight-tech">標準差向量 (σ)</strong>。Decoder 從此分佈中<strong class="highlight-tech">採樣 (Sampling)</strong> 得到潛在向量再解碼。
                </p>
                <p class="bento-text mt-2">
                    VAE 的生成能力源於此「分佈」概念，可在潛在空間的「空白」區域採樣並解碼出合理圖像。潛在空間形成更集中且連續的區域，同時保留群集特性，但訓練難度較高。
                </p>
            </div>

            <div class="bento-box col-span-lg-2 row-span-lg-2">
                <i class="fas fa-microchip icon-large"></i>
                <h2 class="bento-title">VAE 的模型架構與訓練要點 <span class="text-base font-normal text-slate-500">VAE Architecture & Training</span></h2>
                <p class="bento-text">VAE 模型架構：</p>
                <ol class="bento-list bento-text mt-2">
                    <li><strong>Encoder Network：</strong> 接收輸入，通常使用 <strong class="highlight-tech">Convolution (卷積)</strong> 層提取特徵。</li>
                    <li><strong>輸出分佈參數：</strong> Encoder 輸出均值向量 (μ) 和標準差向量 (σ)。</li>
                    <li><strong>Sampling (採樣)：</strong> 從以 μ 為均值、σ 為標準差的高斯分佈中採樣得到 Latent Vector。</li>
                    <li><strong>Decoder Network：</strong> 接收採樣的 Latent Vector，通常使用 <strong class="highlight-tech">Deconvolution (反卷積)</strong> 層重建圖像。</li>
                    <li><strong>輸出生成圖像。</strong></li>
                </ol>
            </div>

            <div class="bento-box col-span-lg-1">
                <h3 class="bento-subtitle"><i class="fas fa-lightbulb mr-2"></i>重參數化技巧 <span class="text-sm font-normal text-slate-500">Reparameterization Trick</span></h3>
                <p class="bento-text">
                    解決「隨機抽取」不可微分問題。從標準高斯分佈採樣隨機向量 $\epsilon$，再透過 `latent_vector = μ + σ * ε` 得到目標分佈採樣，使誤差可反向傳播。
                </p>
            </div>

            <div class="bento-box col-span-lg-1">
                <h3 class="bento-subtitle"><i class="fas fa-bullseye mr-2"></i>損失函數 <span class="text-sm font-normal text-slate-500">Loss Function</span></h3>
                <p class="bento-text">包含兩部分：</p>
                <ul class="bento-list bento-text text-sm">
                    <li><strong>Reconstruction Loss：</strong> 如 L2 Loss，度量生成圖像與原始輸入相似度。</li>
                    <li><strong>KL Divergence：</strong> 度量潛在分佈與標準高斯先驗分佈的差異，促使潛在空間連續平滑。</li>
                </ul>
            </div>

             <div class="bento-box col-span-lg-3">
                <i class="fas fa-paint-brush icon-large"></i>
                <h2 class="bento-title-large">動漫角色影像生成實驗與發現 <span class="text-lg font-normal text-slate-500">Anime Character Generation Experiments</span></h2>
                <p class="bento-text">
                    使用約 4000 張動漫角色頭部圖片 (128x128px)，潛在空間設為 50 維，訓練 1000 個世代。
                </p>
            </div>

            <div class="bento-box col-span-lg-1">
                <h3 class="bento-subtitle"><i class="fas fa-chart-area mr-2"></i>潛在空間分佈 <span class="text-sm font-normal text-slate-500">Latent Space Distribution</span></h3>
                <p class="bento-text">VAE 形成巨大、內部混雜的連續區域，而非 AE 的離散群集。相似圖像潛在向量距離近 (約1.15-1.75)，不同人物則較遠 (>2.0)，保留了相似性結構。</p>
            </div>

            <div class="bento-box col-span-lg-1">
                <h3 class="bento-subtitle"><i class="fas fa-sliders-h mr-2"></i>圖像特徵操控 <span class="text-sm font-normal text-slate-500">Feature Manipulation</span></h3>
                <p class="bento-text">微擾潛在向量可平滑改變生成圖像特徵，如<strong class="highlight-tech">眼睛顏色、表情、髮型</strong> (有時傾向變捲髮)。</p>
            </div>

            <div class="bento-box col-span-lg-1">
                <h3 class="bento-subtitle"><i class="fas fa-users-cog mr-2"></i>角色融合 <span class="text-sm font-normal text-slate-500">Character Fusion</span></h3>
                <p class="bento-text">線性組合不同角色潛在向量可生成融合特徵的新角色，但有時效果像「拼接」而非自然融合。</p>
            </div>
             <div class="bento-box col-span-lg-3">
                <h3 class="bento-subtitle"><i class="fas fa-photo-video mr-2"></i>連續生成比較 <span class="text-sm font-normal text-slate-500">Continuous Generation Comparison</span></h3>
                <p class="bento-text">
                VAE 在潛在空間中沿路徑移動能產生<strong class="highlight-tech">平滑過渡的圖像序列</strong>，而傳統 AE 在空白區域解碼圖像會模糊或損壞。這與 VAE 潛在空間更集中、標準差較小的分佈特性相關。
                </p>
            </div>


            <div class="bento-box col-span-lg-3">
                <i class="fas fa-tasks icon-large"></i>
                <h2 class="bento-title-large">挑戰與未來展望 <span class="text-lg font-normal text-slate-500">Challenges & Future Outlook</span></h2>
                <div class="grid md:grid-cols-2 lg:grid-cols-3 gap-6 mt-4">
                    <div>
                        <h4 class="font-semibold text-slate-700 mb-1"><i class="fas fa-image mr-1"></i>圖像解析度與清晰度</h4>
                        <p class="bento-text text-sm">目前 128x128px 放大後模糊，需更大數據與更強模型提升品質。</p>
                    </div>
                    <div>
                        <h4 class="font-semibold text-slate-700 mb-1"><i class="fas fa-thumbs-up mr-1"></i>生成質量與主觀性</h4>
                        <p class="bento-text text-sm">評估生成圖像「可愛度」等主觀，需客觀指標評估多樣性與真實感。</p>
                    </div>
                    <div>
                        <h4 class="font-semibold text-slate-700 mb-1"><i class="fas fa-brain mr-1"></i>潛在空間理解與控制</h4>
                        <p class="bento-text text-sm">VAE 學到的潛在特徵難以直接對應，缺乏精確控制。可考慮結合 Conditional GANs 等技術。</p>
                    </div>
                    <div>
                        <h4 class="font-semibold text-slate-700 mb-1"><i class="fas fa-database mr-1"></i>數據集特性影響</h4>
                        <p class="bento-text text-sm">動漫圖像多樣性影響模型效果，乾淨一致數據集 (如姿態歸一化) 更佳。</p>
                    </div>
                    <div>
                        <h4 class="font-semibold text-slate-700 mb-1"><i class="fas fa-cogs mr-1"></i>模型訓練複雜性</h4>
                        <p class="bento-text text-sm">VAE 訓練較 AE 複雜，需仔細調整損失函數權重。</p>
                    </div>
                    <div>
                        <h4 class="font-semibold text-slate-700 mb-1"><i class="fas fa-copyright mr-1"></i>版權問題</h4>
                        <p class="bento-text text-sm">使用動漫角色圖像訓練和展示存在潛在版權問題，需謹慎處理。</p>
                    </div>
                </div>
                <p class="bento-text mt-4">
                    <strong>未來展望：</strong> 提高解析度、增強特徵控制、探索更複雜數據集、結合 VAE-GAN、StyleGAN 等技術。Cosplay 照片與動漫角色跨領域轉換也是有趣方向。
                </p>
            </div>
        </div>
    </div>

    <script>
    document.addEventListener('DOMContentLoaded', () => {
        const { animate, inView } = motion;

        const headerH1 = document.querySelector('header h1');
        if (headerH1) {
            animate(headerH1, { opacity: [0, 1], y: [-50, 0] }, { duration: 0.8, ease: 'easeOut' });
        }
        const headerP = document.querySelector('header p');
        if (headerP) {
            animate(headerP, { opacity: [0, 1], y: [50, 0] }, { duration: 0.8, delay: 0.2, ease: 'easeOut' });
        }

        const topInfoBox = document.querySelector('.top-info-box');
        if (topInfoBox) {
            topInfoBox.style.opacity = 0;
            topInfoBox.style.transform = 'translateY(-30px)';
            animate(topInfoBox, { opacity: 1, y: 0 }, { duration: 0.7, ease: 'easeOut' });
        }

        const bentoBoxes = document.querySelectorAll('.bento-box');
        bentoBoxes.forEach((box, index) => {
            box.style.opacity = 0;
            box.style.transform = 'translateY(20px) scale(0.95)';
            inView(box, () => {
                animate(box, { opacity: 1, y: 0, scale: 1 }, { duration: 0.5, delay: (index % Math.min(bentoBoxes.length, 3)) * 0.08, ease: 'easeOut' });
            }, { amount: 0.1 });
        });
    });
    </script>
</body>
</html>