<!DOCTYPE html>
<html lang="zh-Hant">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VAE 動漫角色影像生成 - Deep Learning 101</title>
    <script src="https://cdn.tailwindcss.com/3.4.3"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/framer-motion/10.18.0/framer-motion.umd.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
            background-color: #f5f5f7; /* Apple-like light grey */
            color: #1d1d1f; /* Apple-like dark grey for text */
        }
        .bento-box {
            background-color: #ffffff; /* White boxes */
            border-radius: 1.5rem; /* Generous rounding */
            padding: 2rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05), 0 10px 20px rgba(0,0,0,0.05);
            overflow: hidden; /* Ensures content respects border radius */
            display: flex;
            flex-direction: column;
        }
        .bento-title {
            font-size: 1.75rem; /* 28px */
            line-height: 2.25rem; /* 36px */
            font-weight: 700;
            margin-bottom: 1rem;
            color: #1d1d1f;
        }
        .bento-title-large {
            font-size: 2.5rem; /* 40px */
            line-height: 3rem; /* 48px */
            font-weight: 700;
            margin-bottom: 1.5rem;
        }
        .bento-subtitle {
            font-size: 1.125rem; /* 18px */
            font-weight: 600;
            color: #0071e3; /* Apple blue for subtitles/accents */
            margin-top: 0.5rem;
            margin-bottom: 0.5rem;
        }
        .bento-text {
            font-size: 1rem; /* 16px */
            line-height: 1.75; /* More spacing for readability */
            color: #333333; /* Slightly lighter than main text */
        }
        .bento-text strong, .bento-text b {
            font-weight: 600;
            color: #1d1d1f;
        }
        .bento-text em { /* For markdown emphasis */
            font-style: italic;
            color: #555; /* Slightly darker for emphasis */
        }
        .bento-text a {
            color: #0071e3;
            text-decoration: none;
        }
        .bento-text a:hover {
            text-decoration: underline;
        }
        .bento-list {
            list-style-position: inside;
            padding-left: 0.5rem;
        }
        .bento-list li {
            margin-bottom: 0.75rem;
            padding-left: 1rem;
            position: relative;
        }
        .bento-list li::before {
            content: "\f111"; /* Font Awesome circle icon */
            font-family: "Font Awesome 6 Free";
            font-weight: 900;
            color: #0071e3; /* Apple blue */
            font-size: 0.5rem;
            position: absolute;
            left: -0.25rem;
            top: 0.5em;
        }

        .highlight-tech {
            background: linear-gradient(90deg, rgba(0, 113, 227, 0.15) 0%, rgba(0, 113, 227, 0.05) 100%);
            padding: 0.1rem 0.5rem;
            border-radius: 0.5rem;
            display: inline-block;
        }
        .icon-large {
            font-size: 3rem;
            margin-bottom: 1rem;
            color: #0071e3; /* Apple blue for icons */
        }
        .content-wrapper {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }
        .grid-container {
            display: grid;
            gap: 1.5rem; /* Spacing between boxes */
            grid-template-columns: repeat(1, minmax(0, 1fr));
        }

        @media (min-width: 768px) { /* md */
            .grid-container {
                grid-template-columns: repeat(2, minmax(0, 1fr));
            }
        }

        @media (min-width: 1024px) { /* lg */
            .grid-container {
                grid-template-columns: repeat(3, minmax(0, 1fr));
            }
            .col-span-lg-1 { grid-column: span 1 / span 1; }
            .col-span-lg-2 { grid-column: span 2 / span 2; }
            .col-span-lg-3 { grid-column: span 3 / span 3; }

            .row-span-lg-1 { grid-row: span 1 / span 1; }
            .row-span-lg-2 { grid-row: span 2 / span 2; }
        }

        .bento-box > .motion-div-full-height {
            flex-grow: 1;
            display: flex;
            flex-direction: column;
        }

        .top-info-box {
            background-color: #e9e9ed;
            padding: 1.5rem 2rem;
            border-radius: 1.5rem;
            margin-bottom: 2rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.03);
            text-align: center; /* Centered text */
        }
        .top-info-title {
            font-size: 2rem; /* 32px */
            font-weight: 700;
            color: #1d1d1f;
            margin-bottom: 0.5rem;
        }
        .top-info-text {
            font-size: 1rem;
            line-height: 1.6;
            color: #333333;
        }
        .top-info-text p {
            margin-bottom: 0.5rem;
        }
        .top-info-text strong { /* Ensure strong inside top-info-text is also bold */
             font-weight: 700;
        }
        .top-info-text a {
            color: #0071e3;
            font-weight: 500;
            text-decoration: none;
        }
        .top-info-text a:hover {
            text-decoration: underline;
        }
        .chinese-main-title {
            font-size: 2.8rem; /* Even larger for main Chinese title */
            font-weight: 700;
        }
        .english-subtitle {
            font-size: 1.5rem; /* Smaller English subtitle */
            color: #666;
            font-weight: 400;
        }
    </style>
</head>
<body>
    <div id="app" class="content-wrapper">

        <div class="top-info-box">
            <h1 class="top-info-title">Deep Learning 101</h1>
            <div class="top-info-text">

            <p align="center">
            <strong>Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101</strong>
            </p>
            <p align="center">
            AI是一條孤獨且充滿惶恐及未知的旅程，花俏絢麗的收費課程或活動絕非通往成功的捷徑。<br>
            衷心感謝當時來自不同單位的AI同好參與者實名分享的寶貴經驗；如欲移除資訊還請告知。<br>
            由 <a href="https://www.twman.org/" target="_blank">TonTon Huang Ph.D.</a> 發起，及其當時任職公司(台灣雪豹科技)無償贊助場地及茶水點心。<br>
            </p>
                <p align="center">
                    <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
                        <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="180" style="display:inline-block; border-radius: 10px;">
                    </a>
                     <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank" style="display:inline-block;">
                        <img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 100px !important;width: 180px !important; display:inline-block; border-radius: 10px;">
                    </a>
                </p>
            <p align="center">
            <a href="https://www.youtube.com/@DeepLearning101" target="_blank">YouTube</a> |
            <a href="https://www.facebook.com/groups/525579498272187/" target="_blank">Facebook</a> |
            <a href="https://deep-learning-101.github.io/"> 回 GitHub Pages</a> |
            <a href="http://DeepLearning101.TWMAN.ORG" target="_blank">網站</a> |
            <a href="https://huggingface.co/DeepLearning101" target="_blank">Hugging Face Space</a>
            </p>
                <p>
                    <a href="https://www.youtube.com/watch?v=DF9GMPU8wPU" target="_blank" rel="noopener noreferrer"><i class="fab fa-youtube mr-1"></i>2018/06/08, Nat, Boris, Alice, Ian (蠻牛小隊), VAE: A generative model for 2D anime character faces</a><br>
                </p>
            </div>
        </div>

        <header class="text-center my-12">
            <h1 class="chinese-main-title bg-clip-text text-transparent bg-gradient-to-r from-blue-600 to-sky-400">
                動漫角色影像生成之研究筆記與新手入門指南
            </h1>
            <p class="english-subtitle mt-2">
                研究筆記與新手入門指南
            </p>
        </header>

        <div class="grid-container">
            <div class="bento-box col-span-lg-3">
                <i class="fas fa-graduation-cap icon-large"></i>
                <h2 class="bento-title-large">前言 <span class="text-lg font-normal text-slate-500">Introduction</span></h2>
                <p class="bento-text">
                    很高興能與大家分享我們在 <strong>Variational Autoencoder (VAE)</strong> 應用於動漫角色影像生成方面的一些研究心得與實驗發現。對於剛接觸這個領域的新手研究員或學生而言，理解 VAE 的運作原理及其與傳統 <strong>Autoencoder (AE)</strong> 的差異是進入生成模型世界的重要第一步。
                </p>
            </div>

            <div class="bento-box col-span-lg-1 row-span-lg-2">
                <i class="fas fa-compress-alt icon-large"></i>
                <h2 class="bento-title">從 Autoencoder (AE) 談起 <span class="text-base font-normal text-slate-500">Understanding Autoencoders</span></h2>
                <p class="bento-text">
                    在深入 VAE 之前，我們先快速回顧一下傳統的 <strong>Autoencoder (AE)</strong>。對於新手來說，可以把 AE 想成是一個數據壓縮與解壓縮的過程。它的基本原理是透過一個稱為 <strong class="highlight-tech">Encoder</strong> 的神經網路，將高維度的輸入數據（例如一張圖片）壓縮成一個低維度的向量，這個向量位於一個我們稱為 <strong class="highlight-tech">Latent Space (潛在空間)</strong> 的地方，這個向量就是 <strong class="highlight-tech">Latent Vector (潛在向量)</strong>。然後，再透過另一個稱為 <strong class="highlight-tech">Decoder</strong> 的神經網路，將這個潛在向量還原回原始輸入數據的樣子。
                </p>
                <p class="bento-text mt-2">
                    AE 的核心目標很單純：訓練 Encoder 和 Decoder，使得還原出來的數據與原始輸入數據盡可能相似。這個過程主要用於資料的<strong class="highlight-tech">降維</strong>和<strong class="highlight-tech">特徵提取</strong>。
                </p>
                <h3 class="bento-subtitle mt-4">傳統 AE 的生成限制 <span class="text-sm font-normal text-slate-500">Limitations in Generation</span></h3>
                <p class="bento-text">
                    傳統 AE 將每個輸入數據點映射到潛在空間中的一個「點」。潛在空間中的數據會形成離散的 <strong class="highlight-tech">Clusters (群集)</strong>，群集之間可能存在大量的「空白區域」。在這些空白區域進行採樣並解碼，結果往往是模糊不清甚至「爛掉」的圖像，無法生成有意義的新數據。
                </p>
            </div>

            <div class="bento-box col-span-lg-2 row-span-lg-2">
                <i class="fas fa-magic icon-large"></i>
                <h2 class="bento-title">Variational Autoencoder (VAE) <span class="text-base font-normal text-slate-500">Introducing VAEs</span></h2>
                <p class="bento-text">
                    正因為傳統 AE 在生成方面的局限性，我們引入了 <strong>Variational Autoencoder (VAE)</strong>。VAE 不再將輸入圖像編碼為潛在空間中的一個單一的、固定的「點」，而是將其編碼為潛在空間中的一個<strong class="highlight-tech">「區域」或一個「分佈」</strong>，通常我們假定這是一個高斯分佈。
                </p>
                <p class="bento-text mt-2">
                    Encoder 輸出不再是直接的潛在向量，而是描述這個分佈的參數：一個 <strong class="highlight-tech">Mean Vector (μ，均值向量)</strong> 和一個 <strong class="highlight-tech">Standard Deviation Vector (σ，標準差向量)</strong>。Decoder 不是接收 Encoder 直接輸出的向量，而是從這個由 μ 和 σ 定義的高斯分佈中 <strong class="highlight-tech">Sampling (採樣)</strong> 得到一個潛在向量，再進行解碼。
                </p>
                 <p class="bento-text mt-2">
                    VAE 之所以具有生成能力，就是因為引入了這個「分佈」的概念。即使在訓練數據點之間原本是「空白」的區域，VAE 透過學習到的連續分佈，可以在這些區域進行採樣，並解碼出合理且與附近訓練數據相關的圖像。
                </p>
                <p class="bento-text mt-2">
                    在 VAE 的潛在空間中，相似的數據分佈會形成一個更為<strong class="highlight-tech">集中且連續的區域</strong>，不像傳統 AE 那樣形成分散的離散群集。它同時追求資料分散（形成分佈）與聚集（保持群集特性），這也使得 VAE 的訓練難度相較於傳統 AE 更高。
                </p>
            </div>

            <div class="bento-box col-span-lg-3">
                <i class="fas fa-sitemap icon-large"></i>
                <h2 class="bento-title-large">VAE 的模型架構與訓練要點 <span class="text-lg font-normal text-slate-500">VAE Architecture & Training</span></h2>
                <div class="grid md:grid-cols-2 gap-6 mt-4">
                    <div>
                        <h3 class="bento-subtitle">模型架構 <span class="text-sm font-normal text-slate-500">Model Architecture</span></h3>
                        <ul class="bento-list bento-text">
                            <li><strong>Encoder Network</strong>：接收輸入圖像，通常使用 <strong class="highlight-tech">Convolution (卷積)</strong> 層來提取圖像特徵。</li>
                            <li><strong>輸出分佈參數</strong>：Encoder 輸出兩個向量：Mean Vector (μ) 和 Standard Deviation Vector (σ)。</li>
                            <li><strong>Sampling (採樣)</strong>：從以 μ 為均值、σ 為標準差的高斯分佈中採樣得到一個 Latent Vector。</li>
                            <li><strong>Decoder Network</strong>：接收採樣得到的 Latent Vector，通常使用 <strong class="highlight-tech">Deconvolution (反卷積) / Transposed Convolution</strong> 層來逐步重建圖像。</li>
                            <li><strong>輸出生成圖像</strong>：Decoder 輸出與原始輸入圖像具有相同尺寸的生成圖像。</li>
                        </ul>
                    </div>
                    <div>
                        <h3 class="bento-subtitle">訓練關鍵 <span class="text-sm font-normal text-slate-500">Key Training Aspects</span></h3>
                        <p class="bento-text">
                            <strong>Reparameterization Trick (重參數化技巧)</strong>：解決採樣操作不可微分問題，使誤差能反向傳播。方法：從標準高斯分佈採樣 ε，再計算 `latent_vector = μ + σ * ε`。
                        </p>
                        <p class="bento-text mt-2">
                            <strong>Loss Function (損失函數)</strong>：
                        </p>
                        <ul class="bento-list bento-text mt-1">
                            <li><strong>Reconstruction Loss (重建損失)</strong>：度量生成圖像與原始圖像的相似度 (如 L2 Loss)。</li>
                            <li><strong>KL Divergence (Kullback-Leibler Divergence)</strong>：度量潛在分佈與標準高斯先驗分佈的差異，促使潛在空間光滑連續。</li>
                        </ul>
                        <p class="bento-text mt-2">
                            最小化此聯合損失函數即為 VAE 的<strong class="highlight-tech">Optimize (優化)</strong>過程。
                        </p>
                    </div>
                </div>
            </div>

            <div class="bento-box col-span-lg-2">
                <i class="fas fa-palette icon-large"></i>
                <h2 class="bento-title">動漫角色影像生成實驗與發現 <span class="text-base font-normal text-slate-500">Experiments & Findings</span></h2>
                <p class="bento-text">
                    我們收集了約 <strong>4000 張</strong>動漫角色頭部圖片，使用 Titan 顯卡進行 <strong>1000 個世代</strong>訓練，潛在空間維度 <strong>50 維</strong> (AE 為 256 維)，圖像解析度 <strong>128x128</strong> 像素。
                </p>
                <h3 class="bento-subtitle mt-4">主要發現 <span class="text-sm font-normal text-slate-500">Key Observations</span></h3>
                <ul class="bento-list bento-text">
                    <li><strong>潛在空間分佈</strong>：VAE 形成巨大連續區域，類別界限模糊但保留相似性結構 (相似圖像距離近，不同人物遠)。背景不同也可能距離近。</li>
                    <li><strong>圖像特徵操控</strong>：微擾潛在向量可平滑改變圖像，<strong class="highlight-tech">眼睛顏色/表情、髮型</strong>最易受影響。</li>
                    <li><strong>角色融合</strong>：線性組合潛在向量可生成融合特徵的新角色，但效果不一，有時像「拼接」。</li>
                    <li><strong>連續生成比較</strong>：VAE 能平滑過渡圖像序列 (如眼睛顏色、髮型漸變)，傳統 AE 在空白區解碼會「爛掉」。VAE 潛在空間更集中，標準差小 (VAE ~0.2 vs AE ~53.53)。</li>
                </ul>
                <p class="bento-text mt-2">
                    實驗證明 VAE 作為 <strong class="highlight-tech">Generative Model (生成模型)</strong> 的潛力。
                </p>
            </div>

            <div class="bento-box col-span-lg-1">
                <i class="fas fa-tasks icon-large"></i>
                <h2 class="bento-title">挑戰與未來展望 <span class="text-base font-normal text-slate-500">Challenges & Future</span></h2>
                <h3 class="bento-subtitle">挑戰 <span class="text-sm font-normal text-slate-500">Current Challenges</span></h3>
                <ul class="bento-list bento-text">
                    <li>圖像解析度與清晰度 (目前 128x128)。</li>
                    <li>生成圖像質量主觀性，需客觀指標。</li>
                    <li>潛在空間特徵理解與精確控制難。</li>
                    <li>數據集特性 (多樣性、一致性) 影響大。</li>
                    <li>VAE 訓練複雜，需調參。</li>
                    <li>版權問題需注意。</li>
                </ul>
                 <h3 class="bento-subtitle mt-4">未來展望 <span class="text-sm font-normal text-slate-500">Future Directions</span></h3>
                <p class="bento-text">
                    提高解析度、增強特徵控制 (如 Conditional GANs)、處理更複雜數據集 (全身、多風格)、結合其他模型 (VAE-GAN, StyleGAN)、跨領域轉換 (Cosplay)。
                </p>
            </div>

            <div class="bento-box col-span-lg-3">
                <i class="fas fa-flag-checkered icon-large"></i>
                <h2 class="bento-title">總結 <span class="text-base font-normal text-slate-500">Conclusion</span></h2>
                <p class="bento-text">
                    Variational Autoencoder 為動漫角色影像生成提供了<strong class="highlight-tech">可行且富有潛力</strong>的研究途徑。透過學習連續且結構化的潛在空間分佈，VAE 能夠生成具有創意和連貫性的新圖像，並允許一定程度的特徵操控與角色融合。儘管挑戰仍存，此技術在動漫、遊戲等創意產業中具廣闊應用前景。
                </p>
            </div>
        </div>
    </div>

    <script>
    document.addEventListener('DOMContentLoaded', () => {
        const { animate, inView } = motion;

        const headerH1 = document.querySelector('header h1');
        if (headerH1) {
            animate(headerH1, { opacity: [0, 1], y: [-50, 0] }, { duration: 0.8, ease: 'easeOut' });
        }
        const headerP = document.querySelector('header p');
        if (headerP) {
            animate(headerP, { opacity: [0, 1], y: [50, 0] }, { duration: 0.8, delay: 0.2, ease: 'easeOut' });
        }
        
        const topInfoBox = document.querySelector('.top-info-box');
        if (topInfoBox) {
            topInfoBox.style.opacity = 0;
            topInfoBox.style.transform = 'translateY(-30px)';
            animate(topInfoBox, { opacity: 1, y: 0 }, { duration: 0.7, ease: 'easeOut' });
        }

        const bentoBoxes = document.querySelectorAll('.bento-box');
        bentoBoxes.forEach((box, index) => {
            box.style.opacity = 0;
            box.style.transform = 'translateY(20px) scale(0.95)';
            inView(box, () => {
                animate(box, { opacity: 1, y: 0, scale: 1 }, { duration: 0.5, delay: (index % Math.min(bentoBoxes.length, 3)) * 0.08, ease: 'easeOut' });
            }, { amount: 0.1 }); /* Trigger when 10% of the element is in view */
        });
    });
    </script>
</body>
</html>