<!DOCTYPE html>
<html lang="zh-Hant">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Bilateral Learning for Real-Time Image Enhancement - Deep Learning 101</title>
    <script src="https://cdn.tailwindcss.com/3.4.3"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/framer-motion/10.18.0/framer-motion.umd.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol", "Noto Color Emoji";
            background-color: #f5f5f7; /* Apple-like light grey */
            color: #1d1d1f; /* Apple-like dark grey for text */
        }
        .bento-box {
            background-color: #ffffff; /* White boxes */
            border-radius: 1.5rem; /* Generous rounding */
            padding: 2rem;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05), 0 10px 20px rgba(0,0,0,0.05);
            overflow: hidden; /* Ensures content respects border radius */
            display: flex;
            flex-direction: column;
        }
        .bento-title {
            font-size: 1.75rem; /* 28px */
            line-height: 2.25rem; /* 36px */
            font-weight: 700;
            margin-bottom: 1rem;
            color: #1d1d1f;
        }
        .bento-title-large {
            font-size: 2.5rem; /* 40px */
            line-height: 3rem; /* 48px */
            font-weight: 700;
            margin-bottom: 1.5rem;
        }
        .bento-subtitle {
            font-size: 1.125rem; /* 18px */
            font-weight: 600;
            color: #0071e3; /* Apple blue for subtitles/accents */
            margin-top: 0.5rem;
            margin-bottom: 0.5rem;
        }
        .bento-text {
            font-size: 1rem; /* 16px */
            line-height: 1.75; /* More spacing for readability */
            color: #333333; /* Slightly lighter than main text */
        }
        .bento-text strong, .bento-text b {
            font-weight: 600;
            color: #1d1d1f;
        }
        .bento-text em { /* For markdown emphasis */
            font-style: italic;
            color: #555; /* Slightly darker for emphasis */
        }
        .bento-text a {
            color: #0071e3;
            text-decoration: none;
        }
        .bento-text a:hover {
            text-decoration: underline;
        }
        .bento-list {
            list-style-position: inside;
            padding-left: 0.5rem;
        }
        .bento-list li {
            margin-bottom: 0.75rem;
            padding-left: 1rem;
            position: relative;
        }
        .bento-list li::before {
            content: "\f111"; /* Font Awesome circle icon */
            font-family: "Font Awesome 6 Free";
            font-weight: 900;
            color: #0071e3; /* Apple blue */
            font-size: 0.5rem;
            position: absolute;
            left: -0.25rem;
            top: 0.5em;
        }
        .bento-list ol { /* Nested ordered list */
            list-style-type: decimal;
            margin-left: 1.5rem;
            margin-top: 0.5rem;
        }
        .bento-list ul { /* Nested unordered list */
            list-style-type: disc;
            margin-left: 1.5rem;
            margin-top: 0.5rem;
        }
        .bento-list ul li::before, .bento-list ol li::before {
            content: ""; /* Remove parent's ::before for nested lists */
        }


        .highlight-tech {
            background: linear-gradient(90deg, rgba(0, 113, 227, 0.15) 0%, rgba(0, 113, 227, 0.05) 100%);
            padding: 0.1rem 0.5rem;
            border-radius: 0.5rem;
            display: inline-block;
        }
        .icon-large {
            font-size: 3rem;
            margin-bottom: 1rem;
            color: #0071e3; /* Apple blue for icons */
        }
        .content-wrapper {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }
        .grid-container {
            display: grid;
            gap: 1.5rem; /* Spacing between boxes */
            grid-template-columns: repeat(1, minmax(0, 1fr));
        }

        @media (min-width: 768px) { /* md */
            .grid-container {
                grid-template-columns: repeat(2, minmax(0, 1fr));
            }
        }

        @media (min-width: 1024px) { /* lg */
            .grid-container {
                grid-template-columns: repeat(3, minmax(0, 1fr));
            }
            .col-span-lg-1 { grid-column: span 1 / span 1; }
            .col-span-lg-2 { grid-column: span 2 / span 2; }
            .col-span-lg-3 { grid-column: span 3 / span 3; }

            .row-span-lg-1 { grid-row: span 1 / span 1; }
            .row-span-lg-2 { grid-row: span 2 / span 2; }
        }

        .bento-box > .motion-div-full-height {
            flex-grow: 1;
            display: flex;
            flex-direction: column;
        }

        .top-info-box {
            background-color: #e9e9ed;
            padding: 1.5rem 2rem;
            border-radius: 1.5rem;
            margin-bottom: 2rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.03);
            text-align: center; /* Centered text */
        }
        .top-info-title {
            font-size: 2rem; /* 32px */
            font-weight: 700;
            color: #1d1d1f;
            margin-bottom: 0.5rem;
        }
        .top-info-text {
            font-size: 1rem;
            line-height: 1.6;
            color: #333333;
        }
        .top-info-text p {
            margin-bottom: 0.5rem;
        }
        .top-info-text strong {
             font-weight: 700;
        }
        .top-info-text a {
            color: #0071e3;
            font-weight: 500;
            text-decoration: none;
        }
        .top-info-text a:hover {
            text-decoration: underline;
        }
        .chinese-main-title {
            font-size: 2.8rem;
            font-weight: 700;
        }
        .english-subtitle {
            font-size: 1.5rem;
            color: #666;
            font-weight: 400;
        }
        .presenter-info {
            font-size: 1.125rem; /* 18px */
            color: #555;
            margin-bottom: 2rem;
            text-align: center;
        }
        .main-content-section h1.bento-title-large {
            margin-bottom: 0.5rem; /* Reduced margin for main section titles */
        }
        .main-content-section h2.bento-subtitle {
            margin-top: 1.5rem; /* More space before H2 subtitles */
            font-size: 1.3rem; /* Slightly larger H2 subtitles */
            color: #0071e3;
        }
         .main-content-section h3.bento-subtitle {
            margin-top: 1rem;
            font-size: 1.1rem; /* H3 subtitles */
            color: #1a5fb4; /* Darker blue for H3 */
        }
    </style>
</head>
<body>
    <div id="app" class="content-wrapper">

        <div class="top-info-box">
            <h1 class="top-info-title">Deep Learning 101</h1>
            <div class="top-info-text">
                <p>
                    <strong>Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101</strong>
                </p>
                <p>
                    AI是一條孤獨且充滿惶恐及未知的旅程，花俏絢麗的收費課程或活動絕非通往成功的捷徑。<br>
                    衷心感謝當時來自不同單位的AI同好參與者實名分享的寶貴經驗；如欲移除資訊還請告知。<br>
                    由 <a href="https://www.twman.org/" target="_blank">TonTon Huang Ph.D.</a> 發起，及其當時任職公司(台灣雪豹科技)無償贊助場地及茶水點心。
                </p>
                <p style="display: flex; justify-content: center; align-items: center; gap: 1rem; margin-top:1rem; margin-bottom:1rem;">
                    <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
                        <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" style="width: 180px; height: auto; border-radius: 10px;">
                    </a>
                    <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank">
                        <img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 100px !important;width: 180px !important; border-radius: 10px;">
                    </a>
                </p>
                <p>
                    <a href="https://www.youtube.com/@DeepLearning101" target="_blank">YouTube</a> |
                    <a href="https://www.facebook.com/groups/525579498272187/" target="_blank">Facebook</a> |
                    <a href="https://deep-learning-101.github.io/"> 回 GitHub Pages</a> |
                    <a href="http://DeepLearning101.TWMAN.ORG" target="_blank">網站</a> |
                    <a href="https://huggingface.co/DeepLearning101" target="_blank">Hugging Face Space</a>
                </p>
                <p class="mt-2">
                    <a href="https://www.youtube.com/watch?v=q1Fkqtf-AFU" target="_blank" rel="noopener noreferrer"><i class="fab fa-youtube mr-1"></i>2018/08/17, 黃俊仁 (Ken Huang), Deep Bilateral Learning for Real-Time Image Enhancement</a>
                </p>
            </div>
        </div>

        <header class="text-center my-12">
            <h1 class="chinese-main-title bg-clip-text text-transparent bg-gradient-to-r from-blue-600 to-sky-400">
                即時影像增強之深度雙邊學習技術
            </h1>
            <p class="english-subtitle mt-2">
                Deep Bilateral Learning for Real-Time Image Enhancement
            </p>
            <p class="presenter-info mt-4">分享者：黃俊仁 (Ken Huang) | 日期：2018/08/17</p>
        </header>

        <div class="grid-container">
            <div class="bento-box col-span-lg-3 motion-div main-content-section">
                <div class="motion-div-full-height">
                    <i class="fas fa-chart-line icon-large"></i>
                    <h1 class="bento-title-large">研究關鍵重點彙整 <span class="text-lg font-normal text-slate-500">Key Research Highlights</span></h1>

                    <h2 class="bento-subtitle"><i class="fas fa-comments mr-2"></i>1. 社交網絡情緒分析 <span class="text-sm font-normal text-slate-500">(Social Network Sentiment Analysis / 輿情監控)</span></h2>
                    <p class="bento-text">這是一個利用<strong class="highlight-tech">自然語言處理 (NLP)</strong> 技術來分析社群媒體上文字留言情緒的應用 [1, 2]。</p>

                    <h3 class="bento-subtitle">研究背景與目標</h3>
                    <ul class="bento-list bento-text">
                        <li>我們觀察到，有些加密貨幣（ICO）在社群網絡上的討論度很高，但在某些評分網站上的分數卻偏低 [1]。這讓我們懷疑現有的評分機制可能沒有充分反映真實的使用者情緒 [1, 2]。</li>
                        <li>因此，研究的目標是希望透過分析 Facebook、Twitter 等社交網絡上的文字留言 [1, 2]，判斷這些文字是表達正面或負面的情緒 [1, 2]，進而計算出整體的社群情緒評分 [1]。這項技術可以被視為一種「威脅情報 (threat intelligence)」，用來判斷特定項目（如 ICO）的公眾情緒是正面還是負面，並監控其隨時間的變化趨勢 [2]。這也被稱為<strong class="highlight-tech">語情監控 (Public Opinion Monitoring)</strong> [3, 4]。</li>
                    </ul>

                    <h3 class="bento-subtitle">核心技術與流程</h3>
                    <p class="bento-text">這個項目主要使用了 <strong class="highlight-tech">LSTM (長短期記憶網絡)</strong> 和 <strong class="highlight-tech">CNN (卷積神經網絡)</strong> 這兩種深度學習模型來進行情緒分析 [1, 2]。對於新手來說，LSTM 擅長處理序列資料（如文字句子的順序），而 CNN 雖然常用於圖像，但在文字處理中也能捕捉詞語的局部關聯，兩者結合能提升情緒判斷的準確性。</p>
                    <p class="bento-text">分析流程包含幾個重要步驟 [1, 5]：</p>
                    <ol class="bento-list bento-text" style="list-style-type: decimal; padding-left: 1.5rem;">
                        <li><strong>資料預處理 (Data Preprocessing)</strong>：這是 NLP 中非常基礎但關鍵的一步。原始的文字資料往往很「髒」，包含許多符號、錯字、非標準用語等 [1, 4, 5]。預處理的目的是清理和規範化這些數據，例如移除符號、替換常用詞，或濾掉不必要的雜質，以獲得更「純淨」的資料 [1, 4, 5]。這能大幅降低模型處理的難度 [4]。</li>
                        <li><strong>Tokenization (編碼)</strong>：將清理後的文字分解成更小的單元（如單詞或子詞），並將這些文字單元轉換成數字編碼 [1, 5, 6]。神經網絡需要數字輸入，這一步就是將人類可讀的文字轉化為機器可讀的數字表示 [5]。</li>
                        <li><strong>Embedding (詞嵌入)</strong>：這是深度學習處理文字資料中一個非常核心的概念 [1, 5, 6]。Embedding 層會將前面得到的數字編碼轉換成連續的、低維度的<strong class="highlight-tech">特徵向量 (Feature Vector)</strong> [1, 5, 6]。這些向量不僅僅是數字，更重要的是它們能夠捕捉詞語之間的語義和語法關係 [6]。簡單來說，意思相似或用法相似的詞語，在嵌入空間中的向量會比較靠近 [5]。Embedding Layer 通常是神經網絡處理文字輸入的第一層 [1, 5]。</li>
                        <li><strong>模型訓練與預測</strong>：將 Embedding 層輸出的特徵向量輸入到 LSTM 和 CNN 模型中 [1, 2]。模型會根據已有的標註資料集（例如論文或 Kaggle 上的公開數據集）進行訓練 [1, 7]，學習文字模式與情緒之間的關聯 [1, 5]。訓練好的模型就可以用來預測新的文字評論是正面還是負面，並給出一個情緒分數 [1, 5]。分數以 +/- 2.5 作為判斷正面或負面的基準 [1, 7, 8]。</li>
                    </ol>

                    <h3 class="bento-subtitle">系統建構與展示</h3>
                    <ul class="bento-list bento-text">
                        <li>研究團隊開發了一個網頁應用程式來展示分析結果，這個應用程式被稱為 SOCINT 網站 (未公開) 或簡稱為 sock (Social Intelligence) [1, 3, 7, 8]。</li>
                        <li>系統會收集大量社交網絡數據，目前主要爬取 Facebook 的公開評論，已經收集了約 150 萬筆資料 [1, 4, 7, 8]。Twitter 資料由於 API 限制和變動頻繁等原因，整合遇到較多困難 [1, 4, 8, 9]。</li>
                        <li>根據每日收集到的留言內容計算情緒分數 [1, 7]。</li>
                        <li>提供時間區間選擇（如 3天、7天、10天或 14天）來觀察情緒趨勢 [1, 7, 8, 10]。</li>
                        <li>展示「好的評論」(good comment) 和「壞的評論」(bad comment) 範例供用戶參考 [1, 4, 8]。</li>
                        <li>整合其他 ICO 相關資訊，例如 Facebook 和 Twitter 連結、ing token 上的介紹頁面（主要針對對 ICO 感興趣的用戶，提供白皮書分析、社群媒體成長趨勢、Github commit 次數等資訊）[1, 4, 7, 8]、ICO 基本資訊、社群媒體成長趨勢、Github 資料 [1, 4, 8] 以及合約檢測功能 [4, 8]。</li>
                        <li>設置類似 Help Us 的頁面，展示模型預測的情緒分數，並鼓勵用戶調整評分，以收集更多真實用戶的感受，持續優化模型 [4, 8]。這對於提升模型在真實世界數據上的泛化能力非常重要 [4]。</li>
                    </ul>

                    <h3 class="bento-subtitle">面臨的挑戰</h3>
                    <ul class="bento-list bento-text">
                        <li>社交網絡數據收集困難重重，尤其是 Twitter 的 API 限制多，爬取規則變動頻繁 [1, 4, 8, 9]。Facebook 爬蟲也容易遇到被暫停帳號或需要額外驗證的問題 [9, 11].</li>
                        <li>確保用於訓練模型的數據集能夠代表真實世界中各種複雜的語言情況和情緒表達 [1].</li>
                        <li>模型的原始輸出結果不一定直接適用，可能需要根據實際應用場景進行調整和優化 [1, 12].</li>
                        <li>模型訓練需要大量的時間和計算資源 [1].</li>
                        <li>文字預處理中的許多邊角情況需要特別處理，例如連續的驚嘆號、表情符號等 [1, 4].</li>
                    </ul>

                    <h3 class="bento-subtitle">未來發展方向</h3>
                    <ul class="bento-list bento-text">
                        <li>將情緒分析的應用範圍擴展到 ICO 以外的其他領域，例如餐廳評價、旅遊景點評論或網路產品評價等 [1, 10].</li>
                        <li>考慮開放 API 供開發者使用歷史數據進行研究或開發 [1, 10].</li>
                        <li>持續優化模型性能，提高分析的準確性和效率 [1].</li>
                        <li>可能會探索如何處理更加細微或帶有隱喻的情緒表達 [11].</li>
                    </ul>

                    <hr class="my-8 border-gray-300">

                    <h2 class="bento-subtitle"><i class="fas fa-brain mr-2"></i>2. 深度推理 <span class="text-sm font-normal text-slate-500">(Deep Reasoning / Machine Reasoning)</span></h2>
                    <p class="bento-text">這項研究的核心目標是讓機器能夠執行類似人類的<strong class="highlight-tech">高階認知行為</strong>，而不僅僅是感知層面的任務 [2, 13]。</p>

                    <h3 class="bento-subtitle">核心概念</h3>
                    <ul class="bento-list bento-text">
                        <li><strong>深度推理 / 機器推理</strong> 指的是機器能夠進行更複雜的邏輯思考，包括<strong class="highlight-tech">演繹 (Deduction)</strong> 和<strong class="highlight-tech">歸納 (Induction)</strong> [2, 3, 13]。</li>
                        <li>這與現有深度學習在<strong class="highlight-tech">感知 (Perception)</strong> 問題上的成功有所不同 [2, 13]。感知任務如圖像識別、語音識別主要依賴於從大量數據中學習模式，例如使用 CNN、RNN、LSTM 等模型 [13]。而推理需要更高的抽象能力和邏輯結構理解 [2, 13]。</li>
                        <li><strong>推理的類型</strong>：
                            <ul class="bento-list bento-text">
                                <li><strong>歸納法 (Induction)</strong>：從具體的、個別的案例或觀察中總結出一般性的規律或原則（Bottom-up 方式） [3, 13, 14]。例如，觀察到許多天鵝是白色的，歸納出「天鵝可能是白色的」或「所有天鵝都是白色的」（這個結論有時不一定完全準確，取決於觀察的完整性） [14, 15]。</li>
                                <li><strong>演繹法 (Deduction)</strong>：從已知的一般原則或前提中推導出具體的、必然的結論（Top-down 方式） [3, 13, 14]。例如，已知「所有人都會死」和「蘇格拉底是人」，可以演繹出「蘇格拉底會死」 [14]. 演繹推理在邏輯上更為嚴謹 [15].</li>
                            </ul>
                        </li>
                    </ul>

                    <h3 class="bento-subtitle">推理與記憶的關係</h3>
                     <ul class="bento-list bento-text">
                        <li>人類進行推理的過程與<strong class="highlight-tech">記憶</strong>密切相關 [2, 13, 16, 17]。我們需要將接收到的信息存儲到大腦的記憶中，並在需要時讀取、運用這些信息來進行思考和推導 [2, 13, 16, 17]。</li>
                        <li>大腦的<strong class="highlight-tech">額葉 (Frontal Lobe)</strong> 被認為與高階認知和推理能力有關 [13, 17]。神經科學研究認為，額葉的功能機制可能涉及一個類似<strong class="highlight-tech">控制器 (Controller)</strong> 的部分，它控制著如何處理和存取<strong class="highlight-tech">記憶緩衝區 (Memory Buffer)</strong> 中的信息 [13, 17]。</li>
                        <li>研究人員希望能夠將人類大腦中處理記憶和控制的機制應用到神經網絡模型中，藉此增強模型的推理能力 [2, 17]。具有更強記憶能力的模型被認為可能更能進行推理 [16, 17].</li>
                    </ul>

                    <h3 class="bento-subtitle">相關研究與數據集</h3>
                    <ul class="bento-list bento-text">
                        <li><strong>bAbI 數據集 (bAbI Dataset)</strong>：由 Facebook AI Research 提出的數據集 [2, 3, 13, 18]，專門用於測試機器的推理能力 [2, 3, 13]. 這對於新手來說是理解機器推理能力評估的一個重要資源。
                            <ul class="bento-list bento-text">
                                <li>它包含 20 種不同類型的推理任務 [13, 18]，例如單步或多步的支持事實推理（需要從故事中找到支持答案的事實）、位置推理、計數、集合操作、路徑尋找等 [13, 18]。</li>
                                <li>數據集是由機器生成的 [13, 14]，結構化程度較高 [13]。每個任務通常包含一個簡短的故事或描述，以及一個相關的问题，目標是從故事中推導出答案 [13, 18]。答案往往是明確的 [17]。</li>
                                <li>測試的準確性通常以 100 分為滿分 [13, 18]。目前大多數模型在 bAbI 數據集上的表現看似不錯（例如 RNN 可以達到 100 分，但這是針對訓練集中出現過的實體） [13, 18]。</li>
                                <li>然而，bAbI 這類數據集由於是結構化的合成數據，與真實世界的自然語言推理有所不同 [13]。一個重要的挑戰在於處理<strong class="highlight-tech">未出現過的實體名稱 (entity)</strong> 的推理 [13, 19]。如果模型在訓練時沒有見過某個實體，但在測試時出現了，它能否對其進行正確推理？這比在訓練集中出現過的實體推理更困難 [13, 19]。</li>
                            </ul>
                        </li>
                        <li><strong>記憶增強的神經網絡 (Memory-Augmented Neural Networks)</strong>：研究正在探索將神經網絡與外部記憶體結合的模型，例如 <strong class="highlight-tech">神經圖靈機 (Neural Turing Machine, NTM)</strong> 和 <strong class="highlight-tech">可微分神經計算機 (Differentiable Neural Computer, DNC)</strong> [2, 3, 16, 17].
                            <ul class="bento-list bento-text">
                                <li>NTM 的核心思想是將神經網絡（作為控制器）與可讀寫的外部記憶體（Memory Matrix）結合 [2, 3, 16, 17]，旨在模仿傳統電腦的 CPU 和記憶體架構 [2, 3, 19, 20]. LSTM 雖然有內部記憶單元 (Cell state)，但 NTM 和 DNC 的外部記憶體更大、更靈活，並且控制器可以學習如何精確地讀取和寫入記憶體的特定位置 [18-20].</li>
                                <li>NTM 提供了兩種主要的記憶體<strong class="highlight-tech">定址方式 (Memory Addressing)</strong>：<strong class="highlight-tech">基於內容 (Content-based)</strong> 定址（根據輸入內容與記憶體中存儲內容的相似度來決定讀寫位置）和<strong class="highlight-tech">基於位置 (Location-based)</strong> 定址（通過在現有位置上進行移位或結合歷史位址信息來確定位置）[3, 21]。這使得模型可以學習更複雜的數據結構和存取模式，處理需要記憶和推理的任務 [2, 16, 17].</li>
                                <li>這些記憶增強模型在理論上具有處理更複雜推理問題的潛力 [16]。然而，目前的挑戰在於訓練難度較高 [16, 22]，模型可能不穩定 [22]，離實際應用還有一定距離 [16, 22]。研究正在探索更簡化且有效的記憶增強模型 [16].</li>
                                <li>NTM 的 IO（輸入/輸出）與傳統 RNN/LSTM 類似 [22]。雖然 NTM 的記憶能力理論上更強且能處理更長的序列 [22]，但實際應用不普及的主要原因是訓練困難且結果不穩定 [22]。</li>
                            </ul>
                        </li>
                        <li><strong>其他相關研究</strong>：與語言學（特別是<strong class="highlight-tech">語義學</strong>）的結合被認為可以幫助理解自然語言的邏輯結構 [16]，這對於設計推理模型非常有指導意義 [15, 16].</li>
                    </ul>

                    <h3 class="bento-subtitle">總結與未來方向</h3>
                    <ul class="bento-list bento-text">
                        <li>推理能力是機器邁向更高層次智能的關鍵 [2, 13]。</li>
                        <li>記憶增強模型為實現機器推理提供了潛在的方向 [2, 16]。</li>
                        <li>未來的研究可能包括將這些模型應用於更自然的推理任務 [16]，提高模型的訓練穩定性和泛化能力 [16]。</li>
                        <li>如何處理非結構化或邏輯不那麼嚴謹的自然語言中的推理問題仍然是重要的挑戰 [15, 17].</li>
                    </ul>

                    <hr class="my-8 border-gray-300">

                    <h2 class="bento-subtitle"><i class="fas fa-camera-retro mr-2"></i>3. 深度學習應用於即時圖像處理 <span class="text-sm font-normal text-slate-500">(Real-time Image Processing with Deep Learning)</span></h2>
                    <p class="bento-text">這部分主要探討了如何利用深度學習加速圖像處理演算法，使其能在移動裝置上達到<strong class="highlight-tech">即時 (Real-time)</strong> 性能 [12, 23]。核心技術圍繞著 <strong class="highlight-tech">雙邊濾波器 (Bilateral Filter)</strong> 及其加速方法展開 [23, 24]。</p>

                    <h3 class="bento-subtitle">雙邊濾波器 (Bilateral Filter)</h3>
                    <ul class="bento-list bento-text">
                        <li><strong>概念</strong>：雙邊濾波器是一種圖像處理濾波器 [3, 23]，它在進行加權平均時，不僅考慮像素的<strong class="highlight-tech">空間距離 (Spatial Domain)</strong>（就像傳統的高斯濾波器 Gaussian Filter）[3, 24]，還額外考慮了像素之間的<strong class="highlight-tech">像素值差異 (Pixel Value)</strong>（亮度或顏色差異）[3, 6, 24]。</li>
                        <li><strong>優勢</strong>：正是因為考慮了像素值差異，雙邊濾波器在平滑圖像（降低雜訊）的同時，能夠有效地<strong class="highlight-tech">保留邊緣 (Edge Preservation)</strong> 細節 [3, 6, 24]。在高斯濾波中，如果像素距離近，即使它們的像素值差異很大（代表邊緣），它們的權重仍然可能很高，導致邊緣被模糊 [24]。而雙邊濾波器在遇到像素值差異大的地方（如物體邊緣），會降低這些像素的權重，從而減少邊緣的模糊程度 [24]。這項特性使得雙邊濾波器非常適合用於美顏磨皮等應用，可以在平滑皮膚的同時保留五官的邊緣清晰度 [24]。</li>
                        <li><strong>問題</strong>：傳統雙邊濾波器的主要缺點是<strong class="highlight-tech">計算速度很慢 (Performance is poor)</strong> [6, 24]，難以在行動裝置上實現即時處理 [23, 24]。這是因為它的計算過程包含兩個高斯函數的計算 [24]，而且對於圖像中的每一個像素，都需要考慮其周圍鄰域像素的空間距離和像素值差異來計算權重並進行加權平均 [6, 24]。</li>
                    </ul>

                    <h3 class="bento-subtitle">加速技術的演進</h3>
                    <ul class="bento-list bento-text">
                        <li><strong>早期加速方法</strong>：針對傳統雙邊濾波器計算速度慢的問題，研究人員開始探索加速方法 [24]。嘉文程 (Jiawen Chen) 等人曾在 2006 年和 2007 年提出了雙邊濾波器的加速演算法，包括一些 GPU 加速的實作細節 [24-26]。</li>
                        <li><strong>雙邊格網 (Bilateral Grid)</strong>：作為加速方法的一部分，<strong class="highlight-tech">雙邊格網</strong>這種資料結構被提出 [3, 26]。它的核心思想是將圖像從二維平面擴展到包含像素強度（或顏色）資訊的<strong class="highlight-tech">三維空間</strong> [3, 26]。在這個三維格網中進行操作（例如卷積）可以比在原始圖像上直接應用雙邊濾波器更快 [6, 26]。然後再從這個三維格網中<strong class="highlight-tech">切片 (Slicing)</strong> 或<strong class="highlight-tech">採樣</strong>回二維圖像 [3, 27]。你可以把 Bilateral Grid 想像成一個三維的查找表或數據結構，裡面儲存了圖像轉換的資訊 [26].</li>
                        <li><strong>基於降採樣與格網的加速 (Bilateral guided upsampling - 2016 paper)</strong>：嘉文程等人在 2016 年又提出了新的加速技術 [24, 26]。他們觀察到，不論多複雜的圖像處理演算法，如果只看圖像的一小塊區域（局部區域），輸入像素與輸出像素之間的轉換關係其實相對簡單，可以用一個<strong class="highlight-tech">線性函數</strong>來逼近 [26]。基於這個觀察，他們的方法是先將高解析度的輸入圖像<strong class="highlight-tech">降採樣 (Downsampling)</strong> 到一個較低的解析度（例如 256x256）[3, 26, 27]。在低解析度的圖像上應用複雜的圖像處理演算法會非常快 [27]。然後，利用這個低解析度輸入和輸出的對應關係，學習或擬合出一個 Bilateral Grid [26, 27]。最後，利用這個 Bilateral Grid，並結合原始的高解析度輸入圖像（作為 guidance），進行<strong class="highlight-tech">上採樣 (Upsampling)</strong> 操作 [3, 27]。這個上採樣過程應用的是 Bilateral Grid 中儲存的線性轉換，由於是簡單的線性運算，可以利用 GPU 大幅加速 [27]。這種方式可以在保持視覺效果接近高解析度直接處理的同時，顯著提升速度 [27]。</li>
                    </ul>

                    <h3 class="bento-subtitle">深度學習加速方法 (Learning for Real-time Image Processing - 2017 paper)</h3>
                    <p class="bento-text">這是 K 介紹的 Google 2017 年發表在 SIGGRAPH 上的論文 [23-25]。這篇論文的核心思想與 2016 年的方法類似，但將其中一個關鍵步驟替換成了深度學習模型 [27]。</p>
                    <ul class="bento-list bento-text">
                        <li><strong>核心思想</strong>：不是像 2016 年那樣從低解析度輸入輸出的對應關係去「擬合」一個 Bilateral Grid，而是<strong class="highlight-tech">訓練一個神經網絡，讓它直接從低解析度的輸入圖像中學習並預測出 Bilateral Grid</strong> [6, 27]。</li>
                        <li><strong>流程</strong>：
                            <ol class="bento-list bento-text" style="list-style-type: decimal; padding-left: 1.5rem;">
                                <li>將高解析度輸入圖像降採樣到低解析度 (例如 256x256) [27].</li>
                                <li>將低解析度圖像輸入一個神經網絡 [6, 27]。</li>
                                <li>神經網絡的目標是<strong class="highlight-tech">預測 (predict)</strong> 出一個 Bilateral Grid [6, 27]。這個 Bilateral Grid 可以想像成一個三維的矩陣，其中每個網格點儲存了一個表示顏色轉換的<strong class="highlight-tech">色彩轉換矩陣 (Color Transform Matrix)</strong> [3, 26]。這些矩陣代表了局部的線性變換 [27, 28]。</li>
                                <li>網絡結構包含處理低層次特徵的卷積層，並分成<strong class="highlight-tech">局部特徵 (Local Feature)</strong> 和<strong class="highlight-tech">全局特徵 (Global Feature)</strong> 兩個分支 [6, 29]。
                                    <ul class="bento-list bento-text">
                                        <li><strong>局部特徵分支</strong>：用於捕捉和處理圖像的局部細節，以便實現局部性的處理效果 [6, 29]。例如，只對圖像中的人臉區域進行提亮或美化，而其他區域不受影響 [29]。</li>
                                        <li><strong>全局特徵分支</strong>：用於捕捉圖像的整體上下文信息，確保處理後的圖像具有連續性和一致性 [6, 29]。例如，讓整片天空的顏色變化是平滑連續的，避免出現不自然的色塊或偽影 (artifact) [29]。</li>
                                        <li>這兩個分支的結果會被<strong class="highlight-tech">融合 (fusion)</strong> 起來，生成最終的 Bilateral Grid [29].</li>
                                    </ul>
                                </li>
                                <li>最後，利用預測出的 Bilateral Grid 和原始高解析度輸入圖像（作為 guidance），進行<strong class="highlight-tech">上採樣 (Upsampling)</strong> 操作 [3, 27, 29]。這一步通過對 Bilateral Grid 進行<strong class="highlight-tech">切片 (Slicing)</strong> 和應用其中的轉換矩陣來實現 [3, 12]。這個過程可以利用 GPU 大幅加速，從而達到即時的處理速度 [12, 27]。</li>
                            </ol>
                        </li>
                        <li><strong>優勢</strong>：這種方法的巨大突破在於，它<strong class="highlight-tech">不需要實作</strong>原始的複雜圖像處理演算法本身 [12]。只要有目標演算法的輸入和輸出範例（用於訓練神經網絡），神經網絡就能學習如何產生對應的 Bilateral Grid [12]。這使得將複雜的圖像處理效果部署到運算能力有限的行動裝置上變得非常容易和快速 [12, 23]。模型一旦訓練好，就能在手機上實現每秒 40-50 幀的即時處理速度 [12]。</li>
                        <li><strong>應用</strong>：這項技術可以應用於模擬各種複雜的圖像處理演算法，例如 <strong class="highlight-tech">HDR (High Dynamic Range)</strong> 效果、色彩風格轉換、局部調整等 [3, 12, 23]。Google 的 Photography 產品就應用了這項技術來實現自動修圖，提升圖像的亮度、對比度、色彩鮮豔度等 [12, 23, 24]。</li>
                        <li><strong>實作細節</strong>：雖然論文提供了模型架構，但要在行動裝置上達到即時性能，最後的 Bilateral Grid 上採樣/切片步驟需要針對 GPU 進行底層程式碼實作 [12]。直接使用模型並不能達到最佳效能 [12]。在訓練時，需要準備大量的輸入/輸出圖像對作為數據集 [12]。</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>

    <script>
    document.addEventListener('DOMContentLoaded', () => {
        const { animate, inView } = motion;

        const headerH1 = document.querySelector('header h1.chinese-main-title');
        if (headerH1) {
            animate(headerH1, { opacity: [0, 1], y: [-50, 0] }, { duration: 0.8, ease: 'easeOut' });
        }
        const headerP = document.querySelector('header p.english-subtitle');
        if (headerP) {
            animate(headerP, { opacity: [0, 1], y: [50, 0] }, { duration: 0.8, delay: 0.2, ease: 'easeOut' });
        }
         const presenterInfo = document.querySelector('header p.presenter-info');
        if (presenterInfo) {
            animate(presenterInfo, { opacity: [0, 1], y: [50, 0] }, { duration: 0.8, delay: 0.4, ease: 'easeOut' });
        }

        const topInfoBox = document.querySelector('.top-info-box');
        if (topInfoBox) {
            topInfoBox.style.opacity = 0;
            topInfoBox.style.transform = 'translateY(-30px)';
            animate(topInfoBox, { opacity: 1, y: 0 }, { duration: 0.7, delay: 0.3, ease: 'easeOut' });
        }

        const motionDivs = document.querySelectorAll('.motion-div');
        motionDivs.forEach((div, index) => {
            div.style.opacity = 0;
            div.style.transform = 'translateY(20px) scale(0.95)';
            inView(div, () => {
                animate(div, { opacity: 1, y: 0, scale: 1 }, { duration: 0.5, delay: (index % Math.min(motionDivs.length, 3)) * 0.1 + 0.5, ease: 'easeOut' });
            }, { amount: 0.1 });
        });
    });
    </script>
</body>
</html>