---
layout: default
title: Deep Learning 101, 台灣曾經最高最早發起的深度學習社群 @ 83F, 台北101
---

<p align="center">
  <strong>The top private AI Meetup in Taiwan, launched on 2016/11/11 @ 83F, Taipei 101</strong>
</p>
<p align="center">
  <strong>Deep Learning 101, 台灣曾經最高最早發起的深度學習社群 @ 83F, 台北101</strong><br><br>
  AI是條寂寞且惶恐的道路，花俏的收費課程或活動絕不會是條捷徑<br>
  本頁內容為過往實名分享制的讀書會，感謝來自不同公司參與者的支持；如欲移除資訊還請告知。<br>
  Deep Learning 101 只由 TonTon Huang Ph.D. 及其當時任職公司無償贊助場地及茶水點心，無 Co-organizer<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="400">
  </a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/">台灣人工智慧社團 FB</a> |
  <a href="https://www.twman.org/">TonTon Huang Ph.D.</a> |  
  <a href="http://DeepLearning101.TWMAN.ORG">台灣人工智慧社團 網站</a> |
  <a href="https://huggingface.co/DeepLearning101">Hugging Face</a>
</p>
<p align="center">
<a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" ></a>
</p>

# 第十章 序列建模：循環和遞歸網路 - [YouTube](https://www.youtube.com/watch?v=p6xzPqRd46w) - <a href="https://deep-learning-101.github.io/">回上一頁 GitHub Pages</a>

### Recurrent and Recursive Nets (2017/05/05)

# 循環神經網路 (Recurrent Neural Network, RNN) 及其相關架構與挑戰

本摘要根據您提供的資料來源，匯整了關於循環神經網路 (RNN) 及其相關概念的重點。

## 什麼是 RNN？為什麼需要 RNN？

*   循環神經網路 (RNN) 是一種能夠處理**序列數據**的神經網路架構。
*   與傳統神經網路 (如前饋網路 FFN 和卷積網路 CNN) 不同，RNN 具有**循環結構**，允許資訊在時間步之間傳遞。
*   RNN 的主要優勢在於能夠處理具有**順序關係**的數據。序列數據的含義往往取決於元素的順序，傳統網路難以捕捉這種順序的重要性及**跨越較長距離的依賴關係**。
*   FFN 在不同位置使用不同權重，參數量隨序列長度顯著增加，且難以捕捉遠距離依賴。
*   CNN 使用小型滑動窗口，主要捕捉**局部**關係 (如鄰近詞彙或像素)，難以捕捉跨越較大距離的依賴。
*   RNN 透過在時間步之間傳遞資訊，能夠處理**可變長度的序列**並**在序列的不同位置共享知識**。

## RNN 的參數共享

*   參數共享是 RNN 的一個關鍵特性。在處理序列的不同時間步時，RNN 使用**相同的權重集**和相同的更新規則。例如，連接 `h^(t-1)` 到 `h^(t)` 的權重矩陣 `W` 在所有時間步是共享的。
*   參數共享帶來的好處包括：
    *   **處理可變長度的序列：** 由於每個時間步的計算模塊相同，RNN 可以靈活處理任意長度的輸入序列。
    *   **在時間上共享統計強度：** 模型可以在序列的不同位置和不同長度的序列之間泛化學到的模式。
    *   **減少模型參數：** 參數共享避免了參數隨序列長度線性增長的問題，顯著減少了總參數量，使得模型更高效且不容易過擬合。

## RNN 的訓練：通過時間反向傳播 (BPTT)

*   通過時間反向傳播 (BPTT) 是將標準的反向傳播算法應用於**展開後的循環神經網路計算圖**，用於計算損失函數關於 RNN 參數的梯度。
*   **工作原理：**
    1.  **前向傳播：** 沿著時間序列對 RNN 進行前向計算，得到每個時間步的隱藏狀態、輸出和總損失。這相當於在展開圖上進行一次完整的前向傳播。
    2.  **反向傳播：** 從最終時間步的損失開始，沿著展開的計算圖**從後向前反向傳播梯度**。計算損失關於每個時間步的輸出和隱藏狀態的梯度，以及關於共享參數的局部梯度。
    3.  **梯度累加：** 由於參數是共享的，損失函數關於共享參數的總梯度是其在每個時間步產生的局部梯度的**總和** (`∂L/∂W = Σ_t ∂L/∂W^(t)`)。在反向傳播中計算出每個局部梯度後，將它們累加得到最終梯度。

## RNN 的變種與挑戰

### 雙向 RNN (BiRNN)

*   **優點：** 能夠在每個時間步利用輸入序列的**完整過去和未來上下文資訊**，對於需要理解全局依賴的任務 (如機器翻譯編碼器、NER、情感分析) 很有幫助，通常性能優於單向 RNN。
*   **局限性：** 需要**一次性獲得整個輸入序列**才能計算反向 RNN 的狀態，不適用於需要對正在輸入的序列進行**實時預測** (online task) 的任務 (如實時語音識別)。計算成本約為單向 RNN 的兩倍。
*   BiRNN 是一個 RNN 模型，這與 Seq2Seq 模型中的 Encoder 和 Decoder 是兩個**分開的** RNN 不同。

### 遞歸神經網路 (Recursive Neural Network)

*   遞歸網路通過在輸入的**樹狀結構**上遞歸應用一個相同的神經網路模塊來學習和傳播資訊。
*   資訊通常從**葉節點向根節點**傳播 (自底向上)。葉節點先轉為向量表示，非葉節點的表示則通過將其子節點的表示作為輸入，送入共享神經網路層計算得到。根節點的表示常用於代表整個輸入樹的語義或結構資訊。
*   也可以設計自頂向下傳播資訊的遞歸網路。
*   RNN 可以看作是遞歸網路的一個**特殊情況**，即處理的樹結構退化為一個**線性鏈**。遞歸網路更擅長處理具有**明顯層次化結構**的數據 (如句子的語法結構)。

### 序列到序列 (Sequence to Sequence, Seq2Seq) 模型

*   Seq2Seq 模型常用於處理輸入和輸出都是序列的任務，如**機器翻譯**和**對話生成**。
*   典型架構包含一個 **Encoder** 和一個 **Decoder**，通常使用 RNN (如 GRU 或 LSTM)。Encoder 將輸入序列編碼為一個固定長度的上下文向量，Decoder 則根據這個向量生成輸出序列。
*   Google 的機器翻譯模型曾使用 GRU 作為核心單元，並嘗試將輸入句子**反轉**以改善效果。解釋是反轉後，輸入序列開頭的詞與輸出序列開頭的詞在序列中的距離變近，有助於模型學習。
*   在對話生成中的應用：Google 曾使用 Seq2Seq 模型進行通用聊天和客服系統對話。訓練資料來源包括電影腳本和客服系統記錄。
*   Seq2Seq 模型訓練的資料準備：需要整理**對話資料對** (輸入對應輸出)。中文需要先進行**斷詞**。將詞彙建立**索引**並建立詞彙表。可能需要處理**低頻詞**或未知詞，將其替換為特殊符號 (`UNK`)。
*   訓練時為提高效率，常使用 **mini-batch**，將長度相似的序列分到同一 batch，不足長度的部分用 **padding** 補齊。
*   **困惑度 (Perplexity)** 是衡量語言模型預測能力的指標。困惑度越低表示模型預測下一個符號的不確定性越小，模型學得越好。
*   **Dual Encoder (雙編碼器)** 模型：一種用於提升**檢索式**對話系統排名的模型。它將輸入 (context) 和潛在的回應 (response) 分別通過兩個 Encoder 編碼為向量，然後計算它們的相似度 (如 cosine 相似度)，希望好的對話對的向量相似度高。可用於對檢索到的多個回應進行排序。

### 外部記憶網路 (External Memory Network)

*   如神經圖靈機 (NTM) 或可微神經計算機 (DNC)，包含 **控制器** (通常是 RNN)、**外部記憶** (二維矩陣) 和 **讀寫頭**。
*   讀寫頭通過**注意力機制**與外部記憶交互。
    *   **基於內容的尋址 (Content-based addressing)：** 根據查詢向量與記憶內容的相似性決定讀寫位置。
    *   **基於位置的尋址 (Location-based addressing)：** 控制器直接指定讀寫位置，或基於先前位置確定。
*   現代外部記憶網路常結合使用這兩種尋址方式。

### 梯度消失與梯度爆炸問題 (Vanishing/Exploding Gradient Problem)

*   這是訓練 RNN 的主要挑戰，特別是在處理**長序列**時。
*   在 BPTT 過程中，由於使用**鏈式法則**，梯度需要通過多個時間步**連續相乘**。
*   如果乘數 (通常是權重矩陣和激活函數的梯度) 的值**持續小於 1**，梯度會指數級**減小**，導致遠距離時間步的梯度變得微不足道，模型難以學習長距離依賴，稱為**梯度消失**。
*   如果乘數值**持續大於 1**，梯度會指數級**增大**，導致訓練不穩定，稱為**梯度爆炸**。

### 長短期記憶網路 (LSTM) 與門控循環單元 (GRU)

*   LSTM 和 GRU 是 RNN 的變體，旨在解決標準 RNN 的梯度消失問題。
*   **LSTM** 引入了 **記憶單元 (Cell State)** 和三個**門 (Gates)**：遺忘門 (Forget Gate)、輸入門 (Input Gate) 和輸出門 (Output Gate)。這些門控制著資訊如何在記憶單元中流動和更新。
*   記憶單元的更新包含**加法**操作 (前一個記憶單元的內容加上新的輸入內容，受遺忘門和輸入門控制)。這種**加法**的結構是 LSTM 能夠緩解梯度消失的關鍵，因為梯度可以更容易地通過加法節點傳播，而不是僅僅依賴乘法鏈。
*   **GRU** 是 LSTM 的簡化版本，參數較少。它將遺忘門和輸入門合二為一成為**更新門 (Update Gate)**，並引入**重置門 (Reset Gate)**。GRU 也沒有單獨的記憶單元，直接在隱藏狀態中進行資訊傳遞和控制。
*   GRU 和 LSTM 在許多任務上表現相似，且通常優於標準 RNN。

### 增加 RNN 深度

*   在 RNN 中堆疊多個 RNN 層可以增加模型的**表達能力**，學習更複雜的非線性函數和層次化的時間表示。
*   較低層可能捕捉局部、短期的模式，而較高層學習更全局、長期的結構。這類似於深度前饋或卷積網路。

## RNN 的其他應用

除了序列到序列任務，RNN 還廣泛應用於：
*   語音識別
*   文本生成
*   情感分析
*   命名實體識別 (NER)
*   時序數據預測
*   圖像描述生成
*   影片分析
*   異常檢測
*   推薦系統
*   作為其他模型的組成部分 (如結合 CNN 的圖像描述模型)
*   在圖像處理中作為 CNN 的替代方法，將圖像視為序列處理。

## 提及的人物與組織

資料來源中提到了以下人物和組織，他們在 RNN 的發展、教學或應用中扮演了角色：
*   **Line 與未知講者：** 本次報告的兩位講者。
*   **李宏毅：** 老師或研究者，其教學風格對講者理解 RNN 有幫助。
*   **Andrew Ng：** 前百度首席科學家，在語音辨識領域有重要貢獻。
*   **Stanford CS224N 的課程：** 講者提及的關於自然語言處理的課程。
*   **Google：** 在機器翻譯和對話生成中應用 Seq2Seq 模型。
*   **印度人老師：** 製作了 LSTM 教學影片的老師。
*   **Karpathy (Andrej Karpathy)：** 部落格作者，其對 LSTM 的解釋清晰直觀。
*   **教育部的國語辭典：** 在中文斷詞討論中提及的資源。
*   **中研院：** 在中文斷詞和語料庫方面有工作。
*   **百度：** Andrew Ng 曾在此工作。
*   **Workshop on Statistical Machine Translation (WMT)：** 提供機器翻譯數據集的工作坊。
*   **PTT：** 講者收集對話數據的來源之一。
*   **HDF format：** 一種數據格式，在圖像數據處理中提及。
*   **jieba 斷詞器：** 一種中文斷詞工具。

這些人物和組織的提及主要圍繞著 RNN 的理論、發展、挑戰及其在自然語言處理領域的應用。