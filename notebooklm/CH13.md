---
layout: default
title: Deep Learning 101, 台灣曾經最高最早發起的深度學習社群 @ 83F, 台北101
---

<p align="center">
  <strong>The top private AI Meetup in Taiwan, launched on 2016/11/11 @ 83F, Taipei 101</strong>
</p>
<p align="center">
  <strong>Deep Learning 101, 台灣曾經最高最早發起的深度學習社群 @ 83F, 台北101</strong><br><br>
  AI是條寂寞且惶恐的道路，花俏的收費課程或活動絕不會是條捷徑<br>
  本頁內容為過往實名分享制的讀書會，感謝來自不同公司參與者的支持；如欲移除資訊還請告知。<br>
  Deep Learning 101 只由 TonTon Huang Ph.D. 及其當時任職公司無償贊助場地及茶水點心，無 Co-organizer<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="400">
  </a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/">台灣人工智慧社團 FB</a> |
  <a href="https://www.twman.org/">TonTon Huang Ph.D.</a> |  
  <a href="http://DeepLearning101.TWMAN.ORG">台灣人工智慧社團 網站</a> |
  <a href="https://huggingface.co/DeepLearning101">Hugging Face</a>
</p>
<p align="center">
<a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" ></a>
</p>

# 第十三章 線性因子模型 - [YouTube](https://www.youtube.com/watch?v=zVENYs30Ny4&) - <a href="https://deep-learning-101.github.io/">回上一頁 GitHub Pages</a>

### Linear Factor Models (2017/08/11)

## 1. 線性因子模型的基本概念

*   **核心思想：** 觀測數據 `x` 假定是由一組潛在的、未觀察到的因子 `h` 通過一個線性變換，再加上一些噪聲生成的 [1-3]。
*   **生成過程：**
    1.  從一個先驗分布 `p(h)` 中抽取潛在因子 `h` [1, 3]。此先驗分布通常假設是因子化的 (`p(h) = Π_i p(h_i)`)，表示潛在因子之間先驗獨立 [1-3]。
    2.  觀測數據 `x` 由潛在因子 `h` 通過一個線性變換 `Wh + b` 生成，並疊加上一些噪聲：`x = Wh + b + noise` [1, 3]。其中 `W` 是因子載荷矩陣，`b` 是偏置向量 [1]。
*   **潛在因子 (h) 的先驗分布：** 通常是因子化的，每個 `p(h_i)` 通常是一個簡單的分布 [1, 2]。常見選擇取決於具體模型：
    *   **高斯分布 (N(0, I))：** PPCA 和因子分析 (FA) 的標準假設 [1-4]。不會誘導稀疏性 [2]。
    *   **非高斯分布 (如 Laplace 或 Student-t)：** 稀疏編碼 (Sparse Coding) 的關鍵假設，誘導潛在編碼 `h` 的稀疏性 [2, 3, 5, 6]。
*   **噪聲項：** 通常假設是獨立的（在 `x` 的不同維度上），並且服從高斯分布，例如均值為零、協方差為對角矩陣的高斯噪聲 [1-3]。對角協方差矩陣意味著觀測數據 `x` 不同維度上的噪聲是獨立的 [2, 4]。
*   **角色：** 線性因子模型是許多更複雜的機率模型（包括一些深度學習模型）的基礎 [7-9]。它們可用於：
    *   **降維：** 當 `h` 的維度低於 `x` 時 [7, 9]。
    *   **特徵學習：** `h` 可視為潛在特徵 [7, 9]。
    *   **密度估計：** 定義 `p_model(x)` [7, 9]。
    *   **混合模型的組件** [7, 9]。
*   **簡單性：** 被認為是「最簡單的生成模型和學習數據表示的最簡單模型」，因為其基於簡單的線性變換和通常為高斯的分布假設，主要捕捉數據的線性和二階統計特性 [8-10]。

## 2. 特定線性因子模型

### 2.1 機率 PCA (PPCA) 和因子分析 (FA)

*   **共同點：** 都是線性因子模型；潛在因子 `h` 服從標準正態分布 N(0, I)；觀測數據 `x` 在給定 `h` 時服從高斯分布 [3, 4, 7]。
*   **主要區別 (噪聲協方差矩陣)：**
    *   **PPCA：** 假設噪聲是 **各向同性 (isotropic)** 的高斯噪聲，協方差矩陣是 `σ²I` [2-4, 7]。所有觀測維度上的噪聲方差相同 [2, 4]。
    *   **FA：** 假設噪聲協方差矩陣是 **對角的** (`ψ = diag(σ²)`)，但不一定是各向同性的 [2-4, 7]。每個觀測維度 `x_i` 可以有自己的噪聲方差 `σ_i²` [2, 4]。在給定 `h` 時，觀測數據 `x` 的各維度是條件獨立的，因為噪聲在不同維度上獨立 [2, 4]。
*   **參數學習：** 通常使用 **期望最大化 (EM) 算法** [3, 11-13]。EM 算法對於這兩者是通用的迭代學習方法，用於尋找最大概似估計 [11-13]。雖然標準 PCA (與 PPCA 在噪聲趨近零時相關) 有解析解，但 EM 對於帶有潛變量的機率模型更具普適性 [12, 14]。音訊中提到 EM 算法在處理高維數據時，每一步的計算量可能比直接計算特徵值分解要少 [14]。
*   **與標準 PCA 的關係：** 機率 PCA 相比標準 PCA 可以更容易地集成先驗信息或用於數據生成 [15, 16]。PPCA 的機率模型允許生成數據，其產生的數據點可能在線性子空間附近散開，形成一個「厚」的流形近似，而標準 PCA 投射後數據點嚴格位於線性子空間上 [15-17]。音訊中提到，當 PPCA 的噪聲方差 `σ²` 趨近於零時，其結果應趨近於標準 PCA [14-16]。

### 2.2 獨立成分分析 (ICA)

*   **核心思想：** 將觀測到的多維信號分解為若干個統計上 **獨立的非高斯** 源信號的線性混合 [3, 11, 12]。觀測數據 `x` 被建模為 `x = Wh` (可包含噪聲和偏置) [11]。
*   **關鍵假設：** 與 PCA 和因子分析根本不同之處在於，ICA 假設潛在因子 `h` (即源信號) 是 **非高斯** 且 **統計上相互獨立** 的 [3, 12, 18]。
*   **非高斯性的必要性：** 根據中心極限定理，多個獨立隨機變量的和趨向於高斯分布 [12, 18]。如果源信號是高斯的，它們的線性混合仍是高斯的，無法唯一地分離出原始源信號 [12, 18]。只有當源信號是非高斯時，混合信號才保留非高斯性，ICA 得以通過最大化非高斯性來分離成分 [12, 18]。音訊中提到，如果信號中包含兩個或更多高斯獨立成分，ICA 無法將它們分開 [19]。
*   **目標：** 從混合信號 `x` 中恢復原始的、獨立的源信號 `h` 以及混合矩陣 `W` (或其逆，分離矩陣) [3, 18]。
*   **學習準則：** 通常涉及最大化觀測數據的概似，或等價地最大化某種衡量源信號獨立性的度量 [3, 20]。常用度量包括：
    *   **最大化非高斯性：** 高斯變量的熵在所有同方差變量中最大，因此可以通過最大化非高斯性來分離。常用度量如 **負熵 (Negentropy)** 和 **峰度 (Kurtosis)** [3, 20-23]。負熵衡量一個變量與同協方差高斯變量的熵差，差越大非高斯性越高 [20, 22, 23]。峰度衡量分布的「尖峭」程度，非高斯分布的峰度非零 [20, 21, 23]。音訊中提到峰度對異常值比較敏感，而負熵較魯棒，但計算複雜，實際常使用其近似 [21-23]。
    *   **最小化互信息 (Mutual Information)：** 最小化估計出的源信號之間的互信息，使其盡可能獨立 [3, 20, 22, 23]。互信息與負熵有關，最小化互信息大致等價於最大化非高斯性 [22, 23]。
*   **應用：** ICA 有廣泛應用 [23, 24]。例如：腦磁圖 (MEG) 中的腦活動分離（JON 專家提及）[23-25]、圖像去噪（分離圖像與噪聲）[23, 24]、金融時間序列分析（分離不同的金流動態）[23, 24]。
*   **生物學基礎：** 音訊討論中提到，儘管 ICA 的機制可能與真實生物系統（如視覺皮層）的運作方式（如 CNN 的分層特徵提取）不完全直接對應，但在更高層次的認知中，神經元對抽象概念的反應表現出類似獨立成分的特性，少數神經元對特定概念有響應，這與稀疏編碼的生物學證據有關聯 [26, 27]。

### 2.3 慢特徵分析 (SFA)

*   **核心思想：** **慢度原則 (slowness principle)** [3, 20]。場景中描述物體的單個量或重要特徵的變化通常比構成場景的原始感官輸入（如像素值）的變化慢得多 [3, 20, 28]。
*   **目標：** 從快速變化的時間信號中學習 **時間上緩慢變化** 的不變特徵 [3, 5, 20]。
*   **學習過程：** SFA 旨在找到輸入信號的線性變換 `f(x^(t))`，使得輸出的特徵 `f(x^(t))` 隨時間變化盡可能緩慢 [3, 20, 28]。目標函數通常是最小化輸出特徵在相鄰時間步之間的平方差的期望，同時滿足一些約束條件（如輸出零均值、單位方差以及不同特徵之間去相關） [3, 20]。可以通過廣義特徵值問題來求解 [20]。
*   **學習到的特徵特性：** 具有 **時間上的緩慢變化性** 和 **對快速變化的輸入的不變性** [3, 5]。捕捉了輸入信號中隨時間持續存在的、相對穩定的方面 [5]。
*   **應用場景：** 從視覺輸入中學習不變特徵（如物體身份而非位置）[3, 5]、機器人導航（學習環境中穩定地標）[3, 5]、語音處理（提取緩慢變化的語義信息）[3, 5]、或其他需要從快速變化的時間序列中提取緩慢變化潛在狀態的任務 [3, 5]。
*   **限制與生物學基礎：** 音訊討論中對 SFA 的介紹較為簡略，提到在某些快速變化的場景下（如快速運動的物體），其適用性可能受限 [23, 28]。對其生物學基礎也表示不確定 [16, 26]。

### 2.4 稀疏編碼 (Sparse Coding)

*   **核心思想：** 輸入數據 `x` 可以由一個「超完備」的基向量（稱為 **字典 W**）的 **稀疏線性組合** 來表示 (`x ≈ Wh`) [3, 5, 6, 29]。
*   **超完備字典：** 與 PCA/FA 不同，稀疏編碼中潛在編碼 `h` 的維度通常 **大於** 觀測數據 `x` 的維度 [3, 29-31]。
*   **稀疏性：** 對於每個輸入 `x`，其對應的編碼 `h` 中的 **大部分元素都應該是零或接近零** [5, 6, 29]。
*   **潛在編碼 (h) 的先驗分布：** 選擇能誘導稀疏性的 **非高斯分布**，例如 Laplace 分布或 Student-t 分布 [2, 3, 5, 6]。這些分布在零點附近有高機率密度，尾部較重 [2]。
*   **參數學習：** 通常包含兩個交替步驟 [3, 6]：
    1.  **推斷潛在編碼 h：** 給定字典 `W` 和輸入 `x`，找到最優的稀疏編碼 `h*`。通常通過最小化一個包含重構誤差項和稀疏懲罰項的目標函數來實現，例如 `argmin_h ||x - Wh||² + λ||h||₁` (公式 13.18) [3, 6, 31]。`||h||₁` 是 L1 範數懲罰，用於誘導稀疏性 [31]。音訊中提到，L1 範數不可微，實際計算時常使用可微的近似函數 [31, 32]。
    2.  **更新字典 W：** 給定稀疏編碼 `h`，更新字典 `W` 以更好地重構數據 [3, 6]。
*   **學習複雜性：** 因推斷 `h` (即使對於給定的 `W`) 通常沒有閉式解 (因為稀疏先驗的非高斯性)，稀疏編碼的學習比 PCA 更複雜 [3, 6]。
*   **學習到的字典基特性：** 通常是 **局部的、定向的、帶通的 (localized, oriented, and bandpass)**，類似於邊緣檢測器或 Gabor 濾波器 [3, 29, 33]。當在自然圖像塊上訓練時，學習到的基向量非常像哺乳動物初级視覺皮層 (V1) 中簡單細胞的感受野 [3, 23, 29, 33]。這提供了稀疏編碼在生物學上的證據支持 [23, 26]。
*   **與 Autoencoder 的關係：** 稀疏編碼可以在 Autoencoder 框架下實現，通過將重構誤差和稀疏懲罰項加入 Autoencoder 的目標函數中進行學習 [23, 30, 32, 34]。音訊中展示了學習到的字典基，並提到可以通過添加空間上的懲罰項 (如 Topographic Sparse Coding) 來鼓勵相似的特徵在學習到的字典中位置相近 [34]。
*   **與機率模型的關係：** 稀疏編碼也可以從機率模型角度解釋，潛在編碼 `h` 具有稀疏先驗（如 Laplace 分布），噪聲通常為高斯 [3, 31]。最小化目標函數的方法在某些近似下與最大化後驗機率 (MAP) 估計是等價的 [31]。

## 3. PCA 的流形解釋 (Manifold Interpretation)

*   **流形學習角度：** 線性因子模型，包括 PCA 和因子分析，可以被理解為學習一個數據流形 [10, 29, 33]。PCA 可以被視為試圖找到一個能夠最好地（在均方誤差意義下）近似數據所在流形的 **低維線性子空間（或仿射子空間）** [10, 29, 35]。如果數據點大致分佈在一個嵌入在高維空間中的、相對「平坦」的低維流形上，PCA 找到的主成分子空間就是對這個流形的一個線性逼近 [10, 29, 35]。
*   **編碼器 (Encoder)：** 將原始數據點 `x` 投影到由前 `d` 個主成分張成的主子空間上，得到低維表示 `h` [10, 29, 35]。定義為 `h = f(x) = W_dᵀ (x - μ)`，其中 `W_d` 包含前 `d` 個主成分向量，`μ` 是數據均值 [10, 29, 35]。
*   **解碼器 (Decoder)：** 從低維表示 `h` 重構回原始數據空間中的點 `x̂` [10, 29, 35]。重構點位於主子空間中 [29, 35]。定義為 `x̂ = g(h) = W_d h + μ` [10, 29, 35]。
*   **重構誤差：** PCA 最小化重構誤差 `E[||x - x̂||²]` [8-10, 33]。這個 **均方重構誤差等於數據協方差矩陣中那些被丟棄的（對應較小特徵值的）特徵值之和** `Σ_{i=d+1}^D λ_i` (公式 13.23) [8-10, 33]。這表明 PCA 通過保留方差最大的方向（對應最大的特徵值）來最小化重構誤差 [8-10]。

## 4. 參數學習方法：期望最大化 (EM) 算法

*   **應用：** 期望最大化 (EM) 算法是一種廣泛用於學習帶有潛變量機率模型參數的迭代算法 [3, 11-14]。
*   **步驟：** EM 算法交替進行兩個步驟 [12, 13]：
    *   **E 步 (Expectation Step)：** 在當前參數下，計算潛變量 `h` 的後驗分布 `p(h|x, θ)` [12, 13]。
    *   **M 步 (Maximization Step)：** 在 E 步計算出的潛變量後驗分布下，更新模型參數 `θ` 以最大化數據的期望完整對數概似 (expected complete log-likelihood) [12, 13]。
*   **原因：** 直接最大化包含潛變量邊緣化的觀測數據概似 `log p(x)` 通常很困難 [12, 13]。EM 算法通過引入潛變量後驗分布，將問題分解為兩個通常更容易處理的步驟 [12, 13]。音訊中提到 EM 算法是一種比較一般化的方法，適用於很多生成模型 [14]。圖示解釋中提到 EM 算法通過 E 和 M 步驟的交替進行，不斷提升觀測數據對數概似的下界，從而最大化概似 [36]。Bishop 的著作被引用來解釋 EM 算法的圖示含義 [25, 36].

## 5. 主要人物 (提及或暗示參與討論)

*   **林遠：** 本次報告主講人，介紹章節內容 [25]。
*   **JON：** 在 ICA 應用於腦磁圖 (MEG) 分析方面被提及為專家 [23, 25]。
*   **Bishop：** 其著作被引用解釋 EM 算法的圖示 [25, 36]。
*   **溫達 (Wendell)：** 在討論稀疏編碼和 Autoencoder 示例時被提及，提供了相關資料或課程 [25, 30, 34]。
*   **Larry：** 在討論 EM 算法時被林遠提及互動 [25, 36]。