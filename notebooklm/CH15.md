---
layout: default
title: Deep Learning 101, 台灣曾經最高最早發起的深度學習社群 @ 83F, 台北101
---

<p align="center">
  <strong>The top private AI Meetup in Taiwan, launched on 2016/11/11 @ 83F, Taipei 101</strong>
</p>
<p align="center">
  <strong>Deep Learning 101, 台灣曾經最高最早發起的深度學習社群 @ 83F, 台北101</strong><br><br>
  AI是條寂寞且惶恐的道路，花俏的收費課程或活動絕不會是條捷徑<br>
  本頁內容為過往實名分享制的讀書會，感謝來自不同公司參與者的支持；如欲移除資訊還請告知。<br>
  Deep Learning 101 只由 TonTon Huang Ph.D. 及其當時任職公司無償贊助場地及茶水點心，無 Co-organizer<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="400">
  </a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/">台灣人工智慧社團 FB</a> |
  <a href="https://www.twman.org/">TonTon Huang Ph.D.</a> |  
  <a href="http://DeepLearning101.TWMAN.ORG">台灣人工智慧社團 網站</a> |
  <a href="https://huggingface.co/DeepLearning101">Hugging Face</a>
</p>
<p align="center">
<a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" ></a>
</p>

# 第十五章 表示學習  - [YouTube](https://www.youtube.com/watch?v=MA52s5dQaGY) - <a href="https://deep-learning-101.github.io/">回上一頁 GitHub Pages</a>

### Representation Learning (2017/07/07)

### 表示學習的核心思想與理想屬性

*   **表示學習**的核心思想是學習數據的良好表示或特徵，以便更容易解決後續的學習任務，例如分類或回歸 [3]。
*   一個好的表示能夠捕捉數據中的**本質結構和有用信息** [3]。不同的表示方式會組織和呈現信息，好的表示能使相關模式更突出，無關信息被抑制，從而簡化後續處理 [3]。例如，阿拉伯數字比羅馬數字更容易進行算術運算；圖像的像素值難以用於物體識別，但將其轉換為邊緣、角點等特徵後則更容易 [3]。
*   理想的表示應捕捉數據的**潛在成因 (latent factors of variation)** [4, 5]。基於潛在成因的表示具有更高的可解釋性、泛化能力和魯棒性 [5]。
*   **解耦表示 (Disentangled Representation)** 是一種理想的表示，其中單個潛在單元對數據變化的單個獨立生成因素敏感 [4-6]。這使得表示具有強可解釋性、易於控制操作，並有利於遷移和泛化到新任務或領域 [5, 6].
*   **分佈式表示 (Distributed Representation)** 是指數據的表示由許多特徵同時激活形成，且每個特徵可參與多個數據表示 [4, 7, 8]。相較於局部表示（如 one-hot 編碼）使用單個單元代表一個概念，分佈式表示的主要優勢在於：
    *   **指數級的表示能力**：n個二值特徵可表示 2^n 個概念 [7, 8]。
    *   **更好的泛化能力**：特徵共享使得模型能利用學到的知識推斷新概念 [7, 8]。
    *   **更豐富的相似性度量**：可通過共享特徵或向量距離衡量相似性 [8]。
*   **平滑性/局部不變性 (Smoothness/Local Invariance)** 指相似輸入應映射到相似表示，或表示對不重要的局部變化（如平移、旋轉）保持不變 [9, 10]。這有助於提高魯棒性、促進泛化並簡化決策邊界 [10]。
*   **層次組織 (Hierarchical Organization)** 假設現實世界概念具有層次結構 [9, 10]。深度網路的多層結構自然體現了這一點，從低層簡單特徵組合到高層抽象特徵 [9, 10]。
*   **稀疏性 (Sparsity)** 假設表示中只有少數特徵活躍 [11]。這有助於提取本質信息、提高可解釋性、計算效率並作為正則化手段避免過擬合 [11]。
*   **流形學習 (Manifold Learning)** 假設高維數據位於低維非線性流形上 [9]。表示學習的目標之一是找到並學習數據在流形上的表示，以捕捉內在結構 [9]。音訊中以圖片平移為例，說明在流形上本質表示可能相同 [9]。

### Autoencoder 及其變體

*   **Autoencoder** 是一種神經網路，訓練目標是使輸出與輸入相似 [12]。它包含將輸入映射到隱藏層（Code Layer）的編碼器（Encoder）和將隱藏層映射回輸出的解碼器（Decoder） [12]。
*   **欠完備自編碼器 (Undercomplete Autoencoder)** 的隱藏層維度比輸入維度小，強迫模型捕捉輸入數據的最顯著特徵，學習有意義的表示 [1, 12]。常用的損失函數是均方誤差 (Mean Squared Error) [12]。
*   **正則化自編碼器 (Regularized Autoencoder)** 通過在損失函數中添加額外的懲罰項來學習有用的表示，即使在隱藏層維度較大（過完備）的情況下 [1]。
*   **稀疏自編碼器 (Sparse Autoencoder)** 通過在隱藏層輸出上施加稀疏性懲罰來鼓勵學習更多的特徵，防止所有神經元都對每個輸入做出反應 [13]。
*   **去噪自編碼器 (Denoising Autoencoder)** 訓練目標是從帶有雜訊的輸入中重構出原始的乾淨數據 [13, 14]。損失函數是比較重構輸出與**原始乾淨輸入**的差異 [13].
*   **收縮自編碼器 (Contractive Autoencoder)** 在損失函數中添加懲罰項，使其表示對輸入的微小變化不敏感，有助於學習數據分佈的結構 [14, 15]。
*   **深度 Autoencoder** 相較於淺層具有更高的表示效率，可能需要更少的數據和訓練時間 [14].
*   **參數化稀疏編碼 (Parametric Sparse Coding, PSC)** 可以看作是一種 Autoencoder 的變體，包含參數化的編碼器和解碼器，用於預測輸出並推斷稀疏的潛在表示（H） [16]。它在物體偵測等非監督學習任務中有應用 [16]。

### 深度學習中的預訓練與遷移學習

*   **貪心逐層無監督預訓練 (Greedy Layer-wise Unsupervised Pre-training)** 是一種早期訓練深度網路的方法 [17]。它逐層獨立地無監督訓練網路，將前一層輸出作為下一層輸入，並固定已訓練層的參數 [17, 18]。這個預訓練過程為後續的監督學習（精調）提供更好的參數初始值 [17]。
*   這種預訓練在早期深度學習中很重要，因為它有助於緩解梯度消失/爆炸等優化困難，找到較好的參數區域 [17-19]。
*   無監督預訓練的有效性取決於多種因素 [19]：
    *   **標註數據稀少但有大量未標註數據**時特別有效，能利用未標註數據學習有用表示 [5, 19]。
    *   可以作為一種**正則化**手段，引導學習過程捕捉輸入分佈結構，提升泛化能力 [18, 19].
    *   有助於解決**深度網路的優化困難** [19].
    *   **任務相關性**很重要：預訓練學習到的表示對最終監督任務有用時效果更好 [19].
*   其有效性**並非總是得到保證**，可能因任務不相關或現代優化技術的發展而效果有限 [19].
*   **領域自適應 (Domain Adaptation)** 是遷移學習的一種特殊情況，通常假設任務相同但輸入數據分佈不同 [4]。
*   遷移學習的一種常見方法是利用在大規模數據集上預訓練好的模型，作為特徵提取器或進行**精調 (Fine-tuning)** 應用於新任務 [4, 20].
*   專家建議在實際應用中**不要從零開始訓練極深的網路** ("don't be a hero")，而是使用已有的、在大型數據集上預訓練好的模型，並根據自己的數據進行精調 [20]。
*   精調策略通常取決於你的目標數據集大小及其與預訓練數據集的相似性 [20, 21]：
    *   **數據小且相似**：將預訓練模型作為固定的特徵提取器，只訓練替換掉的最後一層分類器 [20].
    *   **數據大且相似**：替換並精調預訓練模型的最後幾層 [20].
    *   **數據小但不同**：替換最後幾層，並考慮在前面層加入額外的分類器（如 SVM）來避免只學到過於特定的特徵 [20, 21]。
    *   **數據大但不同**：替換並精調較多層 [21].
*   精調時，可以對不同的層使用不同的學習率，例如前面層用較小的學習率以保留預訓練學到的通用特徵 [20]。
*   **層次遷移 (Layer Transfer)** 是指複製預訓練模型中的部分層到新模型的初始化中 [22]. 在圖像任務中，通常認為前面幾層學習通用特徵（如邊緣、形狀），而後面層學習更具體的高級特徵 [22]。因此，在圖像領域常複製前面幾層 [22]，但在語音領域可能恰好相反 [22]。實際應用中，常使用的可能是接近輸出層（softmax前）的表示 [22].

### 實際應用與挑戰

*   **Autoencoder** 在圖像和視訊的非監督學習、物體偵測等方面有應用 [16, 23].
*   **深度神經網路 (DNN)** 在廣告推薦系統中有實際應用 [23-25]。例如，在行動應用程式 (APP) 通知推薦中，利用多種用戶和設備特徵（如 CPU 核數、電量、網路狀態、情境等）來預測用戶點擊通知的可能性 [23, 24]。這能顯著提升點擊率及相關收益，同時減少對不感興趣用戶的打擾 [23, 25]。
*   一種創新的應用是將可執行文件或網路流量等二進位數據轉換成圖像，然後使用 CNN 進行分析，例如用於**病毒檢測** [2, 24, 26, 27]。這種方法可能可以利用圖像的視覺模式來識別病毒家族，**甚至可能用肉眼觀察**，從而避免複雜的特徵工程和逆向工程 [2, 26, 28]。這對於識別未知病毒可能特別有用，可以利用在圖像（如 ImageNet）上預訓練的模型（如 Inception V3），即使原始數據（二進位）與圖像無關，像素的排列仍能反映其結構 [27-29]。
*   將深度學習模型應用於實際問題面臨諸多挑戰 [23]：
    *   **模型大小與部署**：複雜模型參數多，檔案大（幾百 MB），部署到資源受限的行動裝置或需要快速響應的後台服務時會導致計算延遲過高 [23, 25, 30]。需要在模型性能和大小之間權衡 [23, 25, 30]。
    *   **計算資源與訓練時間**：訓練大型、高性能的模型需要強大的硬體（GPU、記憶體）和長時間的訓練 [27, 29]。
    *   **實務與學術的落差**：學術研究可能更關注模型性能和創新，而業界則必須考慮實際部署的成本、效率、用戶體驗（如響應延遲）和收益 [24, 28, 30]。有時在特定應用場景下，相對簡單的模型（如 Random Forest）可能因其訓練和預測速度快而更實用，尤其是在對延遲敏感的應用中 [30]。

總結來說，這些資料涵蓋了表示學習的核心概念、不同 Autoencoder 變體的機制、深度學習早期重要的無監督預訓練方法、以及現代應用中關鍵的遷移學習策略。同時，也通過實際案例探討了深度學習在推薦系統和病毒檢測等領域的應用，並揭示了將理論模型轉化為可實際部署的系統所面臨的顯著挑戰，強調了在性能、資源和實用性之間權衡的重要性。