---
layout: default
title: Deep Learning 101, 台灣曾經最高最早發起的深度學習社群 @ 83F, 台北101
---

<p align="center">
  <strong>The top private AI Meetup in Taiwan, launched on 2016/11/11 @ 83F, Taipei 101</strong>
</p>
<p align="center">
  <strong>Deep Learning 101, 台灣曾經最高最早發起的深度學習社群 @ 83F, 台北101</strong><br><br>
  AI是條寂寞且惶恐的道路，花俏的收費課程或活動絕不會是條捷徑<br>
  本頁內容為過往實名分享制的讀書會，感謝來自不同公司參與者的支持；如欲移除資訊還請告知。<br>
  Deep Learning 101 只由 TonTon Huang Ph.D. 及其當時任職公司無償贊助場地及茶水點心，無 Co-organizer<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="400">
  </a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/">台灣人工智慧社團 FB</a> |
  <a href="https://www.twman.org/">TonTon Huang Ph.D.</a> |  
  <a href="https://deep-learning-101.github.io/">回 GitHub Pages</a> |
  <a href="http://DeepLearning101.TWMAN.ORG">台灣人工智慧社團 網站</a> |
  <a href="https://huggingface.co/DeepLearning101">Hugging Face</a>
</p>
<p align="center">
<a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" ></a>
</p>

# 表示學習重點摘要 - [YouTube](https://www.youtube.com/watch?v=n0rBS3sAqI0)
### Structured Probabilistic Models @ Deep Learning Book Chapter 16 (2017/10/27)

# 深度學習中的結構化機率模型與圖模型：重點彙整

## 1. 核心概念與定義

*   **結構化機率模型 (Structured Probabilistic Model)**，也稱 **圖模型 (Graphical Model)**，其核心思想是使用圖 (graph) 來描述**機率分佈中多個隨機變數之間的直接相互作用** [1-9]。
*   圖中的**節點代表隨機變數** [1, 2, 4-6]，而**邊則表示這些變數之間的直接依賴或相互作用** [1, 2, 4-9]。
*   圖模型之所以得名，是因為其結構是由圖來定義和可視化的 [4, 6]。

## 2. 非結構化建模的挑戰與結構化模型的優勢

*   **非結構化機率建模**在處理高維數據時面臨嚴重的計算挑戰，即**維度災難** [2, 7, 9-14]。
*   維度災難體現在：
    *   **內存開銷巨大**：儲存完整的聯合機率分佈表需要指數級的記憶體 [2, 7, 10, 11]。
    *   **統計效率低下**：參數數量龐大，需要指數級增長的訓練數據量 [7, 10, 11, 13]。
    *   **運行時間成本高昂**：計算推斷任務（如邊緣或條件機率）和從分佈中採樣的計算量呈指數級增長 [2, 7, 9-11, 13, 14]。
*   **結構化機率模型**通過顯式描述變數子集間（通常是稀疏的）相互作用，用較少參數對複雜依賴關係建模 [7, 9, 11, 13, 15, 16]，顯著**降低表示、學習和推斷的成本** [1, 11, 13, 17]。
*   結構化模型的其他優勢包括：允許**融入先驗知識** [11, 13]、提供**模組化和可解釋性** [11, 13]、為開發**高效算法**提供基礎 [11, 13]。

## 3. 圖模型的種類與表示

圖模型主要分為兩大類：有向圖模型和無向圖模型 [7, 8, 18, 19]。

*   **有向圖模型 (Directed Graphical Model / Bayesian Network / Belief Network)**
    *   使用**有向無環圖 (DAG)** 表示變數間的條件依賴關係或因果關係 [3, 8, 17, 19]。
    *   **聯合機率分佈分解**：分解為每個變數在其父節點條件下的局部條件機率分佈的乘積： **p(x) = Π_i p(x_i | Pa_G(x_i))** [2, 15, 17, 19]。
    *   **優勢**：大大**減少了所需的參數數量和計算複雜度** [2, 15, 17, 19]。

*   **無向圖模型 (Undirected Graphical Model / Markov Random Field / Markov Network)**
    *   使用**無向圖**表示變數間的對稱相互作用或關聯，**不指定因果方向** [2, 3, 8, 18-21]。
    *   **適用場景**：適合描述變數間依賴是對稱的情況，如物理系統或圖像像素鄰域的關聯 [2, 18, 19, 21]。
    *   **團 (clique)**：圖中任意兩節點間都有邊連接的節點子集 [2, 3, 19-22]。
    *   **因子 (factor) / 勢函數 (potential function) φ(C)**：定義在圖的團 C 上的非負函數 [2, 3, 19-22]。
    *   **聯合機率定義**：未歸一化機率 p̃(x) 定義為所有團上因子的乘積： **p̃(x) = Π_{C∈G} φ(C)** [2, 19-22]。
    *   **配分函數 (partition function) Z**：用於歸一化未歸一化機率的常數，**Z = Σ_x p̃(x)** [2, 3, 19, 22]。
    *   **計算難題**：配分函數 Z 的計算通常很困難，因需要對所有狀態求和/積分，呈指數級增長 [2, 19, 22, 23]。這使無向模型的**精確學習和推斷具有挑戰性** [2, 19, 22, 23]。

*   **基於能量的模型 (Energy-Based Model, EBM)**
    *   一種無向圖模型框架，未歸一化機率與**能量函數 E(x) 的負指數成正比**： **p̃(x) = exp(-E(x))** [2, 3, 17, 19, 24]。
    *   學習目標是調整 E(x)，使**期望狀態能量低（機率高），非期望狀態能量高（機率低）** [2, 17, 19, 24]。
    *   玻爾茲曼機 (Boltzmann Machine) 是一種早期 EBM [2, 3, 18, 19, 24, 25]。專家乘積 (product of experts) 是另一種 EBM [3, 18, 24, 26]。

*   **因子圖 (Factor Graph)**
    *   一種二分圖，通過顯式引入因子節點更精確表示因式分解結構，有助於消息傳遞算法 [3, 18, 19, 27, 28]。

## 4. 條件獨立性判斷：分離與 d-分離

*   **無向模型中的分離 (Separation)**：在給定集合 S 時，若 A 到 B 的所有路徑被 S 中節點阻斷，則 A 和 B 在給定 S 時條件獨立 [3, 27, 29]。
*   **有向模型中的 d-分離 (d-Separation)**：判斷條件獨立性時考慮邊方向和「對撞」結構 [3, 17, 27, 29]。
    *   **對撞 (collider) / V-結構 (i → m ← j)**：節點 m 同時是 i 和 j 的子節點 [3, 17, 27, 29]。
    *   特殊規則：**對撞節點 m 若未被觀察，則路徑阻斷；若 m 或其後代被觀察，路徑則暢通**，可能使原本獨立的父節點變得相關 [17, 27, 29, 30]。

## 5. 圖模型間的轉換

*   **有向到無向 (Moralization)**：處理「不道德結構」（無直接連接的父節點指向同一個子節點） [3, 27, 29]。道德化過程包括連接共同子節點的父節點並去掉箭頭 [3, 27, 29]，得到道德圖 [27]。
*   **無向到有向**：通常更複雜，需定向邊而不引入新的有向圖獨立性 [27, 29]。弦圖 (chordal graph) 轉換較容易，通過弦化 (triangulation) 實現 [27, 29]。

## 6. 從圖模型中採樣

*   **有向模型**：通常使用**原始採樣 (Ancestral Sampling)**，按拓撲排序從條件分佈依次採樣 [3, 18, 31, 32]。
*   **無向模型**：通常使用**近似採樣**，因精確採樣困難 [3, 18, 31]。**Gibbs 採樣**是一種 MCMC 方法，迭代地從每個變數在給定其他變數下的條件分佈中採樣 [3, 18, 31-33]。

## 7. 學習依賴關係與潛變數

*   **潛變數 (Hidden Variables) h**：模型中未觀察到的變數，用於建模可見變數 v 間的複雜依賴關係、學習抽象表示或簡化計算 [3, 16, 18, 31, 34-36]。
*   **結構學習 (Structure Learning)**：從數據中學習圖模型的圖結構 [3, 18, 31, 33]。深度學習通常不直接學習稀疏連接，而是使用固定結構和正則化等間接方式 [18, 31]。

## 8. 推斷與近似推斷

*   **推斷 (Inference)**：計算未觀察變數（特別是潛變數 h）在給定觀察變數 v 下的**後驗機率分佈 p(h|v)** 或其期望值 [2, 3, 14, 18, 23, 34, 35, 37, 38]。
*   **精確推斷 (Exact Inference)**：對於許多複雜模型是**難以處理的 (intractable)** (#P-hard)，因計算歸一化常數（分母 p(v)）需對所有潛變數配置進行指數級求和或積分 [2, 3, 23, 34, 35, 37]。
*   **近似推斷 (Approximate Inference)**：當精確推斷不可行時使用，旨在尋找後驗分佈的良好近似，例如**置信傳播 (Belief Propagation)** [3, 16, 18, 26, 36]、變分推斷 (Variational Inference) [35, 37]。

## 9. 實例：受限玻爾茲曼機 (Restricted Boltzmann Machine, RBM)

*   **結構**：一種無向圖模型 (EBM)，有**一層可見單元 v 和一層隱藏單元 h**，形成**二分圖結構**（層內無連接） [3, 18, 25, 34, 39]。
*   **能量函數**：通常為 **E(v,h) = -b^T v - c^T h - v^T W h** [3, 25, 34, 39]。
*   **計算優勢（源於二分圖結構）**：
    *   **條件獨立性**：給定一層（v 或 h），另一層的單元是條件獨立的 [3, 34, 39]。
    *   **高效 Gibbs 採樣**：可並行對一層單元進行塊 Gibbs 採樣 [3, 34, 39]。
    *   **易於計算的梯度** [3, 34, 39]。
*   用於學習數據的表示 E_{h~p(h|v)}[h] [3, 34, 40]。早期曾用於深度學習模型的預訓練或初始化，但目前較少使用 [18, 34, 41]。