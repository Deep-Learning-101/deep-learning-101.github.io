---
layout: default
title: Deep Learning 101, 台灣曾經最高最早發起的深度學習社群 @ 83F, 台北101
---

<p align="center">
  <strong>The top private AI Meetup in Taiwan, launched on 2016/11/11 @ 83F, Taipei 101</strong>
</p>
<p align="center">
  <strong>Deep Learning 101, 台灣曾經最高最早發起的深度學習社群 @ 83F, 台北101</strong><br><br>
  AI是條寂寞且惶恐的道路，花俏的收費課程或活動絕不會是條捷徑<br>
  本頁內容為過往實名分享制的讀書會，感謝來自不同公司參與者的支持；如欲移除資訊還請告知。<br>
  Deep Learning 101 只由 TonTon Huang Ph.D. 及其當時任職公司無償贊助場地及茶水點心，無 Co-organizer<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="400">
  </a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/">台灣人工智慧社團 FB</a> |
  <a href="https://www.twman.org/">TonTon Huang Ph.D.</a> |  
  <a href="https://deep-learning-101.github.io/">回 GitHub Pages</a> |
  <a href="http://DeepLearning101.TWMAN.ORG">台灣人工智慧社團 網站</a> |
  <a href="https://huggingface.co/DeepLearning101">Hugging Face</a>
</p>
<p align="center">
<a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" ></a>
</p>

# 表示學習重點摘要 - [YouTube](https://www.youtube.com/watch?v=yw1JDopTuwo)
### Confronting the Partition Function @ Deep Learning Book Chapter 18 (2018/01/12)

# 直面配分函數 (Confronting the Partition Function)

## 核心挑戰：配分函數 Z(θ)

*   **定義與重要性：** 配分函數 Z(θ) 是一個歸一化常數，用於將未歸一化的機率分佈 p̃(x;θ) 轉換為一個有效的機率分佈 p(x;θ) [1-3]。轉換公式為 p(x;θ) = p̃(x;θ) / Z(θ) [1-3]。Z(θ) 的數學形式是 p̃(x;θ) 在所有可能的狀態 x 上的總和（離散變數）或積分（連續變數）[1-3]。確保機率總和/積分為 1 是機率分佈有效性的關鍵 [1-3].
*   **帶來的挑戰：** 對於許多有趣的機率模型，特別是無向圖模型或具有複雜依賴結構及高維狀態空間的深度學習模型，計算 Z(θ) 所需的總和或積分在計算上是**難以處理的 (intractable)** [1-3]。這使得直接計算歸一化機率 p(x;θ) 或其對數概似變得非常困難 [1-3]。無向圖模型（例如疾病傳播、NLP 中文字的非定向關聯性）使用潛在函數 (potential function) 表示關係強度更自然，但這常引入難以計算的 Z [4-8]。

## 最大概似學習中的梯度計算問題

*   **對數概似梯度分解：** 對數概似 log p(x;θ) 對參數 θ 的梯度可以分解為兩項：∇_θ log p(x;θ) = ∇_θ log p̃(x;θ) - ∇_θ log Z(θ) [9, 10]。
*   **正相 (Positive Phase)：** ∇_θ log p̃(x;θ) [9, 10]。通常相對容易計算 [9, 10]。傾向於增加訓練數據點 x 的未歸一化機率（或降低能量函數值），將機率質量**拉向訓練數據** [4, 9, 10]。
*   **負相 (Negative Phase)：** - ∇_θ log Z(θ) [9, 10]。等於 - E_{x'~p(x';θ)}[∇_θ log p̃(x';θ)] [9, 10]。是在模型自身分佈 p(x;θ) 下的期望 [9, 10]。傾向於減少模型生成樣本（「幻想粒子」）的未歸一化機率（或提高能量函數值），將機率質量**推離模型認為高機率的區域** [4, 9-11]。
*   **負相的計算困難：** 計算負相需要從模型自身的分佈 p(x;θ) 中採樣大量的樣本，這通常非常困難 [11-14]。

## 解決負相計算困難的技術 (訓練階段)

主要目標是繞過或近似計算負相或配分函數本身。

### 1. 基於 MCMC 採樣估計負相

*   **朴素 MCMC 方法：** 每次計算梯度時，從隨機狀態開始運行 MCMC 鏈，並等待鏈「混合」達到平穩分佈後採樣 [11-15]。**計算代價極高**（混合時間長，需要數百甚至數千步），實際中不可行 [11-17]。
*   **對比散度 (Contrastive Divergence, CD-k)：** [11, 13, 14, 16, 18-20]
    *   解決朴素 MCMC 效率問題 [12, 13]。
    *   原理：每個梯度步驟從訓練數據樣本初始化 MCMC 鏈，**只運行少量 k 步** (k=1-20) [13, 14, 16, 18, 20]。用鏈結束時樣本估計負相 [13, 14, 18]。
    *   優點：計算效率高，訓練快 [13, 14, 18]。對淺層模型 (如 RBM) 有效 [11, 16, 20]。
    *   缺點：**有偏估計** (不收斂到真實 ML) [13, 14, 18]。易形成**虛假模態** (模型在遠離數據區塊有高機率)，因採樣始於數據點且步數少，探索不足 [13, 14, 16, 18, 19]。對於隱藏單元多的深度模型效果有限 [16, 19, 20]. CD 訓練的模型類似於訓練自編碼器，學習區分真實數據與輕微擾動的「幻想粒子」 [5, 21].
*   **隨機最大概似 (Stochastic Maximum Likelihood, SML) / 持續性對比散度 (Persistent Contrastive Divergence, PCD-k)：** [11, 16, 19, 21-24]
    *   另一種 MCMC 策略 [18, 19]。
    *   原理：MCMC 鏈**在整個訓練過程持續運行** [11, 19, 21-23]。當前步驟的鏈狀態繼承自上一步結束時的狀態 [19, 21-23]。
    *   優點：**偏差比 CD 小** [19, 21, 22]。更好地探索模型分佈空間，**不易形成虛假模態** [19, 21, 22]。更適合訓練**深度模型**，能為隱藏單元提供更好的持續初始化 [16, 19-23]。在學習率小的情況下，負相估計更接近真實梯度 [19, 21, 22]。
    *   潛在問題：學習率過大或 MCMC 步數 k 過少時，鏈可能無法跟上模型變化，影響收斂或探索深度 [16, 23, 24].

### 2. 偽概似 (Pseudolikelihood)

*   **基本思想：** 避免計算完整 Z(θ) [22, 25]。最大化數據中每個變數 x_i 在給定所有其他變數 x_{-i} 條件下的對數機率之和 Σ_i log p(x_i | x_{-i}) [22, 25-27]。
*   **如何避免 Z(θ)：** 條件機率 p(x_i | x_{-i}) 的分母僅涉及對單個變數 x_i 的求和（局部歸一化），計算量遠小於對所有變數的聯合求和（完整 Z） [16, 22, 25-28]。計算量從指數級 O(k^n) 降至線性 O(n*k) [16, 27, 28]。
*   **統計特性與局限性：** 估計是**漸近一致的** (大數據下收斂到真實參數) [22, 25, 29]。有限數據下可能不如最大概似 [22, 25, 29]。對於需要完整聯合分佈的任務 (密度估計、採樣) 表現較差 [25, 29]。**不兼容**只提供 p̃(x) 下界的方法 (如變分推斷) [25, 29]。廣義偽概似 (GPLE) 權衡計算複雜度和偏差 [26]。

### 3. 得分匹配 (Score Matching) 與比率匹配 (Ratio Matching)

*   **得分匹配 (Score Matching)：** [16, 28-31] (Hyvärinen, 2005b [11, 29, 30])
    *   「得分」是對數密度關於**輸入 x** 的梯度 ∇_x log p(x) [29, 31]。
    *   目標：最小化模型得分 ∇_x log p_model(x;θ) 與數據得分 ∇_x log p_data(x) 的平方差期望 [16, 29, 31]。
    *   如何避免 Z(θ)：因為 Z(θ) 不依賴於 x，∇_x log Z(θ) = 0 [16, 29, 31]。目標函數只涉及 ∇_x log p̃(x;θ) [16, 29, 31]。
    *   限制：**不適用於離散數據** [16, 29]，需要易於計算的二階導數 [16, 29]。
*   **比率匹配 (Ratio Matching)：** [16, 29-31] (Hyvärinen, 2007b [11, 29, 30])
    *   擴展得分匹配到**離散數據** (二值數據) [16, 29, 31]。
    *   原理：最小化一個目標函數，涉及數據點 x 與其一位翻轉點 f(x,j) 處**未歸一化機率的比率** p̃(x)/p̃(f(x,j)) [16, 29, 31]。Z(θ) 在比率中被消去 [16, 31]。

### 4. 去噪得分匹配 (Denoising Score Matching)

*   **問題：** 標準得分匹配在有限數據上易**過擬合**經驗分佈 [16, 32]。
*   **解決：** 匹配一個**平滑後數據分佈** p_smoothed(x) 的得分 [16, 32]。p_smoothed(x) 是通過對數據 p_data(y) 應用已知損壞過程 q(x|y) (如加噪) 得到的 [16, 32]。有助於學習更平滑、泛化的密度模型 [16, 32]。
*   **關聯：** 某些**自編碼器訓練算法** (如去噪自編碼器) 與得分匹配或去噪得分匹配等價，隱含地學習了得分函數，繞過 Z(θ) [5, 16, 21, 32, 33]。

### 5. 噪聲對比估計 (Noise-Contrastive Estimation, NCE)

*   **核心思想：** [16, 28, 30, 33-35] (Gutmann and Hyvärinen, 2010 [11, 30, 33])
    *   將**無監督密度估計轉化為監督式二元分類** [16, 33]。
    *   將 log p_model(x) 表示為 log p̃_model(x;θ) + c，其中 c = -log Z(θ) 作為**可學習參數** [16, 33]。
*   **原理：** 引入易於處理的**噪聲分佈 p_noise(x)** [16, 33, 34]。訓練一個邏輯回歸分類器來區分來自真實數據 (假設來自 p_model) 和來自 p_noise 的樣本 [16, 33]。分類器的最大概似可以一致地估計 θ 和 c [16, 33]。分類器輸出與 log p̃_model(x) - log p_noise(x) 相關 [16, 33]。
*   **對 p_noise 的要求：** 易於採樣和密度易於估計 [34]。選擇與 p_model 有一定重疊的 p_noise 能提高效率 [34]。

## 估計配分函數 Z(θ) 本身的重要性與方法

*   **重要性：** 即使訓練時避免計算 Z(θ)，估計它對於以下情況仍然重要且有用 [34, 36-39]：
    *   **模型評估：** 計算模型在測試數據上的真實歸一化對數概似 log p(x;θ) [34, 36-39]。
    *   **模型比較：** 比較不同模型性能時，常比較其平均對數概似，需要 Z 值或其比率 [34, 36-39]。
    *   **監控訓練進度：** 追踪 log Z(θ) 變化有助於理解訓練狀態 [36-38]。
    *   某些採樣方法可能需要 Z(θ) [36-38]。

*   **估計方法：**
    *   **重要性採樣 (Importance Sampling)：** 從已知 Z_0 的提議分佈 p_0 採樣來估計目標分佈 p_1 的 Z_1 [34, 37]。Z_1/Z_0 估計是 (1/K) Σ [p̃_1/p̃_0] [34, 37]。若 p_0 與 p_1 相差大，方差大 [34, 37]。
    *   **退火重要性採樣 (Annealed Importance Sampling, AIS)：** [30, 34-41] (Neal, 2001 [11, 30, 34, 37])
        *   解決重要性採樣中分佈差異大的問題 [34, 37]。
        *   原理：引入一系列中間分佈平滑地從 p_0 過渡到 p_1 [34-40]。通過估計相鄰中間分佈之間的配分函數比率並將它們連乘來估計 Z_1/Z_0 [34-40]。常用於估計無向模型的配分函數 [34, 37]。
    *   **橋式採樣 (Bridge Sampling)：** [11, 30, 34, 37, 41] (Bennett, 1976 [11, 30, 37])
        *   依賴於單個「橋」分佈 p* 在 p_0 和 p_1 之間插值 [34, 37, 41]。通過兩個重要性權重期望的比率估計 Z_1/Z_0 [34, 37, 41]。
    *   **鏈接重要性採樣 (Chained Importance Sampling)：** [11, 30, 34, 37] (Neal, 2005 [11, 30, 37]) 結合 AIS 和橋式採樣 [34, 37]。
    *   **訓練期間估計：** 可設計方法在訓練過程中追踪 Z，結合多種技術 (如 Desjardins et al., 2011 訓練 RBM 的工作) [30, 34, 37]。

## 重要人物與時間線

*   **1976：** Bennett 提出橋式採樣 [11, 30, 34, 37, 42]。
*   **2000：** Hinton 提出對比散度 (CD) [11, 14, 30, 42]。
*   **2001：** Neal 提出退火重要性採樣 (AIS) [11, 30, 34, 37, 40, 42]。
*   **2005：** Neal 提出鏈接重要性採樣 [30, 34, 37, 42]；Hyvärinen 提出得分匹配 [11, 29, 30, 42]。
*   **2007：** Hyvärinen 提出比率匹配 [11, 29, 30, 42]。
*   **2010：** Hinton 討論 CD-k；Gutmann & Hyvärinen 提出噪聲對比估計 (NCE) [11, 30, 33, 42]。
*   **2011：** Desjardins 等人研究訓練 RBM 期間估計 Z 的方法 [30, 34, 37, 42]。
*   **2018-01-12：** 「直面配分函數」相關討論或講座 [3, 30]。
*   **主要人物：** Hinton, Hyvärinen, Gutmann, Neal, Bennett, Desjardins [42].
