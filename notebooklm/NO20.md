---
layout: default
title: Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101
---

<p align="center">
  <strong>Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101</strong>  
</p>
<p align="center">
  AI是一條孤獨且充滿惶恐及未知的旅程，花俏絢麗的收費課程或活動絕非通往成功的捷徑。<br>
  衷心感謝當時來自不同單位的AI同好參與者實名分享的寶貴經驗；如欲移除資訊還請告知。<br>
  由 <a href="https://www.twman.org/" target="_blank">TonTon Huang Ph.D.</a> 發起，及其當時任職公司(台灣雪豹科技)無償贊助場地及茶水點心。<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="180"></a>
    <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 100px !important;width: 180px !important;" ></a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101" target="_blank">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/" target="_blank">Facebook</a> |
  <a href="https://deep-learning-101.github.io/"> 回 GitHub Pages</a> |
  <a href="http://DeepLearning101.TWMAN.ORG" target="_blank">網站</a> |
  <a href="https://huggingface.co/DeepLearning101" target="_blank">Hugging Face Space</a>
</p>


# VAE: A generative model for 2D anime character faces

<a href="https://www.youtube.com/watch?v=DF9GMPU8wPU" target="_blank" rel="noopener noreferrer"><i class="fab fa-youtube mr-1"></i>2018/06/08, Nat, Boris, Alice, Ian (蠻牛小隊), VAE: A generative model for 2D anime character faces</a><br>

# Variational Autoencoder (VAE) 於動漫角色影像生成之研究筆記與新手入門指南

很高興能與大家分享我們在 Variational Autoencoder (VAE) 應用於動漫角色影像生成方面的一些研究心得與實驗發現。對於剛接觸這個領域的新手研究員或學生而言，理解 VAE 的運作原理及其與傳統 Autoencoder (AE) 的差異是進入生成模型世界的重要第一步。

## 從 Autoencoder (AE) 談起：降維與特徵提取的基本功

在深入 VAE 之前，我們先快速回顧一下傳統的 **Autoencoder (AE)**。對於新手來說，可以把 AE 想成是一個數據壓縮與解壓縮的過程。它的基本原理是透過一個稱為 Encoder 的神經網路，將高維度的輸入數據（例如一張圖片）壓縮成一個低維度的向量，這個向量位於一個我們稱為 **Latent Space (潛在空間)** 的地方，這個向量就是 **Latent Vector (潛在向量)**。然後，再透過另一個稱為 Decoder 的神經網路，將這個潛在向量還原回原始輸入數據的樣子。

AE 的核心目標很單純：訓練 Encoder 和 Decoder，使得還原出來的數據與原始輸入數據盡可能相似。這個過程主要用於資料的降維和特徵提取。對於新手來說，理解 AE 的主要應用就是學習數據的精簡表示。

然而，傳統 AE 在生成新數據方面有一個明顯的限制。這是因為 AE 將每個輸入數據點映射到潛在空間中的一個「點」。在訓練完成後，潛在空間中的數據會形成離散的 **Clusters (群集)**，相似的數據點會聚集在一起。問題在於，這些群集之間可能存在大量的「空白區域」，這些區域在訓練數據中沒有對應的點。如果我們嘗試在這些空白區域進行採樣並透過 Decoder 進行解碼，結果往往是模糊不清甚至「爛掉」的圖像，無法生成有意義的新數據。

## Variational Autoencoder (VAE)：邁向生成模型的第一步

正因為傳統 AE 在生成方面的局限性，我們引入了 **Variational Autoencoder (VAE)**。VAE 可以視為 AE 的一個重要改良或升級版。這裡的關鍵點在於：VAE 不再將輸入圖像編碼為潛在空間中的一個單一的、固定的「點」，而是將其編碼為潛在空間中的一個「區域」或一個「分佈」，通常我們假定這是一個高斯分佈。

對於新手而言，理解 VAE 最重要的概念就是這個「分佈」的概念。Encoder 輸出不再是直接的潛在向量，而是描述這個分佈的參數，具體來說，是一個 **Mean Vector (μ，均值向量)** 和一個 **Standard Deviation Vector (σ，標準差向量)**。Decoder 不是接收 Encoder 直接輸出的向量，而是從這個由 μ 和 σ 定義的高斯分佈中 **Sampling (採樣)** 得到一個潛在向量，再進行解碼。

VAE 之所以具有生成能力，就是因為引入了這個「分佈」的概念。即使在訓練數據點之間原本是「空白」的區域，VAE 透過學習到的連續分佈，可以在這些區域進行採樣，並解碼出合理且與附近訓練數據相關的圖像。這使得 VAE 能夠生成新的、未曾在訓練數據中出現過但具有相似特徵的圖像。

在 VAE 的潛在空間中，相似的數據分佈會形成一個更為集中且連續的區域，不像傳統 AE 那樣形成分散的離散群集。雖然 VAE 的潛在空間會形成一個巨大的、混合了許多不同數據群集的連續區域，但它同時仍然能夠保留各個群集的特徵，這是一種同時追求資料分散（形成分佈）與聚集（保持群集特性）的「對抗」概念，這也使得 VAE 的訓練難度相較於傳統 AE 更高。

## VAE 的模型架構與訓練要點

VAE 的模型架構可以概括為：

1.  **Encoder Network**：接收輸入圖像，通常使用 **Convolution (卷積)** 層來提取圖像特徵。
2.  **輸出分佈參數**：Encoder 輸出兩個向量：Mean Vector (μ) 和 Standard Deviation Vector (σ)。
3.  **Sampling (採樣)**：從以 μ 為均值、σ 為標準差的高斯分佈中採樣得到一個 Latent Vector。
4.  **Decoder Network**：接收採樣得到的 Latent Vector，通常使用 **Deconvolution (反卷積) / Transposed Convolution** 層來逐步重建圖像。
5.  **輸出生成圖像**：Decoder 輸出與原始輸入圖像具有相同尺寸的生成圖像。

對於新手來說，這裡有一個關鍵的技術點需要理解：**Reparameterization Trick (重參數化技巧)**。為什麼需要這個技巧？因為從一個分佈中「隨機抽取」樣本這個操作本身是不可微分的，這會導致誤差無法透過標準的 **Backpropagation (反向傳播)** 算法從 Decoder 回傳到 Encoder，也就無法訓練 Encoder 的參數。

Reparameterization Trick 巧妙地解決了這個問題。它不是直接從由 μ 和 σ 定義的目標分佈中採樣，而是從一個簡單的標準高斯分佈（均值為 0，標準差為 1）中採樣一個隨機向量 ε (epsilon)。然後，將這個隨機向量 ε 透過一個可微分的線性變換來得到目標分佈的採樣向量：`latent_vector = μ + σ * ε`。這樣一來，隨機性被轉移到 ε 上，而從 μ 和 σ 得到 latent_vector 的過程變成了可微分的線性運算，從而實現了誤差的反向傳播。

VAE 的訓練目標是最小化一個 **Loss Function (損失函數)**，這個函數通常包含兩部分：

1.  **Reconstruction Loss (重建損失)**：度量 Decoder 生成的圖像與原始輸入圖像之間的相似度，例如使用 **L2 Loss (均方誤差)**。這部分損失促使 VAE 盡可能地還原原始圖像。
2.  **KL Divergence (Kullback-Leibler Divergence)**：度量 Encoder 輸出的潛在分佈（由 μ 和 σ 定義）與一個預設的先驗分佈（通常是標準高斯分佈）之間的差異。這部分損失促使 VAE 的潛在空間分佈趨向於一個光滑且連續的標準高斯分佈，這對於生成新的、合理的數據至關重要。

最小化這個包含重建損失和 KL 散度的聯合損失函數，就是訓練 VAE 的 **Optimize (優化)** 過程。

## 動漫角色影像生成實驗與發現

在我們這次針對動漫角色頭部圖像的 VAE 實驗中，我們收集了約 4000 張動漫角色的頭部圖片作為訓練數據集，並使用了 Titan 顯卡進行了 1000 個世代的訓練，潛在空間維度設定為 50 維（作為對照的傳統 AE 使用了 256 維），圖像解析度為 128x128 像素。

實驗結果印證了 VAE 在潛在空間分佈和生成能力方面的優勢：

1.  **潛在空間分佈**：與傳統 AE 在潛在空間形成分散且獨立的數據群集不同，我們訓練的 VAE 形成了一個巨大的、內部混雜著不同數據類別的連續區域。雖然類別之間的界限不像傳統 AE 那樣分明，但這種分佈使得潛在空間中的中間區域也能夠解碼出有意義的圖像。實驗中計算的潛在向量距離顯示，相似圖像的距離較近（約 1.15 到 1.75），不同人物圖像距離較遠（2.0 以上），這表明 VAE 的潛在空間保留了數據的相似性結構。有趣的是，即使背景不同，相似圖像的潛在距離也可能很近。
2.  **圖像特徵操控**：透過在原始圖像編碼得到的潛在向量上加入微小的擾動（一個小的 epsilon vector），然後進行解碼，我們觀察到生成的圖像會發生平滑連續的變化。我們發現，動漫角色圖像中最容易受到潛在向量微小改動影響的特徵是眼睛的顏色和表情，以及髮型。這表明 VAE 在潛在空間中捕捉並一定程度上解耦了這些視覺特徵。例如，調整潛在向量可以使角色的眼睛顏色變化，嘴巴張開，或者髮型變化（有時會傾向於變成捲髮，這是一個有趣的現象，可能與訓練數據特性有關）。
3.  **角色融合**：我們嘗試將兩個不同角色的潛在向量進行線性組合（例如加權平均），再透過 Decoder 解碼。結果顯示，可以生成融合了這兩個角色部分特徵的新角色圖像，例如繼承其中一個角色的眼睛和另一個角色的劉海或髮型。這種融合過程有時效果顯著，有時則不太理想，生成的圖像可能會出現模糊，感覺上更像是將不同角色的臉部元素「拼接」在一起，而非平滑自然的融合。
4.  **連續生成比較**：與傳統 AE 相比，VAE 在生成連續變化的圖像方面表現出色。當我們在潛在空間中沿著一條路徑移動並連續解碼時（例如從一個角色的潛在向量平滑過渡到另一個角色的潛在向量），VAE 能夠產生平滑過渡的圖像序列，眼睛顏色或髮型會逐漸變化。而傳統 AE 在潛在空間中的空白區域解碼出來的圖像則會變模糊甚至「爛掉」，無法實現平滑過渡。這與 VAE 潛在空間更為集中、標準差較小（實驗中平均標準差 VAE 約 0.2，AE 約 53.53；平均向量長度 VAE 約 19，AE 約 4246）的分佈特性密切相關。

這些實驗證明了 VAE 作為一個 **Generative Model (生成模型)** 的潛力，它能夠生成訓練數據集中未出現過但具有類似風格和特徵的新圖像。

## 挑戰與未來展望

儘管取得了有趣的成果，我們的研究也遇到了一些挑戰，這部分對於新手來說是了解實際研究中可能遇到的問題的寶貴經驗：

1.  **圖像解析度與清晰度**：目前生成的圖像解析度僅為 128x128 像素，放大後會比較模糊。要生成更高品質、更清晰的圖像，需要更大規模的訓練數據和更強大的模型架構。
2.  **生成圖像的質量與主觀性**：評估生成圖像的「可愛度」或整體質量是一個主觀的問題。除了視覺檢查，如何使用客觀指標來評估生成圖像的多樣性和真實感（例如 FID, Inception Score 等，雖然資料中未明確提及這些名詞，但在評估生成模型時是常用的）是需要進一步探索的方向。
3.  **潛在空間特徵的理解與控制**：VAE 學到的潛在空間特徵是模型自行決定的，我們很難直接知道潛在向量的每一個維度具體代表圖像中的哪個特徵（例如眼睛顏色、髮型、表情）。雖然可以透過實驗來「探索」潛在空間，但缺乏精確的特徵控制。未來可以考慮結合 **Conditional GANs (條件式對抗生成網路)** 等技術，允許我們根據文字描述或標籤來控制生成的圖像特徵。
4.  **數據集特性影響**：動漫角色圖像資料集的多樣性（不同畫風、角度、表情、姿勢等）會顯著影響模型效果。實驗發現，使用更乾淨且一致性高的數據集（例如只包含正面頭部圖像，可能需要進行 **Pose Normalization (姿態歸一化)**）能獲得更好的訓練結果和生成質量。
5.  **模型訓練的複雜性**：VAE 的訓練相較於傳統 AE 更複雜，需要仔細調整損失函數中重建損失與 KL 散度的權重，以達到理想的潛在空間分佈和生成效果。
6.  **版權問題**：使用動漫角色的圖像進行訓練和公開展示存在潛在的版權問題，這是在研究中需要注意並謹慎處理的法律與道德議題。

未來的研究可以從提高生成圖像解析度、增強對潛在空間特徵的控制、探索更複雜的數據集（例如包含全身圖像、不同風格的圖像）以及結合其他生成模型技術（如 VAE-GAN、**StyleGAN** 等）等方面進行。此外，收集 **Cosplay (角色扮演)** 照片等真實人臉圖像與動漫角色圖像進行跨領域轉換，也是一個有趣的應用方向。雖然目前尚不確定是否能將我們的 VAE 實現 **Open Source (開源)**，但我們希望能讓更多人了解並體驗這個有趣的模型架構。

總結來說，Variational Autoencoder 確實為動漫角色影像生成提供了一條可行且富有潛力的研究途徑。透過學習連續且結構化的潛在空間分佈，VAE 能夠生成具有創意和連貫性的新圖像，並允許一定程度的特徵操控與角色融合。雖然還有許多挑戰需要克服，但這項技術在動漫、遊戲或其他創意產業中具有廣闊的應用前景。
