---
layout: default
title: Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101
---

<p align="center">
  <strong>Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101</strong>  
</p>
<p align="center">
  AI是一條孤獨且充滿惶恐及未知的旅程，花俏絢麗的收費課程或活動絕非通往成功的捷徑。<br>
  衷心感謝當時來自不同單位的AI同好參與者實名分享的寶貴經驗；如欲移除資訊還請告知。<br>
  由 <a href="https://www.twman.org/" target="_blank">TonTon Huang Ph.D.</a> 發起，及其當時任職公司(台灣雪豹科技)無償贊助場地及茶水點心。<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="180"></a>
    <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 100px !important;width: 180px !important;" ></a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101" target="_blank">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/" target="_blank">Facebook</a> |
  <a href="https://deep-learning-101.github.io/"> 回 GitHub Pages</a> |
  <a href="http://DeepLearning101.TWMAN.ORG" target="_blank">網站</a> |
  <a href="https://huggingface.co/DeepLearning101" target="_blank">Hugging Face Space</a>
</p>


# Machine Reading Comprehension [YouTube](https://www.youtube.com/watch?v=SXLukeWNIkw)

### 2018/12/07	Nat, Alice & Ian

---

在 MRC 中，問題類型大致可以分為兩類 [1, 3, 4, 6]:
*   **封閉領域 (Close Domain / Closed-book QA):** 這是指模型的知識來源僅限於提供的文章或文本 [1, 4-6]。答案必須從原文中直接找到 [1, 5, 9, 10]。這就像我們高中時做的閱讀測驗 [1, 4, 5]。
*   **開放領域 (Open Domain / Open-book QA):** 這種情況下，模型除了給定的文章外，還可以利用額外的背景知識來回答問題 [1, 4-6]。這可能需要模型具備更多的常識或能存取外部知識庫 [4, 5]。目前主流研究，尤其是在 SQuAD 資料集上的工作，多數集中在 Close Domain 問題 [1, 3, 5]。

對於新手來說，理解 MRC 模型和技術的演進非常重要 [3]。早期的模型可能基於循環神經網路 (RNN) 或長短期記憶網路 (LSTM) [1-3, 11]。例如，Match-LSTM (MLSTM) 就是一種結合 LSTM 和 Attention 機制的模型 [1, 2, 9, 12]。雙向 Attention Flow (BiDAF) 則是另一種早期的重要模型，它引入了問題與文章之間的雙向 Attention 交互 [1, 2, 9, 12, 13]。這些基於 RNN/LSTM 的模型在處理序列資料時，有其時序上的限制，一次只能處理有限的時間步長 [14]。

然而，自然語言處理領域的一個關鍵進展是 Attention Mechanism (注意力機制) [1-3, 15]。這個機制讓模型在處理一個詞語時，能夠考慮到句子中所有其他詞語的資訊，並透過加權的方式，讓模型關注到更重要的部分 [1, 2, 9, 14, 15]。例如，在「蘋果很好吃」和「蘋果的股價又漲了」這兩句話中，Attention 機制可以幫助模型理解「蘋果」這個詞在不同語境下的不同含義，因為它會與「很好吃」或「股價」等詞語產生不同的關聯和權重 [9, 14, 15]。這使得模型能夠更有效地捕捉詞語之間的長距離依賴關係 [15]。

基於 Attention 機制，Google 提出了 Transformer 模型，其代表性論文標題就是「Attention is all you need」[1, 2, 11, 13]。這個模型革命性地完全捨棄了 RNN 和 LSTM 的時序結構，僅依賴 Attention 機制來處理序列 [1, 2, 11, 13]。Transformer 通常包含 Encoder 和 Decoder 兩部分，常用於翻譯等序列到序列的任務 [1, 2, 16, 17]。為了彌補 Attention 機制本身缺乏順序資訊的缺點，Transformer 引入了 Positional Encoding (位置編碼)，透過數學函數（如正弦和餘弦函數）為詞語的位置賦予向量表示，讓模型能夠區分不同位置的詞 [1, 2, 16, 18]。在機器閱讀理解任務中，Transformer 模型通常只需要使用其 Encoder 部分 [1, 16]。

隨後，BERT (Bidirectional Encoder Representations from Transformers) 橫空出世，它是一個基於 Transformer Encoder 的大型預訓練模型 [1-3, 7, 17, 19]。BERT 的核心優勢在於其雙向 Self-Attention 機制，使得模型在處理每個詞語時，都能同時考慮到它左邊和右邊的上下文資訊，從而學習到更豐富的語義表示 [1, 2, 17, 19, 20]。

BERT 透過兩個關鍵的預訓練任務來學習通用的語言表示 [1, 2, 18, 19]:
1.  **Masked Language Model (MLM):** 隨機遮蔽輸入序列中的部分詞語，模型需要預測這些被遮蔽的詞 [1, 2, 18, 19, 21]。這迫使模型深入理解詞語之間的關係和上下文 [19].
2.  **Next Sentence Prediction (NSP):** 給定兩個句子 A 和 B，模型需要判斷 B 是否是 A 的下一句話 [1, 2, 18, 19, 21]。這有助於模型理解句子之間的關係 [19].

透過在大量文本資料上進行這兩個任務的預訓練，BERT 學習到了強大的語言表示能力，並在多種 NLP 任務（包括 MRC）上取得了 SOTA (State Of The Art，最先進) 的表現 [1-3, 14, 19]。例如，在 SQuAD 資料集上的排行榜中，基於 BERT 的模型常常位居前列 [1, 3, 14].

說到資料集，SQuAD (Stanford Question Answering Dataset) 是機器閱讀理解領域最重要的基準資料集之一 [1-3, 5, 7, 15]。它由 Stanford University 收集，包含從維基百科文章中提取的大量問題與答案對 [1, 2, 5, 7, 15]。在 SQuAD 1.1 版本中，問題的答案必須是文章中的一段連續文字 [1, 9, 10, 22, 23]。然而，SQuAD 1.1 的一個問題是模型在找不到答案時可能會「亂回答」[9]。為了提高模型的魯棒性，SQuAD 2.0 版本引入了「impossible」問題，即答案不在原文中的問題，模型需要判斷並告知無法回答 [1, 9, 15, 22, 23]。

評估 MRC 模型在 SQuAD 等資料集上的表現，常用的指標包括 [1-3, 15, 22]:
*   **F1 Score (F1 分數):** 衡量模型的精確率 (Precision) 和召回率 (Recall) 的調和平均數 [1, 2, 10, 15, 22]。它綜合考慮了模型找到正確答案的能力 [2].
*   **EM (Exact Match, 精確匹配):** 要求模型輸出的答案必須與標準答案完全一致 [1, 2, 10, 15, 22]。這是一個較為嚴格的指標 [2, 10].

儘管 BERT 等大型模型在準確性上取得了巨大成功，但在實際應用中仍面臨不少挑戰 [3, 19, 24]。特別是在中文 MRC 領域，我們研究人員深感數據集的稀缺性是一個重要的瓶頸 [3, 8, 9, 19, 20, 23-27]。相較於英文，高品質的中文機器閱讀理解資料集非常少 [8, 19, 20, 23-25, 27]。構建這樣的資料集耗時耗力，需要大量的人工標註 [19, 24, 27, 28]。我們團隊也曾嘗試過人工標註中文維基百科文章來建立 Q&A 資料集，這過程需要人工理解文章、設計問題並從文中框選答案，確實非常辛苦 [27, 28]。百度雖然推出了 Dureader 中文資料集，但我們在實際測試其程式碼和資料時也遇到了一些問題 [23, 26, 29]。Delta Research 也有一個中文閱讀理解資料集和標註平台正在建設中，這對於中文 MRC 的發展是一個正面的信號 [23, 27]。

除了資料集問題，大型模型如 BERT 對計算資源的需求也是一個巨大的挑戰 [3, 11, 17, 19, 24, 30]。BERT 的完整訓練或從頭重新訓練需要龐大的計算能力，例如高階 GPU (如 Titan XP, Titan V) 或 TPU [17, 19, 20, 23, 24, 30]。這對於個人或小型團隊來說門檻很高 [24, 30]。雖然 Google 提供了預訓練好的 BERT 模型，我們可以透過 Fine-tuning (微調) 來適應特定任務，但這仍然需要可觀的計算資源 [1, 2, 19, 20, 23]。

此外，中文本身的語言特性，如沒有空格導致需要進行斷詞斷字，以及語義的歧義性，也對模型的理解帶來了挑戰 [19, 20, 23-25]。在實際應用中，將大型機器閱讀理解模型部署到邊緣設備（如機器人、智能音箱）時，還需要考慮模型大小、計算延遲和網路延遲等問題 [24, 28, 29]。雖然 BERT 在準確性上表現出色，但在需要即時反應的場景下，其推理延遲仍然是一個需要克服的障礙 [28, 29]。我們團隊也在測試各種模型，並嘗試根據實際應用場景進行優化和調整，希望能讓模型在實際硬體上實用化 [23].

總而言之，機器閱讀理解領域從早期的模型演進到基於 Attention 的 Transformer 和 BERT，展現了巨大的潛力 [3]。BERT 的出現將該領域的準確性推向了新的高度 [1, 3, 14, 19, 23]。然而，對於中文環境的研究和應用，我們仍然面臨著資料集稀缺、計算資源限制以及語言特性帶來的挑戰 [3, 19, 23-25]。持續投入於高品質中文資料集的建構以及模型的優化和輕量化，是推動中文 MRC 技術發展的關鍵方向 [3, 23, 24, 28].

**關鍵詞彙解釋 (Glossary)** [2]

*   **NLP (Natural Language Processing):** 自然語言處理。研究如何讓電腦理解、處理和生成人類語言的領域 [1, 2].
*   **MRC (Machine Reading Complication):** 機器閱讀理解。指機器從文本中理解內容並回答相關問題的能力 [1, 2].
*   **QA (Question Answering):** 問答系統。一種能夠回答用戶提出的問題的系統 [1, 2].
*   **Sentiment Analysis:** 語情感分析。判斷文本表達的情感（正面、負面、中立等） [1, 2].
*   **Dataset:** 資料集。用於訓練和評估機器學習模型的資料集合 [2].
*   **SQuAD (Stanford Question Answering Dataset):** 一個大型的機器閱讀理解資料集 [1, 2].
*   **F1 Score:** F1 分數。衡量分類模型效能的指標，是精確率和召回率的調和平均數 [1, 2].
*   **EM (Exact Match):** 精確匹配。評估機器閱讀理解模型時，要求模型輸出的答案與標準答案完全一致 [1, 2].
*   **Attention Mechanism:** 注意力機制。一種讓模型在處理序列資料時能夠關注到不同部分權重的方法 [1, 2].
*   **Transformer Model:** Transformer 模型。一種基於注意力機制的神經網路模型，在 NLP 領域表現出色 [1, 2].
*   **Positional Encoding:** 位置編碼。為 Transformer 模型中的詞語添加位置資訊的方法 [1, 2].
*   **RNN (Recurrent Neural Network):** 循環神經網路。一種處理序列資料的神經網路 [1, 2].
*   **LSTM (Long Short-Term Memory):** 長短期記憶網路。一種特殊的 RNN，能夠更好地處理長距離依賴問題 [1, 2].
*   **BERT (Bidirectional Encoder Representations from Transformers):** 一個基於 Transformer Encoder 的大型預訓練模型 [1, 2].
*   **MLM (Masked Language Model):** 遮蔽語言模型。BERT 的預訓練任務之一 [1, 2].
*   **NSP (Next Sentence Prediction):** 下一句預測。BERT 的預訓練任務之一 [1, 2].
*   **SOTA (State Of The Art):** 最先進。指在某個領域或任務中表現最好的技術或模型 [1, 2].
*   **Fine-tuning:** 微調。使用特定任務的資料對預訓練模型進行進一步訓練，使其適應特定任務 [1, 2].
*   **GPU (Graphics Processing Unit):** 圖形處理單元。常用於加速深度學習模型的訓練和推理 [1, 2].
*   **TPU (Tensor Processing Unit):** 張量處理單元。Google 開發的專用於機器學習的硬體加速器 [2].

