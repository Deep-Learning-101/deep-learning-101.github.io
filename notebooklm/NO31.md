---
layout: default
title: Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101
---

<p align="center">
  <strong>Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101</strong>  
</p>
<p align="center">
  AI是一條孤獨且充滿惶恐及未知的旅程，花俏絢麗的收費課程或活動絕非通往成功的捷徑。<br>
  衷心感謝當時來自不同單位的AI同好參與者實名分享的寶貴經驗；如欲移除資訊還請告知。<br>
  由 <a href="https://www.twman.org/" target="_blank">TonTon Huang Ph.D.</a> 發起，及其當時任職公司(台灣雪豹科技)無償贊助場地及茶水點心。<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="180"></a>
    <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 100px !important;width: 180px !important;" ></a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101" target="_blank">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/" target="_blank">Facebook</a> |
  <a href="https://deep-learning-101.github.io/"> 回 GitHub Pages</a> |
  <a href="http://DeepLearning101.TWMAN.ORG" target="_blank">網站</a> |
  <a href="https://huggingface.co/DeepLearning101" target="_blank">Hugging Face Space</a>
</p>


# Machine Teaching

<a href="https://www.youtube.com/watch?v=q1XhbjnFICc" target="_blank" rel="noopener noreferrer"><i class="fab fa-youtube mr-1"></i>2019/06/28, Mark Liou, Machine Teaching</a><br>

# 機器教學與蒸餾：核心概念與方法

本文件彙整了講者關於機器教學（Machine Teaching）與蒸餾（Distillation）主題分享的關鍵重點，主要涵蓋模型蒸餾（Model Distillation）與資料集蒸餾（Dataset Distillation）兩大核心概念及其相關技術和討論。內容基於提供的四份資料來源。

## 核心概念：機器教學

**機器教學**的核心概念是指人類利用機器直接去訓練機器的過程。與傳統的監督式學習（Supervised Learning）從大量標註資料中學習不同，機器教學探討如何利用現有的模型或資料，將知識轉移或濃縮，以教導新的模型，尤其是更小、更有效率的模型，以便部署在資源受限的裝置上。

## 模型蒸餾 (Model Distillation)

**模型蒸餾**的核心思想是將一個較大或性能更好的**教師模型 (Teacher Net)** 的知識轉移到一個通常較小、更有效率的**學生模型 (Student Net)** 上。

*   **經典做法 (Knowledge Distillation):**
    *   學生模型學習的目標是教師模型的輸出結果，而不僅僅是原始資料的真實標籤 (Ground Truth Label)，真實標籤通常是 One-Hot 編碼。
    *   學習教師模型的輸出能夠捕捉到類別之間的細微關係，這種資訊被稱為**暗知識 (Dark Knowledge)**。例如，教師模型在識別數字 "9" 時，除了給予 "9" 最高的機率外，可能也會給 "4" 較高的機率，這表示這張 "9" 的圖片在某些方面與 "4" 相似。學生模型透過學習教師模型的完整機率分佈，可以學到這種暗知識，這對於區分相似類別非常有幫助。

*   **Logits 與 Softmax 的學習:**
    *   直接學習 Softmax 函數前的原始輸出 **Logits** 可能比學習 Softmax 後的機率分佈效果更好、訓練更快。這是因為 Logits 保留了不同類別預測值之間的數值大小關係，而 Softmax 會將其壓縮到 [1] 區間，可能損失了這些資訊。
    *   Loss Function 可以使用 MSE 或 KL Divergence。
    *   **實際操作挑戰:** 在實際應用中，有時難以取得教師模型的 Logits，因為通常只能獲得最終的 Softmax 輸出結果。

*   **Temperature 參數的影響:**
    *   在 Softmax 函數中加入 **Temperature 參數** (T) 可以使機率分佈變得更**平滑**。Temperature 值越大，分佈越平坦，表示模型對預測結果「不那麼確定」。
    *   實驗發現，在蒸餾過程中加入 Temperature 參數可以讓學生模型學得更好。講者認為這與 Ny Teacher 的概念相似，都是讓輸出分佈不那麼尖銳。

*   **Label Smoothing 與 Temperature 的交互作用:**
    *   **Label Smoothing** 是一種常用的正則化技巧，將 One-Hot 編碼的標籤進行平滑處理 (例如將 1 變成 0.8，其他類別平分剩餘的 0.2)。
    *   有研究指出，在蒸餾過程中同時使用 Temperature 和 Label Smoothing 可能會導致結果變差。原因可能是 Label Smoothing 使得訓練數據點在特徵空間中更為集中，導致學生模型無法學到教師模型在這些數據點分佈上的細微差別。因此，在蒸餾時應避免同時使用 Label Smoothing。

*   **Relational Knowledge Distillation (關係知識蒸餾):**
    *   這是一種在 CVPR 2019 提出的方法。
    *   除了學習單一數據點的輸出外，學生模型還要學習**樣本與樣本之間的關係**。這種關係可以通過計算不同數據點在教師模型某一中間層或 Logits 輸出之間的距離或角度來衡量。
    *   學習樣本間的關係有助於捕捉空間上的資訊，其動機是希望放入更多除了單一樣本輸出之外的資訊。

*   **Teacher Assistant System (教師助理系統):**
    *   一篇 DeepMind 於 2019 年 2 月發表的論文提出。
    *   用於解決「**教師模型太大、學生模型太小，導致學生學不好**」的問題。
    *   建議在大型教師和小型學生之間引入一個或多個**中等規模的「助理老師」模型**。
    *   實驗顯示，透過多個階段的知識傳遞（例如，10 層教師 -> 8 層助理 -> 2 層學生），可以逐步提升小型學生模型的性能，這比一步到位直接從大型教師蒸餾效果更好。這暗示著**知識的傳遞需要逐步進行**，就像「老師太厲害，學生學不來」一樣，需要分解步驟。

## 資料集蒸餾 (Dataset Distillation)

**資料集蒸餾**的核心思想是將原始的大型數據集**濃縮成一個小型、合成的數據集**，這個合成數據集包含足夠的資訊，能夠用來訓練一個模型，使其在原始數據集上達到良好的性能。

*   **試圖解決的問題:**
    *   **數據壓縮和傳輸:** 將大型數據集壓縮成小尺寸，方便儲存和傳輸，特別是在網絡頻寬受限或數據量龐大的情況下 (例如台積電或基因數據傳輸問題)。
    *   **數據隱私:** 合成數據集通常與原始數據集的樣貌不同，可以保護原始數據的隱私。模型可以在處理過的數據上訓練，而無需直接接觸敏感的原始數據。

*   **不同概念與方法:**
    *   **利用教師模型輔助標註 (Omni-Supervised):** 利用已訓練好的模型對未標註資料進行標註 (結合數據增強和模型集成)，並將這些標註結果作為學生模型的訓練資料。這是一種利用教師模型生成標註資料來訓練學生模型的方法。
    *   **Data-Free Distillation (無資料蒸餾):** 核心思想是在無法直接接觸原始訓練資料的情況下訓練新模型。構想是傳輸教師模型處理後的**「元資料」(Meta Data)**，例如記錄教師模型每一層 Activation 的情況。通過向教師模型輸入一些較小的模式（如噪聲或自然 패턴），記錄 Activation 狀態並傳輸，用於訓練學生模型。這解決了資料傳輸困難和隱私保護的問題，但準確率可能不如使用原始資料。這種方法也可能被惡意用於攻擊模型 (Poison Image)。
    *   **Dataset Distillation (迭代優化合成數據):** 目標是將大型數據集濃縮成一個極小的合成數據集 (例如 MNIST 6 萬張 -> 幾十張)。
        *   具體流程：**迭代優化**一個小的合成數據集。初始化合成數據集（例如，隨機噪聲圖片）。使用當前的合成數據集訓練一個模型。用這個訓練好的模型預測**原始數據集**的結果，並計算損失（例如與原始資料真實標籤的差距）。利用這個損失的梯度資訊，反向傳播回**合成數據集**，更新合成圖片的像素值，使其更能代表原始數據集的資訊。重複以上步驟。
        *   最終生成的合成數據集圖片可能看起來**模糊或奇怪**，與原始圖片差異很大。
        *   用這個合成數據集訓練出的模型，在原始數據集上的準確率可以達到一定水平 (例如 MNIST 達到 94%)。
        *   其核心價值在於極大**壓縮資料量**、解決**數據傳輸**限制和**保護數據隱私**。收到合成數據集的一方可以用它來訓練**自己的模型架構**，且訓練好的模型可以直接用於處理**原始格式的新數據**，無需對新數據進行特殊編碼。
        *   這種方法生成的合成數據集也可能被惡意使用，例如加入錯誤的標籤來進行**攻擊模型** (Poison Image)，破壞他人模型的性能。
    *   **GANs:** 有論文使用 GAN 的方式來進行 Dataset Distillation，但概念與之前提到的方法沒有本質區別。

## 其他相關概念與社群討論

*   CVPR 2019 的趨勢觀察：**GVN (Graph Neural Network)** 興起，**GAN (生成對抗網路)** 熱度下降，**Self-Supervised Learning (自監督學習)** 和 **Meta Learning (元學習)** 很常見。
*   社群聚會旨在促進交流和學習，鼓勵成員定期分享看到的論文、公司經驗或研究問題。社群內有經驗豐富的成員可以提供幫助。
