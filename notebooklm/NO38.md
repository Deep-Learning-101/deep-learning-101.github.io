---
layout: default
title: Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101
---

<p align="center">
  <strong>Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101</strong>  
</p>
<p align="center">
  AI是一條孤獨且充滿惶恐及未知的旅程，花俏絢麗的收費課程或活動絕非通往成功的捷徑。<br>
  衷心感謝當時來自不同單位的AI同好參與者實名分享的寶貴經驗；如欲移除資訊還請告知。<br>
  由 <a href="https://www.twman.org/" target="_blank">TonTon Huang Ph.D.</a> 發起，及其當時任職公司(台灣雪豹科技)無償贊助場地及茶水點心。<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="180"></a>
    <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 100px !important;width: 180px !important;" ></a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101" target="_blank">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/" target="_blank">Facebook</a> |
  <a href="https://deep-learning-101.github.io/"> 回 GitHub Pages</a> |
  <a href="http://DeepLearning101.TWMAN.ORG" target="_blank">網站</a> |
  <a href="https://huggingface.co/DeepLearning101" target="_blank">Hugging Face Space</a>
</p>


# PAC Bayesian for Deep Learning [YouTube](https://www.youtube.com/watch?v=7arMUGtri2s)

### 2020/02/14	Mark Chang

---

### 學習理論的基礎：從訓練到泛化

作為新手入門，首先我們要釐清機器學習的目標是什麼。簡單來說，機器學習是從資料中學習一個模型，這個模型能夠對未見過的資料進行有效的預測。訓練模型的過程，就是從資料中抽出樣本來訓練模型，努力降低模型在**訓練資料集 (Training Data)** 上的誤差，也就是**訓練誤差 (Training Error)**。

然而，光是訓練誤差低還不夠。一個好的模型，必須在它從未見過的**測試資料集 (Testing Data)** 上也能表現良好，這由**測試誤差 (Testing Error)** 來衡量。**學習理論 (Learning Theory)** 探討的，正是為什麼當訓練誤差降低時，測試誤差也能同時降低的數學原理。我們需要一個數學理論來保證這件事是可以達成的。

這裡就引出了機器學習中最常見的問題之一：**過度擬合 (Overfitting)**。這指的是模型在訓練資料上表現得非常好（訓練誤差極低），但一旦換成新的、未見過的測試資料，表現就急劇下降，測試誤差變得很高。用健身的例子，這就像你只會用某個器材做某個動作，而且練到極致（訓練誤差低），但換個器材或做類似的動作就不行了（測試誤差高），因為你沒有真正理解背後的發力原理（模型泛化能力差）。很多人在遇到 Overfitting 時不知道如何解決，如果對理論有更多了解，就能找出對的方向去解決問題。

### 傳統理論：VC Dimension 的世界

在傳統機器學習理論中，解釋過度擬合的一個核心概念是**模型複雜度 (Model Complexity)**。傳統觀念認為，模型參數越多或模型複雜度越高，越容易導致過度擬合。

為了更精確地衡量模型複雜度，**VC Dimension (Vapnik-Chervonenkis Dimension)** 這個概念被提出來。VC Dimension 的直觀解釋是衡量一個模型能夠「打散」(Shatter) 多少個任意標記的資料點。一個模型能夠打散的點越多，意味著它能學習到的模式越複雜，VC Dimension 就越高。

**給新手的小提示：** 「打散」可以想像成，給模型一組資料點，它能否無論這些點被隨機標記成什麼樣子，都能找到一個分類器（一條線、一個邊界等）將它們完美分開。能完美分開的點越多，模型就越「萬能」，VC Dimension 越高。VC Dimension 就是找到這樣一個最大點數 N，使得模型能夠打散任意這 N 個點的任意標記方式，但無法打散 N+1 個點的任意標記方式。

VC Dimension 的計算僅與**模型架構 (Model Architecture)** 有關，例如參數數量、層數、啟動函數等，而與資料的排列方式或數量無關。傳統理論認為，為了避免過度擬合，訓練資料的數量 (N) 至少要大於模型的 VC Dimension (VC_D)。VC Dimension 越高，訓練誤差和測試誤差之間的差距可能越大，更容易發生過度擬合的現象。傳統理論的界限公式表達了訓練誤差與測試誤差之間的關係，這個關係與 VC Dimension 和訓練資料數量 N 有關。當 N 越大時，訓練誤差與測試誤差的差距越小，越不容易過度擬合。當 VC Dimension 越大時，差距越大，越容易過度擬合。根據這個理論，為了不讓差距過大，訓練資料數量 N 至少要大於 VC Dimension。

這個理論框架為過度擬合提供了一個數學解釋：模型太複雜（VC Dimension 太高）相對於資料量不足 (N)，導致模型記住了訓練資料中的噪聲或特有規律，而無法捕捉資料真實的分佈。因此，傳統上解決過度擬合的思路往往是：降低模型複雜度（減少參數、改變架構，從而降低 VC Dimension）或增加訓練資料量。

### 深度學習的挑戰：傳統理論為何失靈？

然而，當我們將傳統理論應用到深度學習時，遇到了顯著的挑戰。深度學習模型通常擁有**巨量參數**，其理論上的 VC Dimension 往往遠遠超過訓練資料的數量。按照傳統理論，這應該會導致嚴重的過度擬合。

但實際情況是，深度學習模型即使在參數數量遠超過資料量（如 ImageNet 只有五萬張圖，但模型參數動輒百萬甚至更多）的情況下，訓練誤差可以降到非常低，並且在測試資料上依然表現良好，過度擬合的現象並沒有像傳統理論預期的那樣嚴重。這種現象在近年來的研究中被廣泛關注，有時被稱為「雙下降」曲線或 Over-parameterization 的好處，即參數多到某個程度後，測試誤差反而會下降。

這明確地告訴我們，僅僅依賴參數數量或 VC Dimension 的傳統思維，不足以解釋深度學習為何能有效學習並泛化。許多研究人員仍試圖用傳統方法解決深度學習問題（例如盲目減少參數），結果發現效果甚微，只能依賴「暴力搜索」參數或「試誤法」來調優，效率非常低，耗費大量運算資源，卻搞不清楚問題的核心是什麼。傳統機器學習理論，特別是基於 VC Dimension 的理論，在探討深度學習的泛化能力時會發現很多地方不適用，這也是為什麼很多抱持傳統思維去做深度學習的人常常得不到好結果的原因。

### 新的視角：PAC-Bayes 理論的引入

這種傳統理論的不足，促使研究人員尋找新的框架來解釋深度學習的行為，**PAC-Bayes (Probably Approximately Correct Bayes)** 理論便是其中一種嘗試。PAC-Bayes 提供了一個從**資料分佈**和**模型參數分佈**變化的角度來理解泛化能力的新視角。

與傳統理論不同，PAC-Bayes 理論適用於**Stochastic Model**，而不是 VC Dimension 框架下的 Deterministic Model。在 Stochastic Model 中，訓練完成的模型並非一個固定的假設（一組固定的參數），而是一個假設的**分佈 (Distribution)**。這意味著模型的參數被視為來自某個概率分佈，每次進行預測時，是從這個分佈中採樣一組參數來使用。傳統機器學習理論多用於 Deterministic Model，訓練完的模型就是一組固定的參數，因此 VC Dimension 主要也是為 Deterministic Model 設計的。

**給新手的小提示：** 想像你的模型不是只有一套固定的「武功招式」，而是一個「武功招式大全」的分佈。每次要出招，你從這個大全中隨機選一套來用。這個「大全」的分佈，就是 Stochastic Model 的核心概念。

PAC-Bayes 理論的泛化上界公式中，核心衡量指標是訓練後模型參數分佈 (Posterior Distribution, Q) 與訓練前或初始分佈 (Prior Distribution, P) 之間的距離，通常使用 **Kullback-Leibler Divergence (KL Divergence)** 來衡量，並與訓練資料的數量 N 有關。

**PAC-Bayes 的核心思想是 Data Dependent**。它從資料的性質和模型對資料的擬合程度來解釋過度擬合。KL Divergence 越小，表示 Q 與 P 越接近，模型對訓練資料的擬合可能不深，訓練誤差和測試誤差都可能較高（類似欠擬合）。當模型過度擬合訓練資料中的噪聲或特定細節時，Q 會為了完美擬合訓練資料而偏離 P 很遠，KL Divergence 變大，此時訓練誤差很低，但 PAC-Bayes 的上界變大，測試誤差可能很高（過度擬合）。

PAC-Bayes 理論能夠很好地解釋為什麼深度學習模型在參數過多的情況下也能有好的泛化能力。它表明，即使模型架構複雜（VC Dimension 高），如果訓練資料是「乾淨」、「有規律」的，模型可能只需要對其初始分佈進行較小的調整就能擬合資料，此時訓練後的參數分佈 Q 與初始分佈 P 的 KL Divergence 較小，泛化能力依然很好。VC Dimension 無法反應這種數據性質對泛化的影響，因為它與數據無關，只跟模型架構有關。

反之，如果訓練資料是「帶噪聲」、「標籤隨機化」或「圖像隨機化」的，資料本身缺乏規律性。模型為了擬合這些沒有規律的訓練資料（比如隨機標記的數據），需要大幅度地調整其參數分佈，導致 Q 偏離 P 很多，KL Divergence 變大。根據 PAC-Bayes 的公式，這會使得測試誤差的理論上界變高，實際表現也容易出現過度擬合，測試誤差變得非常高（接近隨機猜測的結果）。這有力地證明，會不會過度擬合與**資料集的性質**有關，而不是單純取決於模型的複雜度（VC Dimension）。PAC-Bayes 理論可以實際反映出這種因資料性質導致的泛化表現差異，而傳統 VC 理論無法解釋這種現象。

此外，PAC-Bayes 理論中的 Stochastic Model 概念，也能自然地與 Loss Landscape 中的**平坦最小值 (Flat Minimum)** 聯繫起來。Flat Minimum 指的是損失函數在最佳參數點周圍區域相對平坦，損失變化不大。Stochastic Model 考慮的是參數的分佈，如果這個分佈落在一個 Flat Minimum 區域，即使參數有輕微擾動（採樣），模型表現（誤差）也不會有太大變化，這通常意味著更好的泛化能力。而 Sharp Minimum 周圍損失變化劇烈，對應的泛化能力通常較差。PAC-Bayes 的上界也與這種 Flat Minimum 的特性相關，一個平坦的損失函數區域對應著較小的 PAC-Bayes 泛化上界，表明更好的泛化能力。

### PAC-Bayes 如何指導深度學習的實踐？

從 PAC-Bayes 的視角出發，我們可以重新審視深度學習中一些常用的技巧。

1.  **減少參數數量 (Reducing Parameters)**：傳統上認為減少參數可以降低 VC Dimension，緩解過度擬合。然而，從 PAC-Bayes 的觀點看，參數數量與 KL Divergence 沒有直接關係。簡單地減少參數，對於降低訓練前後模型分佈的差異（KL Divergence）可能沒有幫助，因此效果往往不大，甚至幾乎沒有任何改善。這解釋了為什麼在深度學習中，調整模型大小的效果常常不如預期，用傳統思維去減少參數來解決 Overfitting 常常會失敗。

2.  **權重衰減 (Weight Decay / L2 Regularization)**：這是一種正則化技巧，通過限制權重的大小來懲罰大的權重。從 PAC-Bayes 的角度，權重衰減可以限制訓練後的模型參數分佈 Q 的範圍，使其不會距離初始分佈 P 太遠，從而間接限制了 KL Divergence，有助於降低過度擬合。這是因為權重衰減會鼓勵模型參數保持在一個相對緊湊的區域，使得後驗分佈 Q 不會過度擴散或偏離先驗分佈 P。

3.  **早停法 (Early Stopping)**：在訓練過程中監控模型在驗證集上的表現，當驗證誤差開始增加時停止訓練。這可以防止模型過度擬合訓練資料，使得訓練後的模型分佈 Q 不會為了完美擬合訓練資料而過度偏離初始分佈 P，從而限制了 KL Divergence。早停法就像在訓練過程中人為地阻止 KL Divergence 變得太大，使得模型不會過度擬合訓練數據中的噪聲。

4.  **資料增強 (Data Augmentation)**：通過對訓練資料進行變換（如旋轉、裁剪）來增加資料量和多樣性。傳統觀念認為這增加了訓練資料量 N，有助於泛化。但從 PAC-Bayes 的角度看，資料增強最大的問題在於它改變了資料的分佈 (Data Distribution)。如果增強後的資料分佈與實際測試資料分佈差異很大，訓練在這個增強數據上的模型在測試數據上表現可能有限，甚至可能產生「錯誤」的資料或標籤，進一步增大 KL Divergence 或使得 PAC-Bayes 理論基於「訓練和測試數據來自同一分佈」的假設失效。

5.  **預訓練模型 (Pre-trained Models)**：使用在大規模資料集上預訓練的模型作為起點。這可以提供一個較好的初始模型分佈 (Prior P)，這個分佈已經從海量真實世界數據中學習到了資料中常見的特徵和真實數據分佈。因此，針對特定下游任務的微調只需要對模型分佈進行微小的調整，訓練後的後驗分佈 Q 不會距離預訓練得到的初始分佈 P 太遠，KL Divergence 會比較小，有利於泛化，特別是對於資料量有限的任務。與其花時間調整模型架構或參數數量，尋找一個好的預訓練模型或改進數據品質反而是更有用的方向。

### 解決深度學習過度擬合的建議 (基於 PAC-Bayes 觀點)

從 PAC-Bayes 的視角出發，解決深度學習過度擬合的一個重要方向是從**訓練資料**著手。

1.  **提高資料品質 (Improving Data Quality)**：確保資料標記的準確性，減少訓練數據中的雜訊和錯誤。乾淨、有規律的數據能讓模型更容易找到數據的真實分佈，從而減少訓練前後模型參數分佈的變化（較小的 KL Divergence），有利於泛化。
2.  **收集更多資料 (Collecting More Data)**：特別是當解決的問題本身具有很高的變異性 (variability) 和複雜性時，需要大量的資料來捕捉其複雜性。深度學習需要大量資料的原因更多地來自於所解決的問題本身的複雜性，需要大量的訓練樣本來學習這些複雜的模式和捕捉資料分佈的特性，而不是僅僅因為模型的高 VC Dimension。更多的數據有助於模型學習數據的真實分佈，而不是訓練集中的特定細節。
3.  **選擇合適的預訓練模型 (Choosing Suitable Pre-trained Models)**：選擇一個在與目標任務相似的資料集上訓練的模型，可以提供一個更好的起點 (Prior)，這個 Prior 已經接近目標數據的真實分佈，使得微調所需的調整幅度較小，KL Divergence 較小，有助於泛化。

總結來說，VC Dimension 作為傳統理論，能衡量模型架構的複雜度，但在解釋深度學習為何在高參數下仍能泛化時遇到困難。PAC-Bayes 提供了一個更貼近深度學習實際行為的框架，它從資料分佈和模型參數分佈變化的角度解釋泛化能力，強調資料品質和訓練過程對模型分佈的影響，並通過 KL Divergence 來衡量過度擬合的程度。理解 PAC-Bayes 的思維，能幫助我們更有效地診斷和解決深度學習中的泛化問題，而不是停留在傳統的參數數量或模型複雜度的直覺層面。
