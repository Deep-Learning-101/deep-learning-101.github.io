---
layout: default
title: Deep Learning 101, 台灣曾經最高最早發起的深度學習社群 @ 83F, 台北101
---

<p align="center">
  <strong>The top private AI Meetup in Taiwan, launched on 2016/11/11 @ 83F, Taipei 101</strong>
</p>
<p align="center">
  <strong>Deep Learning 101, 台灣曾經最高最早發起的深度學習社群 @ 83F, 台北101</strong><br><br>
  AI是條寂寞且惶恐的道路，花俏的收費課程或活動絕不會是條捷徑<br>
  本頁內容為過往實名分享制的讀書會，感謝來自不同公司參與者的支持；如欲移除資訊還請告知。<br>
  Deep Learning 101 只由 TonTon Huang Ph.D. 及其當時任職公司無償贊助場地及茶水點心，無 Co-organizer<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="400">
  </a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/">台灣人工智慧社團 FB</a> |
  <a href="https://www.twman.org/">TonTon Huang Ph.D.</a> |  
  <a href="http://DeepLearning101.TWMAN.ORG">台灣人工智慧社團 網站</a> |
  <a href="https://huggingface.co/DeepLearning101">Hugging Face</a>
</p>
<p align="center">
<a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 60px !important;width: 217px !important;" >
<audio controls style="width:200px; height:20px;"><source src="notebooklm-mp3/NO44.mp3" type="audio/mpeg"></audio>
</a>
</p>

# 神經網路的黑執事 - [YouTube](https://www.youtube.com/watch?v=gMaQTqZUW58) - <a href="https://deep-learning-101.github.io/">回上一頁 GitHub Pages</a>

### 2020/08/21	Mark Liou

# 深度學習進階訓練技巧與應用匯整報告

## 核心主題與重要概念

*   **資料正規化 (Data Regularization) 與模型正規化 (Model Regularization)**：影片強調，許多近期有效的模型訓練方法（例如知識蒸餾、各種資料擴增、自監督學習）的核心思想並非傳統上直接對模型權重進行正規化，而是在 **資料本身** 或透過操作資料來約束模型的學習過程，使其學到的表示更魯棒和通用 [1, 2]。演講者認為這是從傳統模型正規化轉變為數據正規化的趨勢 [1, 2]。
*   **善用「隱生」特性 (Emergent Properties)**：講者指出，神經網路內部存在一種「隱生」特性 [3]。雖然早期方法可能忽略，但若善用這種特性，它能變得「非常有用」來處理和消化大量資料 [3, 4]。這也與「隱生惡魔」(emergent demons) 的概念有關，描述模型即使使用了資料擴增，仍可能對微小輸入變化敏感的現象 [5, 6]。
*   **從不同視角看數據**：資料擴增可以被視為從不同視角看待原始數據 [1, 3]。

## 關鍵技術與方法詳解

### 1. Label Smoothing (標籤平滑)

*   **基本概念**：Label Smoothing 是一種正規化技術，用於修改分類問題的目標標籤 [3, 7, 8]。
*   **與 One-Hot Encoding 的比較**：傳統 One-Hot Encoding 只將正確類別的機率設為 1，其他為 0 [3, 8, 9]。Label Smoothing 則會將正確類別的一小部分機率 (α/k) 分攤到其他類別（α 是一個小的數值，k 是類別數量） [3, 8, 9]。
*   **優勢**：這樣做能防止模型對訓練資料過度自信 [8]，並在訓練中考慮到其他類別的可能性 [3, 8, 9]。這有助於提升模型泛化能力 [8]。做 Label Smoothing 會發現它能達到更好的結果 [3]。
*   **與知識蒸餾的關聯**：Label Smoothing 的效果類似於知識蒸餾中的「溫度」(temperature) 概念 [3, 5-7]。溫度會使模型的輸出（soft targets）變得不那麼尖銳，更平緩 [3, 5-7]。這使得模型能從較低的機率分佈中學習額外資訊 [3, 7]，對區分相似類別有幫助 [3]。Label Smoothing 可被視為一種簡單形式的知識蒸餾 [3]。

### 2. 資料擴增 (Data Augmentation) 技術

*   **核心角色**：資料擴增是提升模型性能的「共識」方法 [3]，特別是在有限標記資料時 [3, 10]。它透過對現有資料進行變換，增加訓練樣本多樣性 [5, 6, 10]。
*   **目的**：迫使模型學習更魯棒 [3, 10, 11]、更具泛化能力的特徵 [3, 5, 8]。希望模型不受資料擴增或變化的影響，產生一致或接近的輸出 [3, 12, 13]。
*   **具體技術**：
    *   **Mixup**：混合兩張圖片及其標籤 [2, 3, 7]。
    *   **CutMix**：將一張圖片的部分區域替換為另一張圖片的區域，並混合標籤 [2, 3, 7, 12]。這被認為比簡單裁切更有效，鼓勵模型關注物體關鍵部分 [2, 12]。
    *   **Cutout**：在圖片中隨機遮擋一個區域 [3, 5, 6, 12]。迫使模型從剩餘部分識別，提高對遮擋的魯棒性 [3, 5, 12]。
    *   **Manifold Mixup**：在神經網路中間層（隱藏層或流形）對多個樣本的特徵表示進行混合 [3, 5, 6, 12]。旨在學習更平滑的特徵空間 [3, 12]。

### 3. 半監督學習 (Semi-Supervised Learning) 與未標記資料利用

*   **重要性**：當標記資料有限但未標記資料豐富時，半監督學習變得重要 [5, 10]。
*   **Pseudo-labeling (偽標籤)**：用已訓練模型預測未標記資料，將預測結果作為「偽標籤」訓練或微調模型 [10]。
*   **Unsupervised Data Augmentation (UDA)**：結合有標記資料的監督學習和未標記資料上的資料擴增 [3, 5, 6, 10, 13]。
    *   **核心思想**：對未標記資料進行資料擴增後，模型的輸出應與原始未擴增的未標記資料的輸出保持一致或相似 [3, 5, 10, 12, 13]。這是一種鼓勵模型對資料擴增保持「不變性」(invariance) 的策略 [10, 12, 13]。
    *   **應用**：特別適用於標記資料稀缺的情況 [10, 13, 14]。UDAL (Unsupervised Data Augmentation with Learnable Augmentations) 也是相關方法 [2]。

### 4. 自監督學習 (Self-Supervised Learning) 與對比學習 (Contrastive Learning)

*   **興起**：自監督學習是一種無需人工標記，透過設計輔助任務來學習資料表示的方法 [6, 8]。
*   **對比學習概念**：核心思想是學習區分「相似」和「不相似」的樣本對 [1, 3, 6, 15, 16]。希望「自己跟自己」的表示很像，而「自己跟別人」的表示很不一樣 [3, 6, 15, 16]。這是一種讓模型從數據本身學習有效表示的方式 [2]。
*   **CP (Contrastive Predictive Coding)**：一種對比預測方法 [5, 6, 15, 17-19]。
    *   **核心思想**：學習一種編碼表示，使得模型能夠區分在時間或空間上相鄰的數據樣本（正樣本對）與隨機抽取的數據樣本（負樣本） [5, 18, 19]。它與 RNN encoder-decoder 結構有聯繫 [16, 18]，可以理解為模型學習從數據的一部分預測另一部分 [16, 18]。它希望模型能夠學習到將不同視角下的同一個物體或同一時間序列中的相鄰部分映射到相似的表示空間 [3]。
    *   **價值**：提出透過對比思維來理解和構建生成模型，對領域有啟發 [15, 19]。

### 5. Domain Adaptation (領域適應)

*   **關聯性**：當模型訓練在一個領域（源領域）但應用於不同領域（目標領域）時會遇到挑戰 [11, 20]。前面討論的利用未標記資料和資料擴增的技術有助於模型學習對資料分佈變化更魯棒的特徵，從而更好地適應新的領域 [11]。透過在未標記的目標領域資料上應用這些技術，模型可以更好地適應新的資料分佈 [11]。

## 時間軸梗概 (非嚴格時間，更多為概念發展順序)

*   **早於 2019 年 6 月 28 日前**：傳統模型正規化（如權重調整）、One-hot 編碼、Dropout 等技術存在 [2]。開始嘗試利用未標記數據 [2]。
*   **2019 年 6 月 28 日**：講者發表了學界當時可能稱為「小帝」的相關論文，認為其方法更貼近生物處理資訊方式 [2, 7, 21]。
*   **2019 年 6 月 28 日之後到近期**：知識蒸餾/溫度概念受到重視 [2, 3, 7]。Mixup, CutMix, Cutout, Manifold Mixup 等資料擴增技術出現 [2, 3, 7, 12]。UDA 等利用未標記數據一致性訓練的方法發展 [2, 3, 10, 13]。對比學習興起，成為重要自監督範式 [1-3, 16]。CP 方法被提出（由「M 的那位人」發表） [5, 6, 18, 21]。

## 提及人物 (可能存在轉錄不準確)

*   **演講者**：核心人物，分享對技術的見解和經驗，提及 2019 年的論文 [2, 3, 7, 14, 21]。
*   **金馬克**：現場聽眾，被提及「帥氣的姿勢」和聲音互動 [1, 4, 21]。
*   **郭大佬**：現場聽眾或熟識者，被詢問是否在觀看直播 [4, 21]。
*   **學界**：代表當時學術界對某些方法的觀點，曾稱講者的研究為「小帝」 [2, 7, 21]。
*   **M 的那位人**：CP 論文的作者 [18, 21]。
*   **（未具名）與演講者合作拍片的人**：在分享 UDAL 實驗經驗時提及 [14, 21]。

## 訓練挑戰與個人心得

*   **訓練難度**：某些技術（如早期的 Label Smoothing）可能「不太好練」 [3, 13]。
*   **數據量影響**：數據量稀缺會顯著影響訓練效果 [13, 14]。
*   **模型強大程度**：技術效果也與神經網路本身的強大程度有關 [3, 13]。
*   **計算資源**：某些模型（如某些對比學習方法）可能需要大量計算資源（顯卡） [15]。

總而言之，這些影片深入探討了如何透過巧妙的資料處理和訓練目標設計，利用未標記資料、增強模型魯棒性，進而在有限標記數據情況下提升模型性能，並指出資料正規化和自監督/半監督學習是重要的發展方向 [3, 22].
