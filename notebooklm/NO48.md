---
layout: default
title: Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101
---

<p align="center">
  <strong>Deep Learning 101, Taiwan’s pioneering and highest deep learning meetup, launched on 2016/11/11 @ 83F, Taipei 101</strong>  
</p>
<p align="center">
  AI是一條孤獨且充滿惶恐及未知的旅程，花俏絢麗的收費課程或活動絕非通往成功的捷徑。<br>
  衷心感謝當時來自不同單位的AI同好參與者實名分享的寶貴經驗；如欲移除資訊還請告知。<br>
  由 <a href="https://www.twman.org/" target="_blank">TonTon Huang Ph.D.</a> 發起，及其當時任職公司(台灣雪豹科技)無償贊助場地及茶水點心。<br>
</p>  
<p align="center">
  <a href="https://huggingface.co/spaces/DeepLearning101/Deep-Learning-101-FAQ" target="_blank">
    <img src="https://github.com/Deep-Learning-101/.github/blob/main/images/DeepLearning101.JPG?raw=true" alt="Deep Learning 101" width="180"></a>
    <a href="https://www.buymeacoffee.com/DeepLearning101" target="_blank"><img src="https://cdn.buymeacoffee.com/buttons/v2/default-red.png" alt="Buy Me A Coffee" style="height: 100px !important;width: 180px !important;" ></a>
</p>
<p align="center">
  <a href="https://www.youtube.com/@DeepLearning101" target="_blank">YouTube</a> |
  <a href="https://www.facebook.com/groups/525579498272187/" target="_blank">Facebook</a> |
  <a href="https://deep-learning-101.github.io/"> 回 GitHub Pages</a> |
  <a href="http://DeepLearning101.TWMAN.ORG" target="_blank">網站</a> |
  <a href="https://huggingface.co/DeepLearning101" target="_blank">Hugging Face Space</a>
</p>


# Model-Based Reinforcement Learning [YouTube](https://www.youtube.com/watch?v=uKJFypVGJdo)

### 2021/07/16	翁崇恒

---

### Dreamer 演算法概覽 (初探)

首先，對於初接觸強化學習 (Reinforcement Learning, RL) 的朋友來說，RL 的核心目標是訓練一個 Agent 在特定環境 (Environment) 中學習如何做出一系列的動作 (Action)，以最大化累積的獎勵 (Reward) [1]。RL 演算法大致分為兩類：Model-Free 和 Model-Based [1]。

*   **Model-Free RL (無模型強化學習)**：Agent 直接學習如何從狀態 (State) 到動作的映射 (Policy) 或狀態/動作的價值函數 (Value Function)，而不顯式地建立環境的模型 [1]。常見的演算法如 Q-learning, DQN, Policy Gradient, A2C, Rainbow 等都屬於此類 [1, 2]。它們的優點是概念相對直觀，某些情況下效能可以很高；缺點往往是訓練需要大量與環境互動的數據，且訓練過程可能較慢 [2, 3]。
*   **Model-Based RL (基於模型的強化學習, MBRL)**：Agent 會學習一個環境的模型，也稱為世界模型 (World Model)。這個世界模型可以預測在給定當前狀態和動作的情況下，環境會轉移到哪個下一個狀態並給予多少獎勵 [1, 4]。Agent 可以在這個學到的世界模型中進行「想像」或「規劃」，模擬未來的互動序列，並在這個模擬過程中學習或改進其策略 [1, 4]。MBRL 的潛在優勢在於資料效率高，因為 Agent 可以透過想像產生額外的學習訊號，減少對真實環境互動的需求 [1]。

Dreamer 就屬於 MBRL 方法 [1, 2, 4]。Dreamer V1 於 2019 年底發表，V2 於 2021 年初發表 [1, 2]。Dreamer 的一個重要里程碑是它是首個在 MBRL 領域達到人類操作水平的演算法 [1-3]。

### Dreamer 的核心概念：「在夢中學習」(Learning in Dreams)

Dreamer 最獨特且形象化的概念就是「在夢中學習」 [1, 2, 5]。這裡的「夢」指的並不是我們平常的夢境，而是 Agent 在其學到的世界模型的**潛在空間 (Latent Space)** 中進行的自我預測和模擬過程 [1, 2, 5]。

想像一下下棋，新手可能只知道規則隨意下子（類似隨機或簡單 Greedy）；進階者能預測到最終勝負（類似 Value Function）；而真正的高手在走一步棋之前，會在腦中快速模擬多個可能的對弈序列，想像對手可能的反應以及自己的應對，這就是一種「腦內對弈」或「想像未來步驟」的能力，Dreamer 就屬於這個層次 [1]。

在 Dreamer 中，這個「想像」過程具體是：Agent 在潛在空間中，根據當前的潛在狀態和它決定的動作，透過世界模型中的動力學模組 (Dynamic Module)，**自動迴歸 (Auto-regressive)** 地預測下一個潛在狀態、獎勵以及遊戲是否結束 [1, 2, 5]。這個過程可以在不與真實環境互動的情況下，在 Agent 的內部世界模型中進行多步模擬，就像在「做夢」一樣 [1, 2, 5]。Agent 在這個想像出來的序列中學習和優化其決策策略 [1, 4]。

### 訓練效率與實作便利性

Dreamer 的一個顯著優勢是其**訓練速度快** [1-3]。相較於一些傳統的基於影像輸入的 Model-Free 演算法（如 A2C 訓練打磚塊可能需要一個月），Dreamer 在打磚塊遊戲上只需一兩天就能達到高分水平，速度提升數十倍 [1-3, 5]。這主要得益於其 Model-Based 的特性，Agent 學會世界模型後可以在潛在空間中高效地產生大量想像數據用於策略學習 [1, 2]。

此外，Dreamer 的**實作相對容易**，它通常只需要一個 Agent 和一個 Environment 進行互動，不像一些算法需要多個 Environment 並行訓練來提高數據吞吐或探索效率 [1, 2, 5]。所需的 Agent 和 Environment 數量較少，降低了實作複雜度 [1, 2, 5]。

### Model-Based RL 的挑戰與 Dreamer 的突破

傳統的 MBRL 方法常面臨一個挑戰：**世界模型的不準確性** [1]。即使學會了世界模型，由於真實環境的複雜性和不確定性，模型預測的未來狀態可能會與真實狀態存在誤差，且這種誤差會隨著想像的步數 (Imagination Horizon) 增長而累積 [1]。這可能導致 Agent 在想像中學到的策略在真實環境中表現不佳，出現「短視」(short-sighted) 或在長遠規劃上效能下降的問題 [1, 6]。

Dreamer 聲稱透過其特定的 Actor-Critic 方法解決了這個問題 [1]。具體來說，Dreamer V1 透過讓世界模型 (World Model)、策略網絡 (Actor) 和價值網絡 (Critic) **共享同一個潛在空間 (Latent Space)** 的方式，使得它們在訓練更新時能夠相互牽制 [1, 6]。當世界模型預測不準時，會影響 Actor 和 Critic 的學習；反之，Actor 和 Critic 在學習過程中也會對世界模型產生約束 [1, 6]。這種相互影響有助於避免世界模型陷入很差的預測，進而提升策略的表現，即使在 Imagination Horizon 增長時，Dreamer 的效能仍能保持甚至提升 [1, 6]。這被認為是 Dreamer 相較於之前 MBRL 方法的一個重要突破 [1, 6]。

### Dreamer V1 架構細節 (進階)

對於想要更深入了解 Dreamer 的研究者來說，其架構是一個關鍵。Dreamer V1 從實作上來看包含七個模組，可分為三大塊：World Model、Actor 和 Critic [1, 6, 7]。

1.  **World Model**：負責學習和預測環境的動態 [1, 2, 7]。它包含五個子模組：
    *   **Encoder (捲積網絡)**：將 Agent 從環境中觀察到的原始輸入（通常是影像）編碼成低維度的潛在變數 (Latent Variable) 或潛在狀態 (Latent State) [1, 7]。Dreamer V1 的輸入是單張影像 [7]。
    *   **Image Decoder (Reconstruction)**：從潛在狀態還原出原始影像，用於訓練世界模型能夠捕捉影像中的重要資訊 [1, 4, 7]。
    *   **Reward Decoder**：從潛在狀態預測在該狀態下可能獲得的獎勵 [1, 4, 7]。
    *   **Discount Decoder**：預測遊戲是否會結束 [1, 4, 7]。這通常預測一個二元分佈，結束時輸出一個趨近於 0 的值（用於乘上未來獎勵的折扣因子），否則輸出折扣因子本身（如 0.995） [1, 7, 8]。這讓模型學會識別遊戲結束的狀態 [1, 8]。
    *   **Dynamic Module (GRU)**：世界模型的核心 [1, 7]。它是一個門控循環單元 (Gated Recurrent Unit, GRU)，負責在潛在空間中進行狀態轉移的自動迴歸預測 [1, 7, 9]。根據前一時刻的潛在狀態、決定的動作以及前一步的確定性狀態，Dynamic Module 預測當前時刻的潛在狀態和確定性狀態 [9, 10]。這是實現「想像」功能的模組 [1, 9]。

2.  **Actor (策略網絡)**：根據當前的潛在狀態決定 Agent 應該採取什麼動作 [1, 2, 7]。

3.  **Critic (價值網絡)**：評估當前潛在狀態的價值，即預測從該狀態開始未來可能獲得的累積獎勵 [1, 4, 7]。

**潛在狀態 (Latent State)**：在 Dreamer V1 中，潛在狀態由兩部分組成：Stochastic State (隨機狀態) 和 Deterministic State (確定性狀態) [1, 9]。Stochastic State 透過 Reparameterization Trick 產生，帶有隨機性；Deterministic State 則通常透過全連接網絡從 Stochastic State 產生，用於捕捉確定性的時間序列資訊 [1, 9, 10]。Dynamic Module 的 GRU 處理這些狀態資訊來進行預測 [9]。

**Reparameterization Trick**：VAE (變分自動編碼器) 中的一個重要技巧 [1, 7]。由於從學到的分佈中採樣 (Sampling) 的過程通常是不可導的，無法直接進行梯度反向傳播 [1, 7]。Reparameterization Trick 將採樣過程轉化為一個可導的計算圖：從一個固定的標準分佈（如標準常態分佈）中採樣一個隨機雜訊，然後透過一個可學習的轉換（通常是平均值和標準差的線性組合）得到來自目標分佈的樣本 [1, 7]。Dreamer V1 除了 Encoder 之外，所有模組的輸出（包括潛在狀態、還原影像、獎勵、Discount）都透過這個技巧產生一個分佈（大多是常態分佈，Discount 是伯努利分佈） [1, 7]。

**想像步驟 (Imagination Step) 與表徵步驟 (Representation Step)**：這是 Dreamer 處理時序數據和利用世界模型的關鍵機制 [1, 9, 11]。
*   **想像步驟 (Imagine Step)**：Agent 在其學到的潛在空間中進行多步預測 [1, 9]。它根據前一個時間步的**想像**潛在狀態和 Actor 決定的動作，透過 Dynamic Module **自動迴歸**地預測當前時間步的潛在狀態 [1, 9, 11]。這個步驟不使用真實環境的觀察，完全是在Agent內部「想像」的 [1, 9, 11]。這個步驟產生的潛在狀態被視為 VAE 中的 **Prior** [1, 11].
*   **表徵步驟 (Representation Step) 或觀察步驟 (Observe Step)**： Agent 接收到**真實環境**在當前時間步給出的觀察 (影像) [1, 11]。Encoder 將這個真實影像編碼成一個潛在表示 [1, 11]。這個表示與前一個時間步的**想像**狀態結合（例如透過 Concatenate） [11, 12]，再經過網絡產生當前時間步的潛在狀態 [11]。這個步驟產生的潛在狀態被視為 VAE 中的 **Posterior** [1, 11]。

在訓練時，一個重要的目標就是讓 Representation Step 從真實觀察中得到的 Posterior 盡可能地接近 Imagination Step 基於前一狀態預測的 Prior [1, 11, 13]。這透過最小化 Prior 和 Posterior 之間的 **KL 散度 (KL Divergence)** 來實現 [1, 13, 14]。這迫使世界模型學會從前一狀態準確預測下一個狀態的潛在表示，同時也讓潛在表示能夠捕捉真實觀察中的重要資訊 [1, 11, 13]。

**VAE 的應用與 Disentanglement 討論**：Dreamer V1 大量使用了 VAE 的概念 [1, 2, 14]。最小化 Prior 和 Posterior 之間的 KL 散度是 VAE 的核心訓練目標之一 [1, 13, 14]。Dreamer V1 的 Beta 值設定為 1，這相當於標準 VAE [1, 15]。資料中提到 VAE 中的 disentanglement (解纏結) 概念，即希望潛在變數的不同維度能夠捕捉到資料中獨立變化的因素（例如在遊戲中將物體的位置、速度、形狀等資訊分開） [1, 16, 17]。雖然 Dreamer V1 論文未明確證實，但猜測引入 VAE 概念可能部分是為了鼓勵潛在空間的 disentanglement，將與遊戲機制相關的重要資訊與影像本身的細節分開，這可能有利於世界模型的預測和策略的學習 [1, 17-19]。然而，討論中也指出，VAE 的 Beta 值通常大於 1 時才更強調 disentanglement 的效果，而 V1 的 Beta=1，V2 的 Beta=0.8，這可能意味著其主要目的並非嚴格的 disentanglement，而是利用 VAE 的資訊瓶頸 (Information Bottleneck) 特性，迫使模型在有限的潛在空間中只保留對預測重要的高層次資訊（如遊戲機制） [1, 15, 17-19]。這部分在討論中存在不同的觀點和猜測，需要進一步的研究或實驗來證實 [15, 17-19]。

### 數據收集 (實作細節)

為了訓練 Agent，需要收集 Agent 與環境互動的數據 [1, 3, 20]。Dreamer 的數據收集有一些特定的設定 [1, 20]：
*   每完成一個 Episode (一局遊戲)，將整個 Episode 的狀態-動作-獎勵序列 (Trajectory) 儲存起來 [1, 20]。
*   使用 Sticky Action：Agent 執行的動作帶有一點不確定性或黏性 [1, 20]。
*   每個動作重複四次 (frame skip) [1, 20]。
*   每收集 16 個 Step 的數據，更新策略網絡一次 [1, 20]。
*   Episode 的最大長度設定為 10800 Step (約 30 分鐘) [1, 20]。
*   經驗回放緩衝區 (Experience Replay Buffer) 的最大容量為 2 * 10^6 Step [1, 20]。
*   訓練開始前會先隨機採取動作收集 50000 步數據填充緩衝區 [1, 20]。

### 模型更新 (學習過程)

模型的更新分為 World Model 的更新以及 Actor/Critic 的更新 [1, 5, 20]。

**World Model 更新**：世界模型主要透過比較其預測與真實環境觀察之間的差異來學習 [1, 13, 20]。Loss 函數包含多個項 [1, 13]：
*   **KL Divergence Loss**：最小化 Representation Step (從真實影像得到) 的 Posterior 與 Imagination Step (從前一步想像狀態預測得到) 的 Prior 之間的 KL 散度 [1, 13]。這迫使模型在沒有真實觀察的情況下也能準確預測潛在狀態 [1, 11, 13]。
*   **Reconstruction Loss**：從潛在狀態還原的影像與真實影像的差異 [1, 13]。這確保潛在狀態包含了足以還原原始觀察的資訊 [1, 13].
*   **Reward Loss**：從潛在狀態預測的獎勵與真實環境給予的獎勵之間的差異 [1, 13].
*   **Discount Loss**：從潛在狀態預測的 Discount (遊戲是否結束) 與真實 Discount 之間的差異 [1, 13].

這些 Loss 函數的總和用於更新 World Model 中的 Encoder, Image Decoder, Reward Decoder, Discount Decoder, 和 Dynamic Module [1, 13].

**Actor/Critic 更新**：Actor 和 Critic 的更新則完全發生在**想像步驟 (Imagination Step)** 中，利用學到的世界模型在潛在空間中進行多步模擬產生的想像數據 [1, 14].
*   **Lambda Return (λ-return)**：用於評估策略在想像序列中的好壞 [1, 4, 14]. Lambda Return 結合了蒙地卡羅方法 (基於一個完整的序列計算總獎勵) 和時間差分方法 (基於下一個狀態的價值預測) 的優點，用於計算一個狀態或動作的累積折扣獎勵的估計 [1, 4, 14]. Dreamer 在想像序列中計算 Lambda Return 作為訓練 Actor 和 Critic 的目標 [1, 14].
*   **Actor Loss**：Dreamer V1 的 Actor 更新目標是**最大化 Lambda Return** [1, 14]. 這與傳統的 Policy Gradient 方法不同 [1, 14, 21]. V1 的方法是直接嘗試讓策略在想像中的表現更好 [14].
*   **Critic Loss**：Critic 的目標是學習準確地預測 Lambda Return [1, 14]. Loss 函數設計為使 Critic 預測的價值 (Value) 盡量接近計算出的 Lambda Return [1, 14]. 這體現了 Bootstrapping 的概念，即用一個估計值 (Lambda Return) 來更新另一個估計值 (Value Function) [1, 14].

**雞生蛋，蛋生雞問題**：在討論中提到了計算 Lambda Return 時需要使用 Critic 預測的 Value Function，而 Value Function 又需要學習接近 Lambda Return [1, 14, 22]。這似乎是一個循環依賴 [1, 22]。資料中解釋說，這是因為 Imagination Step 中的採樣導致數值存在不確定性，所以需要透過這種近似的方式來進行學習 [1, 22]。

### 探索機制 (Exploration)

在 RL 中，探索 (Exploration) 是指 Agent 嘗試未知的動作或狀態，以發現潛在的更高獎勵 [1, 22]。常見的探索機制如 Epsilon Greedy (以一定機率隨機採取動作) [1, 22]。

Dreamer V1 沒有額外的顯式探索機制（如 Epsilon Greedy） [1, 22]。其探索行為主要來源於模型內部的**不確定性**和**採樣過程** [1, 22]：
1.  與環境互動時，Observation Step 中的潛在狀態採樣帶來的不確定性 [1, 22]。
2.  訓練 Actor/Critic 時，Imagination Step 中潛在空間的採樣 [1, 22]。
3.  World Model 和 Actor/Critic 分別更新時，由於它們共享潛在空間但更新不同步，可能導致潛在空間不穩定，這種不穩定性也會產生探索效果 [1, 16, 22]。
4.  額外的**熵懲罰項 (Entropy Penalty)** 加入到 Actor 的 Loss 中 [1, 22]。最小化 Actor 輸出的動作分佈的熵會鼓勵策略變得確定，而最大化熵則鼓勵策略更具隨機性，有利於探索 [1, 22].

在討論中，這種類型的探索是否等價於 Curiosity (好奇心) 機制被提出 [1, 16]。Curiosity 通常是 Agent 根據其預測誤差或對環境的新奇程度給予額外的獎勵訊號，鼓勵 Agent 探索未知或難以預測的狀態 [1]。資料中講者認為 Dreamer 的不確定性來源不會直接影響 Reward，與常見的 Curiosity 方法不同 [1]。不過，也有觀點認為動態地根據狀態調整探索程度才更像 Curiosity 。這仍然是一個值得探討的點 [16]。

### Dreamer V2 的改進 (進階)

Dreamer V1 在一些任務上表現優異，但作者發現在 Atari 遊戲上表現不佳 [1, 8]。他們推測是 World Model 不夠準確造成的，儘管沒有直接證據 [1, 8]。為了解決這個問題，Dreamer V2 引入了幾個關鍵改進 [1, 4, 8]：

1.  **Discrete Latent Variables (離散潛在變數)**：V2 將潛在狀態的表示從 V1 的連續常態分佈改為**離散的類別分佈 (Categorical Distribution)，通常以 One-Hot 向量表示** [1, 4, 8, 23]. 這個改變被認為對於捕捉 Atari 遊戲中常見的離散或不連續機制（如物體突然出現）更為適合 [1, 8, 21]。為了處理離散變數的梯度傳播，V2 使用了 Straight-Through Gradient Estimator 等技巧 [1, 23]. VQ-VAE (Vector Quantized Variational Autoencoder) 是一種相關的技術，用於將連續空間量化到離散的 Codebook 中 [1, 8].

2.  **Learning by Reinforce**：這是 V2 在 Actor 更新方式上的重大改變，也是**對效能提升最關鍵的因素** [1, 4, 7, 8, 24, 25]. V2 不再直接最大化 Lambda Return，而是使用一種稱為 Learning by Reinforce 的方法來更新 Actor [1, 8, 25, 26]. 儘管名稱容易與 Policy Gradient 或 REINFORCE 演算法混淆，但講者推測這種方法類似於 Policy Gradient，它**直接放大或強化了 Agent 所採取動作的訊號**，使得梯度能夠更有效地傳遞到 Actor [1, 8, 21, 25]. 作者聲稱這種方法是 Unbiased 的，有利於找到更好的解決方案 [1, 21]. 這解決了 V1 中梯度從 Lambda Return 傳遞到 Actor 路徑較長、更新困難的問題 [1, 21].

3.  **KL Balancing**：改進了 KL 散度的計算方式 [1, 4, 8]. V2 在計算 Prior 與 Posterior 的 KL 散度時，將 Prior -> Posterior 的逼近 (forcing Prior towards Posterior) 和 Posterior -> Prior 的逼近 (forcing Posterior towards Prior) 分開，並給予不同的權重 [1, 4, 8, 26]. 資料中提到權重設定為 Prior->Posterior 0.8，Posterior->Prior 0.2 [1, 26]. 這樣做可以讓 Prior 更快地從 Posterior 中學到豐富的資訊，同時防止 Posterior 在逼近 Prior 時丟失太多自身特有的資訊 [1, 4, 8, 21, 26]. 這是一種更策略性的方式來平衡 World Model 的預測能力 (Prior) 與從真實觀察中提取資訊的能力 (Posterior) [1, 8, 26].

### 消融實驗 (Ablation Study)

消融實驗是研究中常用的一種方法，通過移除或替換模型中的某個組件來評估其對整體效能的影響 [24]. Dreamer V2 的論文進行了消融實驗，分析了各個改進對效能的貢獻 [24, 25].

實驗結果顯示，**Learning by Reinforce 是對 Dreamer V2 效能提升最為關鍵的因素** [7, 24, 25]. 其次是 KL Balancing，第三是 Discrete Latent Variable [7, 24, 25]. Image Gradient (圖像還原) 在 World Model 的訓練中也非常重要 [24, 25]. 令人意外的是，Reward Gradient 的重要性在實驗中似乎不高（移除後影響不大），這與直覺有些不符 [24, 25].

關於 Discrete Latent Variable 為何有效，資料中提到了幾種可能的原因 [21, 24]：
*   可以更好地建模混合分佈 (Mixture of Posterior) [21, 24].
*   Straight-Through Gradient 可能有所幫助 [21, 24].
*   離散變數意外地與 Atari 遊戲中某些離散的機制相符，形成一種有效的歸納偏置 (Inductive Bias) [21, 24].

### 總結

Dreamer 系列演算法是基於模型的強化學習領域的重大進展 [1, 9]. Dreamer V1 首次證明了 Model-Based 方法可以在複雜任務上達到人類水平的效能，並具備訓練效率高的優勢 [1, 2, 9]. Dreamer V2 透過引入離散潛在變數、改進策略更新方法 (Learning by Reinforce) 和 KL 散度計算方式 (KL Balancing)，進一步提升了效能，使其能夠成功處理 Atari 等更具挑戰性的遊戲環境 [1, 4, 9, 24]. 這些改進不僅提高了效能，也為理解基於模型的智能體如何在內部世界模型中進行有效學習和規劃提供了新的視角 [1, 9].
